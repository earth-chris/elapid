{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> Contemporary species distribution modeling tools for python. </p> <p> </p> <p>Documentation: earth-chris.github.io/elapid</p> <p>Source code: earth-chris/elapid</p>"},{"location":"#introduction","title":"Introduction","text":"<p><code>elapid</code> is a series of species distribution modeling tools for python. This includes a custom implementation of Maxent and a suite of methods to simplify working with biogeography data.</p> <p>The name is an homage to A Biogeographic Analysis of Australian Elapid Snakes (H.A. Nix, 1986), the paper widely credited with defining the essential bioclimatic variables to use in species distribution modeling. It's also a snake pun (a python wrapper for mapping snake biogeography).</p>"},{"location":"#installation","title":"Installation","text":"<p><code>pip install elapid</code> or <code>conda install -c conda-forge elapid</code></p> <p>Installing <code>glmnet</code> is optional, but recommended. This can be done with <code>pip install elapid[glmnet]</code> or <code>conda install -c conda-forge elapid glmnet</code>. For more support, and for information on why this package is recommended, see this page.</p> <p>The <code>conda</code> install is recommended for Windows users. While there is a <code>pip</code> distribution, you may experience some challenges. The easiest way to overcome them is to use Windows Subsystem for Linux (WSL). Otherwise, see this page for support.</p>"},{"location":"#why-use-elapid","title":"Why use elapid?","text":"<p>The amount and quality of bioegeographic data has increased dramatically over the past decade, as have cloud-based tools for working with it. <code>elapid</code> was designed to provide a set of modern, python-based tools for working with species occurrence records and environmental covariates to map different dimensions of a species' niche.</p> <p><code>elapid</code> supports working with modern geospatial data formats and uses contemporary approaches to training statistical models. It uses <code>sklearn</code> conventions to fit and apply models, <code>rasterio</code> to handle raster operations, <code>geopandas</code> for vector operations, and processes data under the hood with <code>numpy</code>.</p> <p>This makes it easier to do things like fit/apply models to multi-temporal and multi-scale data, fit geographically-weighted models, create ensembles, precisely define background point distributions, and summarize model predictions.</p> <p>It does the following things reasonably well:</p> <p> Point sampling</p> <p>Select random geographic point samples (aka background or pseudoabsence points) within polygons or rasters, handling <code>nodata</code> locations, as well as sampling from bias maps (using <code>elapid.sample_raster()</code>, <code>elapid.sample_vector()</code>, or <code>elapid.sample_bias_file()</code>).</p> <p> Vector annotation</p> <p>Extract and annotate point data from rasters, creating <code>GeoDataFrames</code> with sample locations and their matching covariate values (using <code>elapid.annotate()</code>). On-the-fly reprojection, dropping nodata, multi-band inputs and multi-file inputs are all supported.</p> <p> Zonal statistics</p> <p>Calculate zonal statistics from multi-band, multi-raster data into a single <code>GeoDataFrame</code> from one command (using <code>elapid.zonal_stats()</code>).</p> <p> Feature transformations</p> <p>Transform covariate data into derivative <code>features</code> to expand data dimensionality and improve prediction accuracy (like <code>elapid.ProductTransformer()</code>, <code>elapid.HingeTransformer()</code>, or the all-in-one <code>elapid.MaxentFeatureTransformer()</code>).</p> <p> Species distribution modeling</p> <p>Train and apply species distribution models based on annotated point data, configured with sensible defaults (like <code>elapid.MaxentModel()</code> and <code>elapid.NicheEnvelopeModel()</code>).</p> <p> Training spatially-aware models</p> <p>Compute spatially-explicit sample weights, checkerboard train/test splits, or geographically-clustered cross-validation splits to reduce spatial autocorellation effects (with <code>elapid.distance_weights()</code>, <code>elapid.checkerboard_split()</code> and <code>elapid.GeographicKFold()</code>).</p> <p> Applying models to rasters</p> <p>Apply any pixel-based model with a <code>.predict()</code> method to raster data to easily create prediction probability maps (like training a <code>RandomForestClassifier()</code> and applying with <code>elapid.apply_model_to_rasters()</code>).</p> <p> Cloud-native geo support</p> <p>Work with cloud- or web-hosted raster/vector data (on <code>https://</code>, <code>gs://</code>, <code>s3://</code>, etc.) to keep your disk free of temporary files.</p> <p>Check out some example code snippets and workflows on the Working with Geospatial Data page.</p> <p> <code>elapid</code> requires some effort on the user's part to draw samples and extract covariate data. This is by design.</p> <p>Selecting background samples, computing sample weights, splitting train/test data, and specifying training parameters are all critical modeling choices that have profound effects on inference and interpretation.</p> <p>The extra flexibility provided by <code>elapid</code> enables more control over the seemingly black-box approach of Maxent, enabling users to better tune and evaluate their models.</p>"},{"location":"#how-to-cite","title":"How to cite","text":"<p>BibTeX:</p> <pre><code>@article{\n  Anderson2023,\n  title = {elapid: Species distribution modeling tools for Python}, journal = {Journal of Open Source Software}\n  author = {Christopher B. Anderson},\n  doi = {10.21105/joss.04930},\n  url = {https://doi.org/10.21105/joss.04930},\n  year = {2023},\n  publisher = {The Open Journal},\n  volume = {8},\n  number = {84},\n  pages = {4930},\n}\n</code></pre> <p>Or click \"Cite this repository\" on the GitHub page.</p>"},{"location":"#developed-by","title":"Developed by","text":"<p>Christopher Anderson<sup>1</sup> <sup>2</sup></p> <p> </p> <ol> <li> <p>Earth Observation Lab, Planet Labs PBC \u21a9</p> </li> <li> <p>Center for Conservation Biology, Stanford University \u21a9</p> </li> </ol>"},{"location":"contributing/","title":"Contributing to elapid","text":"<p>Contributions to <code>elapid</code> are welcome, particularly from biogeographers, regular geographers, and machine teachers. You're welcome to fix bugs, add new features, expand or clarify documentation, or posit outlandish new modeling approaches.</p> <p>All contributions should go through the GitHub repository. Bug reports, ideas, and questions should be raised by opening an issue on the GitHub tracker. Suggestions for changes in code or documentation should be submitted as a pull request. All discussion should take place on GitHub to keep the development of <code>elapid</code> transparent.</p> <p>If you decide to contribute, ensure that you're using an up-to-date <code>main</code> branch. The latest development version will always be there, including the documentation.</p>"},{"location":"contributing/#steps-for-contributing","title":"Steps for contributing","text":"<ol> <li>Fork the git repository</li> <li>Create a development environment &amp; install dependencies</li> <li>Create a new branch, make changes to code &amp; add tests</li> <li>Update the docs</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#fork-the-git-repository","title":"Fork the git repository","text":"<p>You will need your own fork to work on the code. Go to the <code>elapid</code> project page and hit the Fork button. You will want to clone your fork to your machine:</p> <pre><code>git clone git@github.com:YOUR-USER-NAME/elapid.git elapid-YOURNAME\ncd elapid-YOURNAME\ngit remote add upstream git://github.com/elapid/elapid.git\n</code></pre> <p>This creates the directory <code>elapid-YOURNAME</code> and connects your repository to the upstream (main project) elapid repository.</p>"},{"location":"contributing/#create-a-development-environment-install-dependencies","title":"Create a development environment &amp; install dependencies","text":"<p>A development environment is a virtual space where you can keep an independent <code>elapid</code> install. This makes it easy to keep both a stable version in one place you use for work, and a development version (which you may break while playing with code) in another. First, you should:</p> <ul> <li>Install miniconda or anaconda</li> <li><code>cd</code> to the <code>elapid</code> source directory</li> </ul> <p>Linux and Mac users can then create the development environment with:</p> <pre><code>make init\n</code></pre> <p>This will create a conda environment named <code>elapid</code> then install the package, it's dependencies, <code>pre-commit</code> &amp; <code>pytest</code>.</p> <p>Windows users need to do things a bit differently, as is often the case:</p> <pre><code>conda create -n elapid -python=3.8 -y\nactivate elapid\nconda install geopandas rasterio scikit-learn tqdm pre-commit pytest pytest-cov pytest-xdist\npre-commit install\npip install -e .\n</code></pre> <p>This library uses <code>black</code>, <code>flake8</code> and <code>isort</code> pre-commit hooks. You should be familiar with pre-commit before contributing.</p>"},{"location":"contributing/#create-a-new-branch-make-changes-to-code-add-tests","title":"Create a new branch, make changes to code &amp; add tests","text":"<p>Make changes to the code on a separate branch to keep you main branch clean:</p> <pre><code>git checkout -b shiny-new-feature\n</code></pre> <p>Make changes to your code and write tests as you go. Write clear, self-documenting code to spend more time developing and less time describing how the code works.</p> <p>If your branch is no longer up-to-date with <code>main</code>, run the following code to update it:</p> <pre><code>git fetch upstream\ngit rebase upstream/main\n</code></pre> <p>Testing is done with <code>pytest</code>, which you can run with either:</p> <pre><code># for linux/mac\nmake test\n\n# for windows\npytest -x -n auto --cov --no-cov-on-fail --cov-report=term-missing:skip-covered\n</code></pre>"},{"location":"contributing/#update-the-docs","title":"Update the docs","text":"<p>There are two places to update docs. One is required (docstrings), the other optional (<code>mkdocs</code> web documentation).</p> <p>Adding docstrings to each new function/class is required. <code>elapid</code> uses Google-style docstrings and, when you contribute to it, you should too. <code>mkdocs</code> automatically renders the API docs for all functions written with this style, so you don't need to re-document each function outside of the code.</p> <p>If your code contributes important new features, or introduces novel/interesting concepts, write new documentation in the <code>docs/</code> directory. The docs system is managed by <code>mkdocs</code>, which renders from Markdown.</p> <p>You can install <code>mkdocs</code> and the associated plugins with:</p> <pre><code>pip install mkdocs mkdocs-material mkdocstrings[python] mkdocs-jupyter livereload\n</code></pre> <p>Then you can render the docs locally with:</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"contributing/#submit-a-pull-request","title":"Submit a pull request","text":"<p>Once you\u2019ve made changes and pushed them to your forked repository, you then submit a pull request to have them integrated into the <code>elapid</code> code base.</p> <p>For more information, you can find a PR tutorial in GitHub\u2019s Help Docs.</p> <p>Thanks, and happy mapping!</p>"},{"location":"install/","title":"Installation guide","text":"<p><code>elapid</code> is accessible from pypi:</p> <pre><code>pip install elapid\n</code></pre> <p>It's also accessible from conda:</p> <pre><code>conda install -c conda-forge elapid\n</code></pre> <p>This should suffice for most linux/mac users, as there are builds available for most of the dependencies (<code>numpy</code>, <code>sklearn</code>, <code>glmnet</code>, <code>geopandas</code>, <code>rasterio</code>).</p> <p>Windows installs can be more difficult. There are two primary challenges you may face: installing the key geospatial dependencies, and installing glmnet.</p> <p>You can avoid both of them by using Windows Subsystem for Linux, which creates a linux environment on your Windows machine. If you're using an older version of Windows, see the options below.</p>"},{"location":"install/#resolving-geospatial-dependencies","title":"Resolving geospatial dependencies","text":""},{"location":"install/#with-conda","title":"With conda","text":"<p>If you have <code>conda</code> installed, use that to install elapid's dependencies:</p> <pre><code>conda create -n elapid python=3.8 -y\nconda activate elapid # or just `activate elapid` on windows\nconda install -y -c conda-forge geopandas rasterio rtree scikit-learn tqdm\npip install elapid\n</code></pre>"},{"location":"install/#installing-on-windows-without-conda","title":"Installing on Windows without conda","text":"<p>You can get Windows builds of several key geospatial packages using <code>pipwin</code>, which installs wheels from an unofficial source:</p> <pre><code>pip install wheel pipwin\npipwin install numpy\npipwin install pandas\npipwin install shapely\npipwin install gdal\npipwin install fiona\npipwin install pyproj\npipwin install six\npipwin install rtree\npip install geopandas\npip install elapid\n</code></pre>"},{"location":"install/#installing-glmnet","title":"Installing glmnet","text":"<p><code>glmnet</code> needs to be manually installed on Windows. But technically it's not required.</p> <p><code>elapid</code> was written to try and match the modeling framework of the R version of Maxent, maxnet. <code>maxnet</code> uses an inhomogeneous Poisson process model, which fits penalized maximum likelihood models, and is handled by the package glmnet.</p> <p>There is a python wrapper for glmnet, which is used by <code>elapid</code>. But it has no Windows build, so it has to compile some fortran code on install. This means you need to have a fortran compiler running (like MinGW-w64 or Cygwin) if you want to install it (<code>pip install glmnet</code>).</p> <p>You can also checkout this GitHub issue to read about other people's solutions or contribute a better solution.</p>"},{"location":"install/#an-important-consideration","title":"An important consideration","text":"<p>To simplify installing and working with <code>elapid</code>, you can install the package and fit Maxent models with or without <code>glmnet</code>. This is handled by fitting the maximum likelihood model with either <code>glmnet</code> or with <code>sklearn</code>.</p> <p>The results are similar, but not the same. This is largely because of differences in how regularization is handled.</p> <p><code>glmnet</code> can handle arrays of regularization terms, which allows fine-scale control over the potential importance of different features. There are a series of calibrated defaults used by <code>elapid</code>, which were originally defined in maxnet but have been updated in opinionated ways.</p> <p>You can typically just assign a single value for regularization scores with <code>sklearn</code>.</p> <p>The differences are relatively small. When comparing models fit in <code>maxnet</code> to <code>elapid</code> models, the level of agreement was <code>r2 = 0.91</code> for glmnet models, <code>r2 = 0.85</code> for sklearn models.</p> <p>Still, I recommend users try their best to install <code>glmnet</code> if you're interested in maintaining fidelity to the other family of Maxent tools.</p>"},{"location":"examples/MaxentSimpleModel/","title":"A simple Maxent model","text":"In\u00a0[1]: Copied! <pre># packages\nimport elapid as ela\n\nimport rasterio as rio\nimport geopandas as gpd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport warnings\n\n# paths\nhttps = \"https://earth-chris.github.io/images/research\"\nvector = f\"{https}/ariolimax-buttoni.gpkg\"\nraster_names = [\n    \"ca-cloudcover-mean.tif\",\n    \"ca-cloudcover-stdv.tif\",\n    \"ca-leafareaindex-mean.tif\",\n    \"ca-leafareaindex-stdv.tif\",\n    \"ca-surfacetemp-mean.tif\",\n    \"ca-surfacetemp-stdv.tif\",\n]\nrasters = [f\"{https}/{raster}\" for raster in raster_names]\nlabels = [raster[3:-4] for raster in raster_names]\n\n# preferences\n%matplotlib inline\nmpl.style.use('ggplot')\nwarnings.filterwarnings(\"ignore\")\n\nprint(f\"Notebook last run with elapid version {ela.__version__}\")\n</pre> # packages import elapid as ela  import rasterio as rio import geopandas as gpd import matplotlib as mpl import matplotlib.pyplot as plt from sklearn import metrics import warnings  # paths https = \"https://earth-chris.github.io/images/research\" vector = f\"{https}/ariolimax-buttoni.gpkg\" raster_names = [     \"ca-cloudcover-mean.tif\",     \"ca-cloudcover-stdv.tif\",     \"ca-leafareaindex-mean.tif\",     \"ca-leafareaindex-stdv.tif\",     \"ca-surfacetemp-mean.tif\",     \"ca-surfacetemp-stdv.tif\", ] rasters = [f\"{https}/{raster}\" for raster in raster_names] labels = [raster[3:-4] for raster in raster_names]  # preferences %matplotlib inline mpl.style.use('ggplot') warnings.filterwarnings(\"ignore\")  print(f\"Notebook last run with elapid version {ela.__version__}\") <pre>Notebook last run with elapid version 1.0.1\n</pre> In\u00a0[2]: Copied! <pre># read the presence data, draw background point samples\npresence = gpd.read_file(vector)\nbackground = ela.sample_raster(rasters[0], count=10_000)\n\n# merge datasets and read the covariates at each point location\nmerged = ela.stack_geodataframes(presence, background, add_class_label=True)\nannotated = ela.annotate(merged, rasters, drop_na=True, quiet=True)\n\n# split the x/y data\nx = annotated.drop(columns=['class', 'geometry'])\ny = annotated['class']\n\n# train the model\nmodel = ela.MaxentModel(transform='cloglog', beta_multiplier=2.0)\nmodel.fit(x, y)\n\n# evaluate training performance\nypred = model.predict(x)\nauc = metrics.roc_auc_score(y, ypred)\nprint(f\"Training AUC score: {auc:0.3f}\")\n\n# save the fitted model to disk\nela.save_object(model, 'demo-maxent-model.ela')\n</pre> # read the presence data, draw background point samples presence = gpd.read_file(vector) background = ela.sample_raster(rasters[0], count=10_000)  # merge datasets and read the covariates at each point location merged = ela.stack_geodataframes(presence, background, add_class_label=True) annotated = ela.annotate(merged, rasters, drop_na=True, quiet=True)  # split the x/y data x = annotated.drop(columns=['class', 'geometry']) y = annotated['class']  # train the model model = ela.MaxentModel(transform='cloglog', beta_multiplier=2.0) model.fit(x, y)  # evaluate training performance ypred = model.predict(x) auc = metrics.roc_auc_score(y, ypred) print(f\"Training AUC score: {auc:0.3f}\")  # save the fitted model to disk ela.save_object(model, 'demo-maxent-model.ela') <pre>Training AUC score: 0.981\n</pre> In\u00a0[3]: Copied! <pre># partial dependence plots show how model predictions vary across the range of covariates\nfig, ax = model.partial_dependence_plot(x, labels=labels, dpi=100)\n</pre> # partial dependence plots show how model predictions vary across the range of covariates fig, ax = model.partial_dependence_plot(x, labels=labels, dpi=100) In\u00a0[4]: Copied! <pre># write the model predictions to disk\noutput_raster = 'demo-maxent-predictions.tif'\nela.apply_model_to_rasters(model, rasters, output_raster, quiet=True)\n\n# and read into memory\nwith rio.open(output_raster, 'r') as src:\n    pred = src.read(1, masked=True)\n</pre> # write the model predictions to disk output_raster = 'demo-maxent-predictions.tif' ela.apply_model_to_rasters(model, rasters, output_raster, quiet=True)  # and read into memory with rio.open(output_raster, 'r') as src:     pred = src.read(1, masked=True) In\u00a0[5]: Copied! <pre># plot the suitability predictions\nfig, ax = plt.subplots(1, 1, figsize=(7, 6), dpi=100)\nplot = ax.imshow(pred, vmin=0, vmax=1, cmap='GnBu')\nax.set_title('$Ariolimax\\ buttoni$ suitability')\nax.set_xticks([])\nax.set_yticks([])\ncbar = plt.colorbar(plot, ax=ax, label=\"relative occurrence probability\", pad=0.04)\nplt.tight_layout()\n</pre> # plot the suitability predictions fig, ax = plt.subplots(1, 1, figsize=(7, 6), dpi=100) plot = ax.imshow(pred, vmin=0, vmax=1, cmap='GnBu') ax.set_title('$Ariolimax\\ buttoni$ suitability') ax.set_xticks([]) ax.set_yticks([]) cbar = plt.colorbar(plot, ax=ax, label=\"relative occurrence probability\", pad=0.04) plt.tight_layout()"},{"location":"examples/MaxentSimpleModel/#a-simple-maxent-model","title":"A simple Maxent model\u00b6","text":"<p>Here's an end-to-end example for fitting and applying a Maxent model.</p> <p>This is to demonstrate the simplest pattern of model training. Full model training and evaluation should include creating train/test splits, cross-validation, feature selection, which aren't covered here.</p> <p>The <code>elapid</code> sample data are occurrence records for Ariolimax buttoni, a species of banana slug native to coastal California, annotated with climate and vegetation data.</p>"},{"location":"examples/WorkingWithGeospatialData/","title":"elapid's Geospatial Features","text":"In\u00a0[1]: Copied! <pre># packages\nimport elapid as ela\n\nimport os\nimport warnings\nfrom pprint import pprint\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport rasterio as rio\nfrom rasterio import plot as rioplot\nfrom sklearn import metrics\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n</pre> # packages import elapid as ela  import os import warnings from pprint import pprint import numpy as np import pandas as pd import geopandas as gpd import rasterio as rio from rasterio import plot as rioplot from sklearn import metrics import matplotlib as mpl import matplotlib.pyplot as plt In\u00a0[2]: Copied! <pre># paths\nhttps = \"https://earth-chris.github.io/images/research\"\ncsv = f\"{https}/ariolimax-ca.csv\"\ngpkg = f\"{https}/ariolimax-ca.gpkg\"\nraster_names = [\n    \"ca-cloudcover-mean.tif\",\n    \"ca-cloudcover-stdv.tif\",\n    \"ca-leafareaindex-mean.tif\",\n    \"ca-leafareaindex-stdv.tif\",\n    \"ca-surfacetemp-mean.tif\",\n    \"ca-surfacetemp-stdv.tif\",\n]\nrasters = [f\"{https}/{raster}\" for raster in raster_names]\nlabels = [\"CloudCoverMean\", \"CloudCoverStdv\", \"LAIMean\", \"LAIStdv\", \"SurfaceTempMean\", \"SurfaceTempStdv\"]\n</pre> # paths https = \"https://earth-chris.github.io/images/research\" csv = f\"{https}/ariolimax-ca.csv\" gpkg = f\"{https}/ariolimax-ca.gpkg\" raster_names = [     \"ca-cloudcover-mean.tif\",     \"ca-cloudcover-stdv.tif\",     \"ca-leafareaindex-mean.tif\",     \"ca-leafareaindex-stdv.tif\",     \"ca-surfacetemp-mean.tif\",     \"ca-surfacetemp-stdv.tif\", ] rasters = [f\"{https}/{raster}\" for raster in raster_names] labels = [\"CloudCoverMean\", \"CloudCoverStdv\", \"LAIMean\", \"LAIStdv\", \"SurfaceTempMean\", \"SurfaceTempStdv\"] In\u00a0[3]: Copied! <pre># preferences\nmpl.style.use('ggplot')\nwarnings.filterwarnings(\"ignore\")\npair_colors = ['#FFCC02', '#00458C']\n\nprint(f\"Notebook last run with elapid version {ela.__version__}\")\n</pre> # preferences mpl.style.use('ggplot') warnings.filterwarnings(\"ignore\") pair_colors = ['#FFCC02', '#00458C']  print(f\"Notebook last run with elapid version {ela.__version__}\") <pre>Notebook last run with elapid version 1.0.1\n</pre> In\u00a0[4]: Copied! <pre>gdf = gpd.read_file(gpkg)\nprint(gdf.head())\n</pre> gdf = gpd.read_file(gpkg) print(gdf.head()) <pre>             species  year                        geometry\n0  Ariolimax buttoni  2005  POINT (432271.994 4348048.850)\n1  Ariolimax buttoni  2005  POINT (643606.559 4427302.366)\n2  Ariolimax buttoni  2006  POINT (537588.166 4194012.740)\n3  Ariolimax buttoni  2007  POINT (468489.994 4361538.275)\n4  Ariolimax buttoni  2007  POINT (537466.797 4194096.065)\n</pre> In\u00a0[5]: Copied! <pre># read and preview the data\ndf = pd.read_csv(csv)\n\n# we convert the lat/lon columns into a GeoSeries dataset, which is spatially aware\ngeometry = ela.xy_to_geoseries(\n    x = df['decimalLongitude'],\n    y = df['decimalLatitude']\n)\n\n# then merge the two together into a GeoDataFrame\nariolimax = gpd.GeoDataFrame(df[[\"species\", \"year\"]], geometry=geometry)\n\n# show the evolution\nprint(\"DataFrame\")\nprint(df.head())\nprint(\"\\nGeoSeries\")\nprint(geometry.head())\nprint(\"\\nGeoDataFrame\")\nprint(ariolimax.head())\n</pre> # read and preview the data df = pd.read_csv(csv)  # we convert the lat/lon columns into a GeoSeries dataset, which is spatially aware geometry = ela.xy_to_geoseries(     x = df['decimalLongitude'],     y = df['decimalLatitude'] )  # then merge the two together into a GeoDataFrame ariolimax = gpd.GeoDataFrame(df[[\"species\", \"year\"]], geometry=geometry)  # show the evolution print(\"DataFrame\") print(df.head()) print(\"\\nGeoSeries\") print(geometry.head()) print(\"\\nGeoDataFrame\") print(ariolimax.head()) <pre>DataFrame\n             species  year  decimalLatitude  decimalLongitude\n0  Ariolimax buttoni  2022        37.897280       -122.709870\n1  Ariolimax buttoni  2020        39.331897       -123.784692\n2  Ariolimax buttoni  2022        37.934980       -122.636929\n3  Ariolimax buttoni  2022        38.032908       -122.721855\n4  Ariolimax buttoni  2022        38.042295       -122.876520\n\nGeoSeries\n0    POINT (-122.70987 37.89728)\n1    POINT (-123.78469 39.33190)\n2    POINT (-122.63693 37.93498)\n3    POINT (-122.72186 38.03291)\n4    POINT (-122.87652 38.04230)\ndtype: geometry\n\nGeoDataFrame\n             species  year                     geometry\n0  Ariolimax buttoni  2022  POINT (-122.70987 37.89728)\n1  Ariolimax buttoni  2020  POINT (-123.78469 39.33190)\n2  Ariolimax buttoni  2022  POINT (-122.63693 37.93498)\n3  Ariolimax buttoni  2022  POINT (-122.72186 38.03291)\n4  Ariolimax buttoni  2022  POINT (-122.87652 38.04230)\n</pre> In\u00a0[6]: Copied! <pre># plot the points geographically by species\nfig, ax = plt.subplots(figsize=(5,5))\nsp = ariolimax.plot(\n    column='species',\n    ax=ax,\n    legend=True,\n    legend_kwds={\"bbox_to_anchor\": (1, 0.5)}\n)\n</pre> # plot the points geographically by species fig, ax = plt.subplots(figsize=(5,5)) sp = ariolimax.plot(     column='species',     ax=ax,     legend=True,     legend_kwds={\"bbox_to_anchor\": (1, 0.5)} ) In\u00a0[7]: Copied! <pre># a couple of dummy locations\nlons = [-122.49, 151.0]\nlats = [37.79, -33.87]\n\n# it's smart to explicity set the CRS/projection to ensure the points are put in the right place\n# the CRS below assigns the points the lat/lon crs type\ncrs = \"EPSG:4326\"\nrandom_locations = ela.xy_to_geoseries(lons, lats, crs=crs)\n\n# these points now have lots of neat geographic methods and properties\nxmin, ymin, xmax, ymax = random_locations.total_bounds\nprint(f\"Bounding box: [{xmin}, {ymin}, {xmax}, {ymax}]\")\n\n# including getting a description of the coordinate reference system (crs)\nprint(\"\")\npprint(random_locations.crs)\n</pre> # a couple of dummy locations lons = [-122.49, 151.0] lats = [37.79, -33.87]  # it's smart to explicity set the CRS/projection to ensure the points are put in the right place # the CRS below assigns the points the lat/lon crs type crs = \"EPSG:4326\" random_locations = ela.xy_to_geoseries(lons, lats, crs=crs)  # these points now have lots of neat geographic methods and properties xmin, ymin, xmax, ymax = random_locations.total_bounds print(f\"Bounding box: [{xmin}, {ymin}, {xmax}, {ymax}]\")  # including getting a description of the coordinate reference system (crs) print(\"\") pprint(random_locations.crs) <pre>Bounding box: [-122.49, -33.87, 151.0, 37.79]\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n</pre> In\u00a0[8]: Copied! <pre># first we'll just read and display the surface temperature data\nfig, ax = plt.subplots(figsize=(6,6))\nsurfacetemp_raster = rasters[4]\nwith rio.open(surfacetemp_raster, 'r') as src:\n    profile = src.profile.copy()\n    \n    # this plots the mean annual temperature data\n    # low temps are dark grey, high temps are light grey\n    rioplot.show(src, ax=ax, cmap=\"gray\")\n    \n    # overlay the ariolimax data\n    ariolimax.to_crs(src.crs).plot(column='species', ax=ax, legend=True)\n</pre> # first we'll just read and display the surface temperature data fig, ax = plt.subplots(figsize=(6,6)) surfacetemp_raster = rasters[4] with rio.open(surfacetemp_raster, 'r') as src:     profile = src.profile.copy()          # this plots the mean annual temperature data     # low temps are dark grey, high temps are light grey     rioplot.show(src, ax=ax, cmap=\"gray\")          # overlay the ariolimax data     ariolimax.to_crs(src.crs).plot(column='species', ax=ax, legend=True) In\u00a0[9]: Copied! <pre>print(\"profile metadata\")\npprint(profile)\n</pre> print(\"profile metadata\") pprint(profile) <pre>profile metadata\n{'blockxsize': 512,\n 'blockysize': 512,\n 'compress': 'lzw',\n 'count': 1,\n 'crs': CRS.from_epsg(32610),\n 'driver': 'GTiff',\n 'dtype': 'float32',\n 'height': 1040,\n 'interleave': 'band',\n 'nodata': -9999.0,\n 'tiled': True,\n 'transform': Affine(1000.0, 0.0, 380000.0,\n       0.0, -1000.0, 4654000.0),\n 'width': 938}\n</pre> In\u00a0[10]: Copied! <pre># sampling\ncount = 10_000\npseudoabsence = ela.sample_raster(surfacetemp_raster, count)\n\n# plotting\npseudoabsence.plot(markersize=0.5)\npseudoabsence.describe()\n</pre> # sampling count = 10_000 pseudoabsence = ela.sample_raster(surfacetemp_raster, count)  # plotting pseudoabsence.plot(markersize=0.5) pseudoabsence.describe() Out[10]: <pre>count                       10000\nunique                       9879\ntop       POINT (1243500 3737500)\nfreq                            2\ndtype: object</pre> In\u00a0[11]: Copied! <pre># some pixel centroids were sampled multiple times\n# you can drop them if you'd like.\n#pseudoabsence.drop_duplicates(inplace=True)\n</pre> # some pixel centroids were sampled multiple times # you can drop them if you'd like. #pseudoabsence.drop_duplicates(inplace=True) <p>If you have a large raster that doesn't fit in memory, you can also set <code>ignore_mask=True</code> to use the rectangular bounds of the raster to draw samples. Many of the sample locations will be in <code>nodata</code> locations but we can address that later.</p> In\u00a0[12]: Copied! <pre># sampling\npseudoabsence_uniform = ela.sample_raster(\n    surfacetemp_raster,\n    count,\n    ignore_mask=True,\n)\n\n# plotting\npseudoabsence_uniform.plot(markersize=0.5)\npseudoabsence_uniform.describe()\n</pre> # sampling pseudoabsence_uniform = ela.sample_raster(     surfacetemp_raster,     count,     ignore_mask=True, )  # plotting pseudoabsence_uniform.plot(markersize=0.5) pseudoabsence_uniform.describe() Out[12]: <pre>count                                           10000\nunique                                          10000\ntop       POINT (1036258.4043319515 4263597.03776096)\nfreq                                                1\ndtype: object</pre> In\u00a0[13]: Copied! <pre>lai_raster = rasters[2]\npseudoabsence_bias = ela.sample_bias_file(lai_raster, count)\npseudoabsence_bias.plot(markersize=0.5)\npseudoabsence_bias.describe()\n</pre> lai_raster = rasters[2] pseudoabsence_bias = ela.sample_bias_file(lai_raster, count) pseudoabsence_bias.plot(markersize=0.5) pseudoabsence_bias.describe() Out[13]: <pre>count                      10000\nunique                      9761\ntop       POINT (445500 4377500)\nfreq                           3\ndtype: object</pre> In\u00a0[14]: Copied! <pre># again, duplicates can be dropped\n# (this can take some time to run)\n#pseudoabsence_bias.drop_duplicates(inplace=True)\n</pre> # again, duplicates can be dropped # (this can take some time to run) #pseudoabsence_bias.drop_duplicates(inplace=True) <p>We can see that we are selecting a disproportionately high number of points along California's coast - home to dense redwood forests where slugs thrive - and disproportionately few samples in the dry deserts in the southeast.</p> In\u00a0[15]: Copied! <pre># create a convex hull of all points by merging them into a single multipoint feature\ntarget_crs = pseudoabsence.crs\nariolimax_hull = gpd.GeoSeries(ariolimax.to_crs(target_crs).unary_union.convex_hull).set_crs(target_crs)\npseudoabsence_range = ela.sample_geoseries(ariolimax_hull, count // 2)\n\n# plot 'em together\nfig, ax = plt.subplots(figsize=(5, 5))\npseudoabsence.plot(markersize=0.25, ax=ax)\nariolimax_hull.plot(color='yellow', alpha=0.7, ax=ax)\npseudoabsence_range.plot(color='blue', ax=ax, markersize=0.25)\n</pre> # create a convex hull of all points by merging them into a single multipoint feature target_crs = pseudoabsence.crs ariolimax_hull = gpd.GeoSeries(ariolimax.to_crs(target_crs).unary_union.convex_hull).set_crs(target_crs) pseudoabsence_range = ela.sample_geoseries(ariolimax_hull, count // 2)  # plot 'em together fig, ax = plt.subplots(figsize=(5, 5)) pseudoabsence.plot(markersize=0.25, ax=ax) ariolimax_hull.plot(color='yellow', alpha=0.7, ax=ax) pseudoabsence_range.plot(color='blue', ax=ax, markersize=0.25) Out[15]: <pre>&lt;Axes: &gt;</pre> <p>The yellow polygon above shows the simple \"range map,\" the blue points are point samples selected from within that range, and the red points are the uniform sample, plotted for reference.</p> In\u00a0[16]: Copied! <pre># we'll work with just the Ariolimax buttoni records moving forward\nbuttoni = ariolimax[ariolimax['species'] == 'Ariolimax buttoni']\npresence = ela.annotate(\n    buttoni.geometry,\n    rasters,\n    labels=labels,\n    drop_na=True,\n)\n\n# we'll use the biased sample locations\nbackground = ela.annotate(\n    pseudoabsence_bias,\n    rasters,\n    labels=labels,\n    drop_na=True\n)\n</pre> # we'll work with just the Ariolimax buttoni records moving forward buttoni = ariolimax[ariolimax['species'] == 'Ariolimax buttoni'] presence = ela.annotate(     buttoni.geometry,     rasters,     labels=labels,     drop_na=True, )  # we'll use the biased sample locations background = ela.annotate(     pseudoabsence_bias,     rasters,     labels=labels,     drop_na=True ) <pre>Raster:   0%|                              | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>Sample:   0%|                              | 0/1647 [00:00&lt;?, ?it/s]</pre> <pre>Sample:   0%|                              | 0/1647 [00:00&lt;?, ?it/s]</pre> <pre>Sample:   0%|                              | 0/1647 [00:00&lt;?, ?it/s]</pre> <pre>Sample:   0%|                              | 0/1647 [00:00&lt;?, ?it/s]</pre> <pre>Sample:   0%|                              | 0/1647 [00:00&lt;?, ?it/s]</pre> <pre>Sample:   0%|                              | 0/1647 [00:00&lt;?, ?it/s]</pre> <pre>Raster:   0%|                              | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>Sample:   0%|                              | 0/10000 [00:00&lt;?, ?it/s]</pre> <pre>Sample:   0%|                              | 0/10000 [00:00&lt;?, ?it/s]</pre> <pre>Sample:   0%|                              | 0/10000 [00:00&lt;?, ?it/s]</pre> <pre>Sample:   0%|                              | 0/10000 [00:00&lt;?, ?it/s]</pre> <pre>Sample:   0%|                              | 0/10000 [00:00&lt;?, ?it/s]</pre> <pre>Sample:   0%|                              | 0/10000 [00:00&lt;?, ?it/s]</pre> <p>It took about 20 seconds to annotate nearly 12,000 points across six raster datasets hosted on the web. Neat.</p> <p>If you don't specify the labels, elapid will assign <code>['b1', 'b2', ..., 'bn']</code> names to each column.</p> <p>Setting <code>drop_na=True</code> works with raster datasets that have nodata values assigned in their metadata. These point locations will be dropped from the output dataframe.</p> <p>Next let's plot some histograms to understand the similarities and differences between the conditions at presence locations and at background locations.</p> In\u00a0[17]: Copied! <pre>fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(6,9))\n\nfor label, ax in zip(labels, axs.ravel()):\n    pvar = presence[label]\n    bvar = background[label]\n    ax.hist(\n        [pvar, bvar],\n        density=True,\n        alpha=0.7,\n        label=['presence', 'background'],\n        color=pair_colors,\n    )\n    ax.set_title(label)\n    \nhandles, lbls = ax.get_legend_handles_labels()\nfig.legend(handles, lbls, loc=(0.6, 0.9))\nplt.tight_layout()\n</pre> fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(6,9))  for label, ax in zip(labels, axs.ravel()):     pvar = presence[label]     bvar = background[label]     ax.hist(         [pvar, bvar],         density=True,         alpha=0.7,         label=['presence', 'background'],         color=pair_colors,     )     ax.set_title(label)      handles, lbls = ax.get_legend_handles_labels() fig.legend(handles, lbls, loc=(0.6, 0.9)) plt.tight_layout() <p>This checks out. Slugs are commonly found in areas with higher annual cloud cover, consistently cool temperatures, and high vegetation growth (even after adjusting for bias and oversampling areas with high LAI). Neat!</p> In\u00a0[18]: Copied! <pre># compute zonal stats within the generated range map\nzs = ela.zonal_stats(\n    ariolimax_hull,\n    rasters,\n    labels = labels,\n    mean = True,\n    percentiles = [25, 75],\n    quiet=True\n)\n\npprint(zs)\n</pre> # compute zonal stats within the generated range map zs = ela.zonal_stats(     ariolimax_hull,     rasters,     labels = labels,     mean = True,     percentiles = [25, 75],     quiet=True )  pprint(zs) <pre>   CloudCoverMean_mean  CloudCoverMean_stdv  CloudCoverMean_25pct  \\\n0             0.350516             0.089797              0.285211   \n\n   CloudCoverMean_75pct  CloudCoverStdv_mean  CloudCoverStdv_stdv  \\\n0              0.402993             0.468056             0.022682   \n\n   CloudCoverStdv_25pct  CloudCoverStdv_75pct  LAIMean_mean  LAIMean_stdv  \\\n0              0.451198               0.48941      1.340659      0.883253   \n\n   ...  LAIStdv_75pct  SurfaceTempMean_mean  SurfaceTempMean_stdv  \\\n0  ...       0.915175             26.174309               6.32528   \n\n   SurfaceTempMean_25pct  SurfaceTempMean_75pct  SurfaceTempStdv_mean  \\\n0              20.634378              30.810196              9.862017   \n\n   SurfaceTempStdv_stdv  SurfaceTempStdv_25pct  SurfaceTempStdv_75pct  \\\n0              1.748951               8.830321              11.059798   \n\n                                            geometry  \n0  POLYGON ((769358.168 3763373.577, 604251.523 4...  \n\n[1 rows x 25 columns]\n</pre> <p>What sets the <code>elapid</code> zonal stats method apart from other zonal stats packages is it:</p> <ul> <li>handles reprojection on the fly, meaning the input vector/raster data don't need to be reprojected a priori</li> <li>handles mutli-band input, computing summary stats on all bands (instead of having to specify which band)</li> <li>handles multi-raster input, reading inputs in serial but creating a single output GeoDataFrame.</li> </ul> In\u00a0[19]: Copied! <pre>presence['SampleWeight'] = ela.distance_weights(presence, n_neighbors=1)\npresence.plot(column='SampleWeight', legend=True, cmap='YlOrBr')\n</pre> presence['SampleWeight'] = ela.distance_weights(presence, n_neighbors=1) presence.plot(column='SampleWeight', legend=True, cmap='YlOrBr') Out[19]: <pre>&lt;Axes: &gt;</pre> <p>There's one lone point in the northwest corner that is assigned a very high weight because it's so isolated.</p> <p>Alternatively, you could compute the average distance from each point to all other points, computing sample weights from point density instead of the distance to the single nearest point. This may be useful if you have small clusters of a few points far away from large, densely populated regions, like the above.</p> <p>This is the default parameter setting (and specified by setting <code>n_neighbors=-1</code>).</p> In\u00a0[20]: Copied! <pre>background['SampleWeight'] = ela.distance_weights(background, n_neighbors=-1)\npresence['SampleWeight'] = ela.distance_weights(presence, n_neighbors=-1)\npresence.plot(column='SampleWeight', legend=True, cmap='YlOrBr')\n</pre> background['SampleWeight'] = ela.distance_weights(background, n_neighbors=-1) presence['SampleWeight'] = ela.distance_weights(presence, n_neighbors=-1) presence.plot(column='SampleWeight', legend=True, cmap='YlOrBr') Out[20]: <pre>&lt;Axes: &gt;</pre> <p>As you can see, there are tradeoffs for finding the best sample weights depending on the spatial distributions of each occurrence dataset. We'll use the sample density weights here to upweight the samples in the sierras (the northeastern cluster of points).</p> <p>This function uses <code>ela.nearest_point_distance()</code>, a handy function for computing the distance between each point and it's nearest neighbor.</p> In\u00a0[21]: Copied! <pre>merged = ela.stack_geodataframes(\n    presence,\n    background,\n    add_class_label=True,\n)\nmerged.describe()\n</pre> merged = ela.stack_geodataframes(     presence,     background,     add_class_label=True, ) merged.describe() Out[21]: CloudCoverMean CloudCoverStdv LAIMean LAIStdv SurfaceTempMean SurfaceTempStdv SampleWeight class count 11584.000000 11584.000000 11584.000000 11584.000000 11584.000000 11584.000000 11584.000000 11584.000000 mean 0.379856 0.475571 1.732820 0.842828 23.619701 9.443998 1.124759 0.137690 std 0.093895 0.024128 1.056417 0.481729 6.170858 2.337629 0.369802 0.344589 min 0.162916 0.313369 0.000000 0.000000 7.016870 3.055165 0.799182 0.000000 25% 0.305535 0.460072 0.893494 0.469951 18.769509 8.040773 0.894965 0.000000 50% 0.375786 0.483313 1.470570 0.738921 22.039745 9.398376 1.000000 0.000000 75% 0.439657 0.495140 2.463829 1.203184 27.946470 10.946168 1.193509 0.000000 max 0.889615 0.500000 4.861628 2.144895 42.884228 17.891376 4.507117 1.000000 In\u00a0[22]: Copied! <pre># initialize and print the model defaults\nmaxent = ela.MaxentModel()\npprint(maxent.get_params())\n</pre> # initialize and print the model defaults maxent = ela.MaxentModel() pprint(maxent.get_params()) <pre>{'beta_categorical': 1.0,\n 'beta_hinge': 1.0,\n 'beta_lqp': 1.0,\n 'beta_multiplier': 1.5,\n 'beta_threshold': 1.0,\n 'clamp': True,\n 'class_weights': 100,\n 'convergence_tolerance': 2e-06,\n 'feature_types': ['linear', 'hinge', 'product'],\n 'n_cpus': 10,\n 'n_hinge_features': 10,\n 'n_lambdas': 100,\n 'n_threshold_features': 10,\n 'scorer': 'roc_auc',\n 'tau': 0.5,\n 'transform': 'cloglog',\n 'use_lambdas': 'best',\n 'use_sklearn': True}\n</pre> In\u00a0[23]: Copied! <pre># set up the x/y data for model fitting\nx = merged.drop(columns=['class', 'SampleWeight'])\ny = merged['class']\nsample_weight = merged['SampleWeight']\n\n# fit and evaluate the model under naive conditions\nmaxent.fit(x, y, sample_weight=sample_weight)\nypred = maxent.predict(x)\n\nprint(f\"Unweighted naive AUC score: {metrics.roc_auc_score(y, ypred):0.3f}\")\nprint(f\"Weighted naive AUC score  : {metrics.roc_auc_score(y, ypred, sample_weight=sample_weight):0.3f}\")\n</pre> # set up the x/y data for model fitting x = merged.drop(columns=['class', 'SampleWeight']) y = merged['class'] sample_weight = merged['SampleWeight']  # fit and evaluate the model under naive conditions maxent.fit(x, y, sample_weight=sample_weight) ypred = maxent.predict(x)  print(f\"Unweighted naive AUC score: {metrics.roc_auc_score(y, ypred):0.3f}\") print(f\"Weighted naive AUC score  : {metrics.roc_auc_score(y, ypred, sample_weight=sample_weight):0.3f}\") <pre>Unweighted naive AUC score: 0.958\nWeighted naive AUC score  : 0.946\n</pre> In\u00a0[24]: Copied! <pre># let's use a 50km grid\ngrid_size = 50_000\ntrain, test = ela.checkerboard_split(merged, grid_size=grid_size)\n\n# we'll re-merge them for plotting purposes\ntrain['split'] = 'train'\ntest['split'] = 'test'\nchecker = ela.stack_geodataframes(train, test)\nchecker.plot(column='split', markersize=0.75, legend=True)\n</pre> # let's use a 50km grid grid_size = 50_000 train, test = ela.checkerboard_split(merged, grid_size=grid_size)  # we'll re-merge them for plotting purposes train['split'] = 'train' test['split'] = 'test' checker = ela.stack_geodataframes(train, test) checker.plot(column='split', markersize=0.75, legend=True) Out[24]: <pre>&lt;Axes: &gt;</pre> In\u00a0[25]: Copied! <pre># set up model fitting\nxtrain = train.drop(columns=['class', 'SampleWeight', 'split'])\nytrain = train['class']\nsample_weight_train = train['SampleWeight']\n\nxtest = test.drop(columns=['class', 'SampleWeight', 'split'])\nytest = test['class']\nsample_weight_test = test['SampleWeight']\n\nmaxent.fit(xtrain, ytrain, sample_weight=sample_weight_train)\nypred = maxent.predict(xtest)\n\nprint(f\"Unweighted checkerboard AUC score: {metrics.roc_auc_score(ytest, ypred):0.3f}\")\nprint(f\"Weighted checkerboard AUC score: {metrics.roc_auc_score(ytest, ypred, sample_weight=sample_weight_test):0.3f}\")\n</pre> # set up model fitting xtrain = train.drop(columns=['class', 'SampleWeight', 'split']) ytrain = train['class'] sample_weight_train = train['SampleWeight']  xtest = test.drop(columns=['class', 'SampleWeight', 'split']) ytest = test['class'] sample_weight_test = test['SampleWeight']  maxent.fit(xtrain, ytrain, sample_weight=sample_weight_train) ypred = maxent.predict(xtest)  print(f\"Unweighted checkerboard AUC score: {metrics.roc_auc_score(ytest, ypred):0.3f}\") print(f\"Weighted checkerboard AUC score: {metrics.roc_auc_score(ytest, ypred, sample_weight=sample_weight_test):0.3f}\") <pre>Unweighted checkerboard AUC score: 0.911\nWeighted checkerboard AUC score: 0.894\n</pre> <p>This lower AUC score is to be expected and should be interpreted as a good thing. It indicates that the naive model was overfitting to nearby points, and the inflated scores were likely the result of spatial autocorrelation.</p> <p>Future modelers might consider computing sample weights after splitting the train/test data to increase weights at checkerboard edges, which might improve model generalization.</p> In\u00a0[26]: Copied! <pre># create lists for plotting, tracking performance\ntest_dfs = []\nauc_scores = []\n\n# split the train / test data\nfold = 0\nn_splits = 3\ngfolds = ela.GeographicKFold(n_splits=n_splits)\n\nfor train_idx, test_idx in gfolds.split(presence):\n    # this returns arrays for indexing the original dataframe\n    # which requires using the pandas .iloc interface\n    p_train = presence.iloc[train_idx]\n    p_test = presence.iloc[test_idx]\n    \n    # simple background split\n    n_bg_train = len(background) // 2\n    b_train = background.iloc[:n_bg_train]\n    b_test = background.iloc[n_bg_train:]\n    \n    # merge 'em\n    train = ela.stack_geodataframes(p_train, b_train)\n    test = ela.stack_geodataframes(p_test, b_test)\n    \n    # set up model fitting\n    xtrain = train.drop(columns=['class', 'SampleWeight'])\n    ytrain = train['class']\n    sample_weight_train = train['SampleWeight']\n    xtest = test.drop(columns=['class', 'SampleWeight'])\n    ytest = test['class']\n    sample_weight_test = test['SampleWeight']\n\n    # evaluation\n    maxent.fit(xtrain, ytrain, sample_weight=sample_weight_train)\n    ypred = maxent.predict(xtest)\n    auc = f\"{metrics.roc_auc_score(ytest, ypred):0.3f}\"\n    auc_scores.append(auc)\n    \n    # store for plotting\n    fold += 1\n    p_test['fold'] = str(fold)\n    test_dfs.append(p_test)\n\n    \nprint(f\"{n_splits}-fold xval AUC scores: {', '.join(auc_scores)}\")\n\nfolds = gpd.GeoDataFrame(pd.concat(test_dfs, axis=0, ignore_index=True), crs=test.crs)\nfolds.plot(column='fold', legend=True)\nprint(\"Test fold\")\n</pre> # create lists for plotting, tracking performance test_dfs = [] auc_scores = []  # split the train / test data fold = 0 n_splits = 3 gfolds = ela.GeographicKFold(n_splits=n_splits)  for train_idx, test_idx in gfolds.split(presence):     # this returns arrays for indexing the original dataframe     # which requires using the pandas .iloc interface     p_train = presence.iloc[train_idx]     p_test = presence.iloc[test_idx]          # simple background split     n_bg_train = len(background) // 2     b_train = background.iloc[:n_bg_train]     b_test = background.iloc[n_bg_train:]          # merge 'em     train = ela.stack_geodataframes(p_train, b_train)     test = ela.stack_geodataframes(p_test, b_test)          # set up model fitting     xtrain = train.drop(columns=['class', 'SampleWeight'])     ytrain = train['class']     sample_weight_train = train['SampleWeight']     xtest = test.drop(columns=['class', 'SampleWeight'])     ytest = test['class']     sample_weight_test = test['SampleWeight']      # evaluation     maxent.fit(xtrain, ytrain, sample_weight=sample_weight_train)     ypred = maxent.predict(xtest)     auc = f\"{metrics.roc_auc_score(ytest, ypred):0.3f}\"     auc_scores.append(auc)          # store for plotting     fold += 1     p_test['fold'] = str(fold)     test_dfs.append(p_test)       print(f\"{n_splits}-fold xval AUC scores: {', '.join(auc_scores)}\")  folds = gpd.GeoDataFrame(pd.concat(test_dfs, axis=0, ignore_index=True), crs=test.crs) folds.plot(column='fold', legend=True) print(\"Test fold\") <pre>3-fold xval AUC scores: 0.888, 0.910, 0.695\nTest fold\n</pre> <p>The plot above shows the spatial distributions of test folds fom the geographic k-fold splits. The third split - the points in the Sierras to the east - had the lowest test performance. This can be interpreted in several ways, which we won't get into here.</p> In\u00a0[27]: Copied! <pre># let's use Ariolimax stramineus records here (fewer samples)\nstramineus = ariolimax[ariolimax['species'] == 'Ariolimax stramineus']\npresence = ela.annotate(\n    stramineus.geometry,\n    rasters,\n    labels=labels,\n    drop_na=True,\n    quiet=True,\n)\nmerged = ela.stack_geodataframes(presence, background, add_class_label=True)\n\n# store the individual sample predictions and evaluate at the end\nyobs_scores = []\nypred_scores = []\n\n# buffered leave-one-out, 5km buffer radius\ndistance = 5_000\nbloo = ela.BufferedLeaveOneOut(distance=distance)\nfor train_idx, test_idx in bloo.split(merged, class_label=\"class\"):\n    train = merged.iloc[train_idx]\n    test = merged.iloc[test_idx]\n    \n    # set up model fitting\n    xtrain = train.drop(columns=['class'])\n    ytrain = train['class']\n    xtest = test.drop(columns=['class'])\n    ytest = test['class']\n\n    # evaluation\n    maxent.fit(xtrain, ytrain)\n    ypred = maxent.predict(xtest)\n    ypred_scores.append(ypred)\n    yobs_scores.append(ytest)\n</pre> # let's use Ariolimax stramineus records here (fewer samples) stramineus = ariolimax[ariolimax['species'] == 'Ariolimax stramineus'] presence = ela.annotate(     stramineus.geometry,     rasters,     labels=labels,     drop_na=True,     quiet=True, ) merged = ela.stack_geodataframes(presence, background, add_class_label=True)  # store the individual sample predictions and evaluate at the end yobs_scores = [] ypred_scores = []  # buffered leave-one-out, 5km buffer radius distance = 5_000 bloo = ela.BufferedLeaveOneOut(distance=distance) for train_idx, test_idx in bloo.split(merged, class_label=\"class\"):     train = merged.iloc[train_idx]     test = merged.iloc[test_idx]          # set up model fitting     xtrain = train.drop(columns=['class'])     ytrain = train['class']     xtest = test.drop(columns=['class'])     ytest = test['class']      # evaluation     maxent.fit(xtrain, ytrain)     ypred = maxent.predict(xtest)     ypred_scores.append(ypred)     yobs_scores.append(ytest) In\u00a0[28]: Copied! <pre># get model predictions for just the background\nbg = merged[merged['class'] == 0]\nxbg = bg.drop(columns='class')\nybg = bg['class'].to_numpy()\nbg_pred = maxent.predict(xbg)\n\n# concatenate the background and leave-one-out points\nbl_pred = np.array(ypred_scores)\nbl_obs = np.array(yobs_scores)\nypred = np.vstack((bg_pred.reshape(-1,1), bl_pred))\nyobs = np.vstack((ybg.reshape(-1,1), bl_obs))\n\n# evaluate\nauc = metrics.roc_auc_score(yobs, ypred)\nprint(f\"Buffered leave-one-out AUC: {auc:0.3f}\")\n\n# no buffer\nx = merged.drop(columns=['class'])\ny = merged['class']\nmaxent.fit(x, y)\nypred = maxent.predict(x)\nauc = metrics.roc_auc_score(y, ypred)\nprint(f\"Naive model AUC: {auc:0.3f}\")\n</pre> # get model predictions for just the background bg = merged[merged['class'] == 0] xbg = bg.drop(columns='class') ybg = bg['class'].to_numpy() bg_pred = maxent.predict(xbg)  # concatenate the background and leave-one-out points bl_pred = np.array(ypred_scores) bl_obs = np.array(yobs_scores) ypred = np.vstack((bg_pred.reshape(-1,1), bl_pred)) yobs = np.vstack((ybg.reshape(-1,1), bl_obs))  # evaluate auc = metrics.roc_auc_score(yobs, ypred) print(f\"Buffered leave-one-out AUC: {auc:0.3f}\")  # no buffer x = merged.drop(columns=['class']) y = merged['class'] maxent.fit(x, y) ypred = maxent.predict(x) auc = metrics.roc_auc_score(y, ypred) print(f\"Naive model AUC: {auc:0.3f}\") <pre>Buffered leave-one-out AUC: 0.951\nNaive model AUC: 0.983\n</pre> <p>This function also modifies the \"leave-one-out\" approach to support testing on multiple points per-fold. You may want to run your cross-validation to evaluate test performance across multiple ecoregions, for example.</p> <p>You can do this by passing the <code>group</code> keyword during train/test splitting, and the value should correspond to a column name in the GeoDataFrame you pass.</p> <pre>bloo = ela.BufferedLeaveOneOut(distance=1000)\nfor train_idx, test_idx in bloo.split(merged, group=\"ecoregion\"):\n    train_points = merged.iloc[train_idx]\n    test_points = merged.iloc[test_idx]\n    # assumes `merged` has an 'ecoregion' column\n</pre> <p>This will find the unique values in the <code>ecoregion</code> column and iterate over those values, using the points from each ecoregion as test folds and excluding points within 1000m of those locations from the training data. This should greatly reduce the compute time because the number of leave-one-out iterations will be grouped by region instead of applied to every single point.</p> <p>This example is simply demonstrative and will not work as-is because this group label doesn't exist for this dataset.</p>"},{"location":"examples/WorkingWithGeospatialData/#elapids-geospatial-features","title":"elapid's Geospatial Features\u00b6","text":"<p>This notebook reviews common patterns for working with vector and raster data using <code>elapid</code>.</p> <p>We'll review the core tools used for extracting raster data from points and polygons, assigning sample weights based on sample density, and creating spatially-explicit train/test data, while also showing general patterns for working with the <code>GeoDataFrame</code> and <code>GeoSeries</code> classes from <code>geopandas</code>, which are frequently used.</p> <p>The sample data we'll use includes species occurrence records from the genus Ariolimax, a group of North American terrestrial slugs, as well as some climate and vegetation data.</p> <p>Several datasets used here are hosted on a web server. These files can be accessed locally with <code>ela.download_sample_data()</code>, but we'll work with them directly on the web to demonstrate that reading remote data is nearly identical to reading local data thanks to the amazing libraries <code>elapid</code> is built on top of.</p>"},{"location":"examples/WorkingWithGeospatialData/#packages-paths-preferences","title":"Packages, paths &amp; preferences\u00b6","text":""},{"location":"examples/WorkingWithGeospatialData/#working-with-point-format-data","title":"Working with Point-format data\u00b6","text":"<p>Species distribution modeling is the science and the art of working with point-format species occurrence data. These are locations on the landscape where a species has been observed and identified and recorded as present in an x/y location. We then model the relative likelihood of observing that species in other locations based on the environmental conditions where that species is observed.</p> <p>Occurrence data are typically recorded with latitude/longitude coordinates for a single location, also known as \"point-format data.\" Environmental data are typically provided as image data with coordinate reference information, or \"raster data.\" <code>elapid</code> provides a series of tools for working with these datasets.</p>"},{"location":"examples/WorkingWithGeospatialData/#reading-vector-files","title":"Reading vector files\u00b6","text":"<p>Point-format data are most commonly provided in formats like GeoPackage, GeoJSON, or Shapefiles. You can read these into memory easily with <code>geopandas</code>.</p>"},{"location":"examples/WorkingWithGeospatialData/#reading-xy-data","title":"Reading x/y data\u00b6","text":"<p>Not all datasets are packaged so conveniently. SDM data often come as CSV files with a column for latitude and longitude, for eample. We can convert that information into a <code>GeoDataFrame</code> so we can natively perform geospatial operations.</p> <p>The example CSV is a subset of this GBIF occurrence dataset, which contains data from four species of Banana Slug across California.</p>"},{"location":"examples/WorkingWithGeospatialData/#converting-from-arrays-to-coordinates","title":"Converting from arrays to coordinates\u00b6","text":"<p>Or if you're working with coordinate data in memory, say as an array or a list, <code>xy_to_geoseries</code> works with pretty much any iterable type.</p>"},{"location":"examples/WorkingWithGeospatialData/#working-with-raster-data","title":"Working with raster data\u00b6","text":"<p>This demo includes 6 raster files defined at the start of the notebook, corresponding to the average and standard deviation in annual cloud cover, temperature, and vegetation growth across California.</p> <p>Following a brief overview of the rasters, the sections below will review how to extract data from them using point and polygon vector data.</p>"},{"location":"examples/WorkingWithGeospatialData/#drawing-random-pseudoabsence-locations","title":"Drawing random pseudoabsence locations\u00b6","text":"<p>In addition to species occurrence records (where <code>y = 1</code>), species distributions models often require a set of random pseudo-absence/background points (<code>y = 0</code>). These are a random geographic sampling of where you might expect to find a species across the target landscape.</p> <p>The background point distribution is extremely important for presence-only models, and should be selected with care. <code>elapid</code> provides a range of methods for sample selection.</p> <p>For the unintiated, these papers by Morgane Barbet-Massin et al. and Phillips et al. are excellent resources to consult for considering how background point samples should be selected.</p>"},{"location":"examples/WorkingWithGeospatialData/#from-a-rasters-extent","title":"From a raster's extent\u00b6","text":"<p>You can use <code>elapid</code> to create a uniform random geographic sample from unmasked locations within a raster's extent:</p>"},{"location":"examples/WorkingWithGeospatialData/#from-a-bias-raster","title":"From a bias raster\u00b6","text":"<p>You could also provide a \"bias\" file, where the raster grid cells contain information on the probability of sampling an area. Bias adjustments are recommended because species occurrence records are often biased towards certain locations (near roads, parks, or trails).</p> <p>The grid cells can be an arbitrary range of values. What's important is that the values encode a linear range of numbers that are higher where you're more likely to draw a sample. The probability of drawing a sample is dependent on two factors: the range of values provided and the frequency of values across the dataset.</p> <p>Because slugs are detrivores, we'll guess that people only go looking for Ariolimax in places with lots of vegetation growth. We'll use the leaf area index (LAI) covariate to select more background samples from highly vegetated areas.</p>"},{"location":"examples/WorkingWithGeospatialData/#from-a-vector-polygon","title":"From a vector polygon\u00b6","text":"<p>Another way to address bias would be to only select points inside area where that species is known to occur (within a certain ecoregion, for example).</p> <p>The Ariolimax dataset contains occurrence records for four species within the genus. If we're interested in understanding the distributions of only one of them, we could construct a rudimentary \"range map\" for this genus by computing the bounding geometry of all points then sampling within those bounds.</p>"},{"location":"examples/WorkingWithGeospatialData/#point-annotation","title":"Point annotation\u00b6","text":"<p>Annotation refers to extracting raster values at the each point occurrence record and storing those records together in a <code>GeoDataFrame</code>. Once you have your presence and pseudo-absence records, you can annotate these records with the corresponding covariate data.</p> <p>This function, since it's geographically indexed, doesn't require the point data and the raster data to be in the same projection. <code>elapid</code> handles reprojection and resampling on the fly.</p> <p>It also allows you to pass multiple raster files, which can be in different projections, extents, or grid sizes. This means you don't have to explicitly re-sample your raster data prior to analysis, which is always a chore.</p>"},{"location":"examples/WorkingWithGeospatialData/#zonal-statistics","title":"Zonal statistics\u00b6","text":"<p>In addition to the tools for working with Point data, <code>elapid</code> contains a routine for calculating zonal statistics from Polygon or MutliPolygon geometry types.</p> <p>This routine reads an array of raster data covering the extent of a polygon, masks the areas outside the polygon, and computes summary statistics such as the mean, standard deviation, and mode of the array.</p> <p>The stats reported are managed by a set of keywords (<code>count=True</code>, <code>sum=True</code>, <code>skew=True</code>). The <code>all=True</code> keyword is a shortcut to compute all of the stats. You'll still have to explicitly pass a list of percentiles, however.</p>"},{"location":"examples/WorkingWithGeospatialData/#geographic-sample-weights","title":"Geographic sample weights\u00b6","text":"<p>Despite ingesting spatial data, many statistically-driven SDMs are not spatial models in a traditional sense: they don't handle geographic information during model selection or in model scoring.</p> <p>One way to add spatial information to a model is to compute geographically-explicit sample weights. <code>elapid</code> does this by calculating weights based on the distance to the nearest neighbor. Points nearby other points receive lower weight scores; far-away points receive higher weight scores.</p> <p>You can compute weights based on the distance to the nearest point by setting <code>n_neighbors=1</code>.</p>"},{"location":"examples/WorkingWithGeospatialData/#stacking-dataframes","title":"Stacking dataframes\u00b6","text":"<p>Now that we have our presence points and our pseudoabsence points, we can merge them together into a single dataset for analysis.</p> <p><code>elapid</code> provides a convenience function for merging GeoDataFrames, designed for stacking presence/background data. It will reproject geometries on the fly if needed, and you can optionally add a 'class' column to the output GeoDataFrame.</p>"},{"location":"examples/WorkingWithGeospatialData/#model-setup","title":"Model setup\u00b6","text":"<p>We won't get into much detail on model parameters, leaving that for another in-depth notebook. This section will focus on fitting models using default settings, then evaluate the effects of different forms of train/test splitting strategies.</p>"},{"location":"examples/WorkingWithGeospatialData/#train-test-splits","title":"Train / test splits\u00b6","text":"<p>Uniformly random train/test splits are generally discouraged in spatial modeling because of the strong spatial structure inherent in many datasets. The non-independence of these data is referred to as spatial autocorrelation. Using distance- or density-based sample weights is one way to mitigate these effects. Another is to split the data into geographically distinct train/test regions to try and prioritize model generalization.</p>"},{"location":"examples/WorkingWithGeospatialData/#checkerboard-splits","title":"Checkerboard splits\u00b6","text":"<p>With a \"checkerbox\" train/test split, points are intersected along a regular grid, and every other grid is used to split the data into train/test sets.</p> <p>The height and width of the grid used to split the data is controlled by the <code>grid_size</code> parameter. This should specify distance in the units of the point data's CRS. In our example, the units are in meters.</p> <p>The black and white structure of the checkerboard means this method can only generate one train/test split.</p>"},{"location":"examples/WorkingWithGeospatialData/#geographic-k-fold-splits","title":"Geographic k-fold splits\u00b6","text":"<p>Alternatively, you can create <code>k</code> geographically-clustered folds using the <code>GeographicKFold</code> cross validation strategy. This method is effective for understanding how well models fit in one region will generalize to areas outside the training domain.</p> <p>This method uses KMeans clusters fit with the x/y locations of the point data, grouping points into spatially distinct groups. This cross-validation strategy is a good way to test how well models generalize outside of their training extents into novel geographic regions.</p> <p>What makes this tricky is the clustering should only be applied to the presence data. So we'll geographically split the presence data and use a simple background train/test split for each fold.</p>"},{"location":"examples/WorkingWithGeospatialData/#buffered-leave-one-out","title":"Buffered leave-one-out\u00b6","text":"<p>Leave-one-out cross-validation refers to training on n-1 points and testing on a single test point, looping over the full dataset to test on each point separately.</p> <p>The buffered leave-one-out strategy, as described by Ploton et al., modifies this approach. While each point is iterated over for testing, the pool of available training points is reduced at each iteration by dropping training points located within a certain distance of the test data. The purpose of this method is to evaluate how a model performs far away from where it was trained.</p> <p>In SDM contexts, however, standard leave-one-out strategies may not be appropriate. Model performance is best evaluated on presence-only data; model performance on background points may not be meaningful.</p> <p>To only run the leave-one-out analysis on presence-only points, specify the column with the 0/1 class labels during train/test splitting:</p>"},{"location":"module/config/","title":"elapid.config","text":"<p>SDM model configuration parameters.</p>"},{"location":"module/features/","title":"elapid.features","text":"<p>Functions to transform covariate data into complex model features.</p>"},{"location":"module/features/#elapid.features.CategoricalTransformer","title":"<code>CategoricalTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Applies one-hot encoding to categorical covariate datasets.</p> Source code in <code>elapid/features.py</code> <pre><code>class CategoricalTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Applies one-hot encoding to categorical covariate datasets.\"\"\"\n\n    def __init__(self):\n        self.estimators_ = None\n\n    def fit(self, x: ArrayLike) -&gt; \"CategoricalTransformer\":\n        \"\"\"Compute the minimum and maximum for scaling.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                The data used to compute the per-feature minimum and maximum\n                used for later scaling along the features axis.\n\n        Returns:\n            self. Returns the transformer with fitted parameters.\n        \"\"\"\n        self.estimators_ = []\n        x = np.array(x)\n        if x.ndim == 1:\n            estimator = OneHotEncoder(dtype=np.uint8, sparse_output=False)\n            self.estimators_.append(estimator.fit(x.reshape(-1, 1)))\n        else:\n            nrows, ncols = x.shape\n            for col in range(ncols):\n                xsub = x[:, col].reshape(-1, 1)\n                estimator = OneHotEncoder(dtype=np.uint8, sparse_output=False)\n                self.estimators_.append(estimator.fit(xsub))\n\n        return self\n\n    def transform(self, x: ArrayLike) -&gt; np.ndarray:\n        \"\"\"Scale covariates according to the feature range.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                Input data that will be transformed.\n\n        Returns:\n            ndarray with transformed data.\n        \"\"\"\n        x = np.array(x)\n        if x.ndim == 1:\n            estimator = self.estimators_[0]\n            return estimator.transform(x.reshape(-1, 1))\n        else:\n            class_data = []\n            nrows, ncols = x.shape\n            for col in range(ncols):\n                xsub = x[:, col].reshape(-1, 1)\n                estimator = self.estimators_[col]\n                class_data.append(estimator.transform(xsub))\n            return np.concatenate(class_data, axis=1)\n\n    def fit_transform(self, x: ArrayLike) -&gt; np.ndarray:\n        \"\"\"Fits scaler to x and returns transformed features.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                Input data to fit the scaler and to transform.\n\n        Returns:\n            ndarray with transformed data.\n        \"\"\"\n        self.fit(x)\n        return self.transform(x)\n</code></pre>"},{"location":"module/features/#elapid.features.CategoricalTransformer.fit","title":"<code>fit(x)</code>","text":"<p>Compute the minimum and maximum for scaling.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.</p> required <p>Returns:</p> Type Description <code>CategoricalTransformer</code> <p>self. Returns the transformer with fitted parameters.</p> Source code in <code>elapid/features.py</code> <pre><code>def fit(self, x: ArrayLike) -&gt; \"CategoricalTransformer\":\n    \"\"\"Compute the minimum and maximum for scaling.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            The data used to compute the per-feature minimum and maximum\n            used for later scaling along the features axis.\n\n    Returns:\n        self. Returns the transformer with fitted parameters.\n    \"\"\"\n    self.estimators_ = []\n    x = np.array(x)\n    if x.ndim == 1:\n        estimator = OneHotEncoder(dtype=np.uint8, sparse_output=False)\n        self.estimators_.append(estimator.fit(x.reshape(-1, 1)))\n    else:\n        nrows, ncols = x.shape\n        for col in range(ncols):\n            xsub = x[:, col].reshape(-1, 1)\n            estimator = OneHotEncoder(dtype=np.uint8, sparse_output=False)\n            self.estimators_.append(estimator.fit(xsub))\n\n    return self\n</code></pre>"},{"location":"module/features/#elapid.features.CategoricalTransformer.fit_transform","title":"<code>fit_transform(x)</code>","text":"<p>Fits scaler to x and returns transformed features.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ndarray with transformed data.</p> Source code in <code>elapid/features.py</code> <pre><code>def fit_transform(self, x: ArrayLike) -&gt; np.ndarray:\n    \"\"\"Fits scaler to x and returns transformed features.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            Input data to fit the scaler and to transform.\n\n    Returns:\n        ndarray with transformed data.\n    \"\"\"\n    self.fit(x)\n    return self.transform(x)\n</code></pre>"},{"location":"module/features/#elapid.features.CategoricalTransformer.transform","title":"<code>transform(x)</code>","text":"<p>Scale covariates according to the feature range.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) Input data that will be transformed.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ndarray with transformed data.</p> Source code in <code>elapid/features.py</code> <pre><code>def transform(self, x: ArrayLike) -&gt; np.ndarray:\n    \"\"\"Scale covariates according to the feature range.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            Input data that will be transformed.\n\n    Returns:\n        ndarray with transformed data.\n    \"\"\"\n    x = np.array(x)\n    if x.ndim == 1:\n        estimator = self.estimators_[0]\n        return estimator.transform(x.reshape(-1, 1))\n    else:\n        class_data = []\n        nrows, ncols = x.shape\n        for col in range(ncols):\n            xsub = x[:, col].reshape(-1, 1)\n            estimator = self.estimators_[col]\n            class_data.append(estimator.transform(xsub))\n        return np.concatenate(class_data, axis=1)\n</code></pre>"},{"location":"module/features/#elapid.features.CumulativeTransformer","title":"<code>CumulativeTransformer</code>","text":"<p>               Bases: <code>QuantileTransformer</code></p> <p>Applies a percentile-based transform to estimate cumulative suitability.</p> Source code in <code>elapid/features.py</code> <pre><code>class CumulativeTransformer(QuantileTransformer):\n    \"\"\"Applies a percentile-based transform to estimate cumulative suitability.\"\"\"\n\n    def __init__(self):\n        super().__init__(n_quantiles=100, output_distribution=\"uniform\")\n</code></pre>"},{"location":"module/features/#elapid.features.FeaturesMixin","title":"<code>FeaturesMixin</code>","text":"<p>Methods for formatting x data and labels</p> Source code in <code>elapid/features.py</code> <pre><code>class FeaturesMixin:\n    \"\"\"Methods for formatting x data and labels\"\"\"\n\n    def _format_covariate_data(self, x: ArrayLike) -&gt; Tuple[np.array, np.array]:\n        \"\"\"Reads input x data and formats it to consistent array dtypes.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n\n        Returns:\n            (continuous, categorical) tuple of ndarrays with continuous and\n                categorical covariate data.\n        \"\"\"\n        if isinstance(x, np.ndarray):\n            if self.categorical_ is None:\n                con = x\n                cat = None\n            else:\n                con = x[:, self.continuous_]\n                cat = x[:, self.categorical_]\n\n        elif isinstance(x, pd.DataFrame):\n            con = x[self.continuous_pd_].to_numpy()\n            if len(self.categorical_pd_) &gt; 0:\n                cat = x[self.categorical_pd_].to_numpy()\n            else:\n                cat = None\n\n        else:\n            raise TypeError(f\"Unsupported x dtype: {type(x)}. Must be pd.DataFrame or np.array\")\n\n        return con, cat\n\n    def _format_labels_and_dtypes(self, x: ArrayLike, categorical: list = None, labels: list = None) -&gt; None:\n        \"\"\"Read input x data and lists of categorical data indices and band\n            labels to format and store this info for later indexing.\n\n        Args:\n            s: array-like of shape (n_samples, n_features)\n            categorical: indices indicating which x columns are categorical\n            labels: covariate column labels. ignored if x is a pandas DataFrame\n        \"\"\"\n        if isinstance(x, np.ndarray):\n            nrows, ncols = x.shape\n            if categorical is None:\n                continuous = list(range(ncols))\n            else:\n                continuous = list(set(range(ncols)).difference(set(categorical)))\n            self.labels_ = labels or make_band_labels(ncols)\n            self.categorical_ = categorical\n            self.continuous_ = continuous\n\n        elif isinstance(x, pd.DataFrame):\n            x.drop([\"geometry\"], axis=1, errors=\"ignore\", inplace=True)\n            self.labels_ = labels or list(x.columns)\n\n            # store both pandas and numpy indexing of these values\n            self.continuous_pd_ = list(x.select_dtypes(exclude=\"category\").columns)\n            self.categorical_pd_ = list(x.select_dtypes(include=\"category\").columns)\n\n            all_columns = list(x.columns)\n            self.continuous_ = [all_columns.index(item) for item in self.continuous_pd_ if item in all_columns]\n            if len(self.categorical_pd_) != 0:\n                self.categorical_ = [all_columns.index(item) for item in self.categorical_pd_ if item in all_columns]\n            else:\n                self.categorical_ = None\n</code></pre>"},{"location":"module/features/#elapid.features.HingeTransformer","title":"<code>HingeTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Fits hinge transformations to an array of covariates.</p> Source code in <code>elapid/features.py</code> <pre><code>class HingeTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Fits hinge transformations to an array of covariates.\"\"\"\n\n    def __init__(self, n_hinges: int = MaxentConfig.n_hinge_features):\n        self.n_hinges = n_hinges\n        self.mins_ = None\n        self.maxs_ = None\n        self.hinge_indices_ = None\n\n    def fit(self, x: ArrayLike) -&gt; \"HingeTransformer\":\n        \"\"\"Compute the minimum and maximum for scaling.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                The data used to compute the per-feature minimum and maximum\n                used for later scaling along the features axis.\n\n        Returns:\n            self. Updatesd transformer with fitted parameters.\n        \"\"\"\n        x = np.array(x)\n        self.mins_ = x.min(axis=0)\n        self.maxs_ = x.max(axis=0)\n        self.hinge_indices_ = np.linspace(self.mins_, self.maxs_, self.n_hinges)\n\n        return self\n\n    def transform(self, x: ArrayLike) -&gt; np.ndarray:\n        \"\"\"Scale covariates according to the feature range.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                Input data that will be transformed.\n\n        Returns:\n            ndarray with transformed data.\n        \"\"\"\n        x = np.array(x)\n        xarr = repeat_array(x, self.n_hinges - 1, axis=-1)\n        lharr = repeat_array(self.hinge_indices_[:-1].transpose(), len(x), axis=0)\n        rharr = repeat_array(self.hinge_indices_[1:].transpose(), len(x), axis=0)\n        lh = left_hinge(xarr, lharr, self.maxs_)\n        rh = right_hinge(xarr, self.mins_, rharr)\n        return np.concatenate((lh, rh), axis=2).reshape(x.shape[0], -1)\n\n    def fit_transform(self, x: ArrayLike) -&gt; np.ndarray:\n        \"\"\"Fits scaler to x and returns transformed features.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                Input data to fit the scaler and to transform.\n\n        Returns:\n            ndarray with transformed data.\n        \"\"\"\n        self.fit(x)\n        return self.transform(x)\n</code></pre>"},{"location":"module/features/#elapid.features.HingeTransformer.fit","title":"<code>fit(x)</code>","text":"<p>Compute the minimum and maximum for scaling.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.</p> required <p>Returns:</p> Type Description <code>HingeTransformer</code> <p>self. Updatesd transformer with fitted parameters.</p> Source code in <code>elapid/features.py</code> <pre><code>def fit(self, x: ArrayLike) -&gt; \"HingeTransformer\":\n    \"\"\"Compute the minimum and maximum for scaling.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            The data used to compute the per-feature minimum and maximum\n            used for later scaling along the features axis.\n\n    Returns:\n        self. Updatesd transformer with fitted parameters.\n    \"\"\"\n    x = np.array(x)\n    self.mins_ = x.min(axis=0)\n    self.maxs_ = x.max(axis=0)\n    self.hinge_indices_ = np.linspace(self.mins_, self.maxs_, self.n_hinges)\n\n    return self\n</code></pre>"},{"location":"module/features/#elapid.features.HingeTransformer.fit_transform","title":"<code>fit_transform(x)</code>","text":"<p>Fits scaler to x and returns transformed features.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ndarray with transformed data.</p> Source code in <code>elapid/features.py</code> <pre><code>def fit_transform(self, x: ArrayLike) -&gt; np.ndarray:\n    \"\"\"Fits scaler to x and returns transformed features.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            Input data to fit the scaler and to transform.\n\n    Returns:\n        ndarray with transformed data.\n    \"\"\"\n    self.fit(x)\n    return self.transform(x)\n</code></pre>"},{"location":"module/features/#elapid.features.HingeTransformer.transform","title":"<code>transform(x)</code>","text":"<p>Scale covariates according to the feature range.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) Input data that will be transformed.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ndarray with transformed data.</p> Source code in <code>elapid/features.py</code> <pre><code>def transform(self, x: ArrayLike) -&gt; np.ndarray:\n    \"\"\"Scale covariates according to the feature range.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            Input data that will be transformed.\n\n    Returns:\n        ndarray with transformed data.\n    \"\"\"\n    x = np.array(x)\n    xarr = repeat_array(x, self.n_hinges - 1, axis=-1)\n    lharr = repeat_array(self.hinge_indices_[:-1].transpose(), len(x), axis=0)\n    rharr = repeat_array(self.hinge_indices_[1:].transpose(), len(x), axis=0)\n    lh = left_hinge(xarr, lharr, self.maxs_)\n    rh = right_hinge(xarr, self.mins_, rharr)\n    return np.concatenate((lh, rh), axis=2).reshape(x.shape[0], -1)\n</code></pre>"},{"location":"module/features/#elapid.features.LinearTransformer","title":"<code>LinearTransformer</code>","text":"<p>               Bases: <code>MinMaxScaler</code></p> <p>Applies linear feature transformations to rescale features from 0-1.</p> Source code in <code>elapid/features.py</code> <pre><code>class LinearTransformer(MinMaxScaler):\n    \"\"\"Applies linear feature transformations to rescale features from 0-1.\"\"\"\n\n    def __init__(\n        self,\n        clamp: bool = MaxentConfig.clamp,\n        feature_range: Tuple[float, float] = (0.0, 1.0),\n    ):\n        self.clamp = clamp\n        self.feature_range = feature_range\n        super().__init__(clip=clamp, feature_range=feature_range)\n</code></pre>"},{"location":"module/features/#elapid.features.MaxentFeatureTransformer","title":"<code>MaxentFeatureTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code>, <code>FeaturesMixin</code></p> <p>Transforms covariate data into maxent-format feature data.</p> Source code in <code>elapid/features.py</code> <pre><code>class MaxentFeatureTransformer(BaseEstimator, TransformerMixin, FeaturesMixin):\n    \"\"\"Transforms covariate data into maxent-format feature data.\"\"\"\n\n    def __init__(\n        self,\n        feature_types: Union[str, list] = MaxentConfig.feature_types,\n        clamp: bool = MaxentConfig.clamp,\n        n_hinge_features: int = MaxentConfig.n_hinge_features,\n        n_threshold_features: int = MaxentConfig.n_threshold_features,\n    ):\n        \"\"\"Computes features based on the maxent feature types specified (like linear, quadratic, hinge).\n\n        Args:\n            feature_types: list of maxent features to generate.\n            clamp: set feature values to global mins/maxs during prediction\n            n_hinge_features: number of hinge knots to generate\n            n_threshold_features: nuber of threshold features to generate\n        \"\"\"\n        self.feature_types = feature_types\n        self.clamp = clamp\n        self.n_hinge_features = n_hinge_features\n        self.n_threshold_features = n_threshold_features\n        self.categorical_ = None\n        self.continuous_ = None\n        self.categorical_pd_ = None\n        self.continuous_pd_ = None\n        self.labels_ = None\n        self.feature_names_ = None\n        self.estimators_ = {\n            \"linear\": None,\n            \"quadratic\": None,\n            \"product\": None,\n            \"threshold\": None,\n            \"hinge\": None,\n            \"categorical\": None,\n        }\n\n    def fit(self, x: ArrayLike, categorical: list = None, labels: list = None) -&gt; \"MaxentFeatureTransformer\":\n        \"\"\"Compute the minimum and maximum for scaling.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                The data used to compute the per-feature minimum and maximum\n                used for later scaling along the features axis.\n            categorical: indices indicating which x columns are categorical\n            labels: covariate column labels. ignored if x is a pandas DataFrame\n\n        Returns:\n            self. Returns the transformer with fitted parameters.\n        \"\"\"\n        self.feature_types = validate_feature_types(self.feature_types)\n        self.clamp = validate_boolean(self.clamp)\n        self.n_hinge_features = validate_numeric_scalar(self.n_hinge_features)\n        self.n_threshold_features = validate_numeric_scalar(self.n_threshold_features)\n\n        self._format_labels_and_dtypes(x, categorical=categorical, labels=labels)\n        con, cat = self._format_covariate_data(x)\n        nrows, ncols = con.shape\n\n        feature_names = []\n        if \"linear\" in self.feature_types:\n            estimator = LinearTransformer(clamp=self.clamp)\n            estimator.fit(con)\n            self.estimators_[\"linear\"] = estimator\n            feature_names += [\"linear\"] * estimator.n_features_in_\n\n        if \"quadratic\" in self.feature_types:\n            estimator = QuadraticTransformer(clamp=self.clamp)\n            estimator.fit(con)\n            self.estimators_[\"quadratic\"] = estimator\n            feature_names += [\"quadratic\"] * estimator.estimator.n_features_in_\n\n        if \"product\" in self.feature_types:\n            estimator = ProductTransformer(clamp=self.clamp)\n            estimator.fit(con)\n            self.estimators_[\"product\"] = estimator\n            feature_names += [\"product\"] * estimator.estimator.n_features_in_\n\n        if \"threshold\" in self.feature_types:\n            estimator = ThresholdTransformer(n_thresholds=self.n_threshold_features)\n            estimator.fit(con)\n            self.estimators_[\"threshold\"] = estimator\n            feature_names += [\"threshold\"] * (estimator.n_thresholds * ncols)\n\n        if \"hinge\" in self.feature_types:\n            estimator = HingeTransformer(n_hinges=self.n_hinge_features)\n            estimator.fit(con)\n            self.estimators_[\"hinge\"] = estimator\n            feature_names += [\"hinge\"] * ((estimator.n_hinges - 1) * 2 * ncols)\n\n        if cat is not None:\n            estimator = CategoricalTransformer()\n            estimator.fit(cat)\n            self.estimators_[\"categorical\"] = estimator\n            for est in estimator.estimators_:\n                feature_names += [\"categorical\"] * len(est.categories_[0])\n\n        self.feature_names_ = feature_names\n\n        return self\n\n    def transform(self, x: ArrayLike) -&gt; np.ndarray:\n        \"\"\"Scale covariates according to the feature range.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                Input data that will be transformed.\n\n        Returns:\n            ndarray with transformed data.\n        \"\"\"\n        con, cat = self._format_covariate_data(x)\n        features = []\n\n        if \"linear\" in self.feature_types:\n            features.append(self.estimators_[\"linear\"].transform(con))\n\n        if \"quadratic\" in self.feature_types:\n            features.append(self.estimators_[\"quadratic\"].transform(con))\n\n        if \"product\" in self.feature_types:\n            features.append(self.estimators_[\"product\"].transform(con))\n\n        if \"threshold\" in self.feature_types:\n            features.append(self.estimators_[\"threshold\"].transform(con))\n\n        if \"hinge\" in self.feature_types:\n            features.append(self.estimators_[\"hinge\"].transform(con))\n\n        if cat is not None:\n            features.append(self.estimators_[\"categorical\"].transform(cat))\n\n        return np.concatenate(features, axis=1)\n\n    def fit_transform(self, x: ArrayLike, categorical: list = None, labels: list = None) -&gt; np.ndarray:\n        \"\"\"Fits scaler to x and returns transformed features.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                Input data to fit the scaler and to transform.\n\n        Returns:\n            ndarray with transformed data.\n        \"\"\"\n        self.fit(x, categorical=categorical, labels=labels)\n        return self.transform(x)\n</code></pre>"},{"location":"module/features/#elapid.features.MaxentFeatureTransformer.__init__","title":"<code>__init__(feature_types=MaxentConfig.feature_types, clamp=MaxentConfig.clamp, n_hinge_features=MaxentConfig.n_hinge_features, n_threshold_features=MaxentConfig.n_threshold_features)</code>","text":"<p>Computes features based on the maxent feature types specified (like linear, quadratic, hinge).</p> <p>Parameters:</p> Name Type Description Default <code>feature_types</code> <code>Union[str, list]</code> <p>list of maxent features to generate.</p> <code>feature_types</code> <code>clamp</code> <code>bool</code> <p>set feature values to global mins/maxs during prediction</p> <code>clamp</code> <code>n_hinge_features</code> <code>int</code> <p>number of hinge knots to generate</p> <code>n_hinge_features</code> <code>n_threshold_features</code> <code>int</code> <p>nuber of threshold features to generate</p> <code>n_threshold_features</code> Source code in <code>elapid/features.py</code> <pre><code>def __init__(\n    self,\n    feature_types: Union[str, list] = MaxentConfig.feature_types,\n    clamp: bool = MaxentConfig.clamp,\n    n_hinge_features: int = MaxentConfig.n_hinge_features,\n    n_threshold_features: int = MaxentConfig.n_threshold_features,\n):\n    \"\"\"Computes features based on the maxent feature types specified (like linear, quadratic, hinge).\n\n    Args:\n        feature_types: list of maxent features to generate.\n        clamp: set feature values to global mins/maxs during prediction\n        n_hinge_features: number of hinge knots to generate\n        n_threshold_features: nuber of threshold features to generate\n    \"\"\"\n    self.feature_types = feature_types\n    self.clamp = clamp\n    self.n_hinge_features = n_hinge_features\n    self.n_threshold_features = n_threshold_features\n    self.categorical_ = None\n    self.continuous_ = None\n    self.categorical_pd_ = None\n    self.continuous_pd_ = None\n    self.labels_ = None\n    self.feature_names_ = None\n    self.estimators_ = {\n        \"linear\": None,\n        \"quadratic\": None,\n        \"product\": None,\n        \"threshold\": None,\n        \"hinge\": None,\n        \"categorical\": None,\n    }\n</code></pre>"},{"location":"module/features/#elapid.features.MaxentFeatureTransformer.fit","title":"<code>fit(x, categorical=None, labels=None)</code>","text":"<p>Compute the minimum and maximum for scaling.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.</p> required <code>categorical</code> <code>list</code> <p>indices indicating which x columns are categorical</p> <code>None</code> <code>labels</code> <code>list</code> <p>covariate column labels. ignored if x is a pandas DataFrame</p> <code>None</code> <p>Returns:</p> Type Description <code>MaxentFeatureTransformer</code> <p>self. Returns the transformer with fitted parameters.</p> Source code in <code>elapid/features.py</code> <pre><code>def fit(self, x: ArrayLike, categorical: list = None, labels: list = None) -&gt; \"MaxentFeatureTransformer\":\n    \"\"\"Compute the minimum and maximum for scaling.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            The data used to compute the per-feature minimum and maximum\n            used for later scaling along the features axis.\n        categorical: indices indicating which x columns are categorical\n        labels: covariate column labels. ignored if x is a pandas DataFrame\n\n    Returns:\n        self. Returns the transformer with fitted parameters.\n    \"\"\"\n    self.feature_types = validate_feature_types(self.feature_types)\n    self.clamp = validate_boolean(self.clamp)\n    self.n_hinge_features = validate_numeric_scalar(self.n_hinge_features)\n    self.n_threshold_features = validate_numeric_scalar(self.n_threshold_features)\n\n    self._format_labels_and_dtypes(x, categorical=categorical, labels=labels)\n    con, cat = self._format_covariate_data(x)\n    nrows, ncols = con.shape\n\n    feature_names = []\n    if \"linear\" in self.feature_types:\n        estimator = LinearTransformer(clamp=self.clamp)\n        estimator.fit(con)\n        self.estimators_[\"linear\"] = estimator\n        feature_names += [\"linear\"] * estimator.n_features_in_\n\n    if \"quadratic\" in self.feature_types:\n        estimator = QuadraticTransformer(clamp=self.clamp)\n        estimator.fit(con)\n        self.estimators_[\"quadratic\"] = estimator\n        feature_names += [\"quadratic\"] * estimator.estimator.n_features_in_\n\n    if \"product\" in self.feature_types:\n        estimator = ProductTransformer(clamp=self.clamp)\n        estimator.fit(con)\n        self.estimators_[\"product\"] = estimator\n        feature_names += [\"product\"] * estimator.estimator.n_features_in_\n\n    if \"threshold\" in self.feature_types:\n        estimator = ThresholdTransformer(n_thresholds=self.n_threshold_features)\n        estimator.fit(con)\n        self.estimators_[\"threshold\"] = estimator\n        feature_names += [\"threshold\"] * (estimator.n_thresholds * ncols)\n\n    if \"hinge\" in self.feature_types:\n        estimator = HingeTransformer(n_hinges=self.n_hinge_features)\n        estimator.fit(con)\n        self.estimators_[\"hinge\"] = estimator\n        feature_names += [\"hinge\"] * ((estimator.n_hinges - 1) * 2 * ncols)\n\n    if cat is not None:\n        estimator = CategoricalTransformer()\n        estimator.fit(cat)\n        self.estimators_[\"categorical\"] = estimator\n        for est in estimator.estimators_:\n            feature_names += [\"categorical\"] * len(est.categories_[0])\n\n    self.feature_names_ = feature_names\n\n    return self\n</code></pre>"},{"location":"module/features/#elapid.features.MaxentFeatureTransformer.fit_transform","title":"<code>fit_transform(x, categorical=None, labels=None)</code>","text":"<p>Fits scaler to x and returns transformed features.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ndarray with transformed data.</p> Source code in <code>elapid/features.py</code> <pre><code>def fit_transform(self, x: ArrayLike, categorical: list = None, labels: list = None) -&gt; np.ndarray:\n    \"\"\"Fits scaler to x and returns transformed features.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            Input data to fit the scaler and to transform.\n\n    Returns:\n        ndarray with transformed data.\n    \"\"\"\n    self.fit(x, categorical=categorical, labels=labels)\n    return self.transform(x)\n</code></pre>"},{"location":"module/features/#elapid.features.MaxentFeatureTransformer.transform","title":"<code>transform(x)</code>","text":"<p>Scale covariates according to the feature range.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) Input data that will be transformed.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ndarray with transformed data.</p> Source code in <code>elapid/features.py</code> <pre><code>def transform(self, x: ArrayLike) -&gt; np.ndarray:\n    \"\"\"Scale covariates according to the feature range.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            Input data that will be transformed.\n\n    Returns:\n        ndarray with transformed data.\n    \"\"\"\n    con, cat = self._format_covariate_data(x)\n    features = []\n\n    if \"linear\" in self.feature_types:\n        features.append(self.estimators_[\"linear\"].transform(con))\n\n    if \"quadratic\" in self.feature_types:\n        features.append(self.estimators_[\"quadratic\"].transform(con))\n\n    if \"product\" in self.feature_types:\n        features.append(self.estimators_[\"product\"].transform(con))\n\n    if \"threshold\" in self.feature_types:\n        features.append(self.estimators_[\"threshold\"].transform(con))\n\n    if \"hinge\" in self.feature_types:\n        features.append(self.estimators_[\"hinge\"].transform(con))\n\n    if cat is not None:\n        features.append(self.estimators_[\"categorical\"].transform(cat))\n\n    return np.concatenate(features, axis=1)\n</code></pre>"},{"location":"module/features/#elapid.features.ProductTransformer","title":"<code>ProductTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Computes the column-wise product of an array of input features, rescaling from 0-1.</p> Source code in <code>elapid/features.py</code> <pre><code>class ProductTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Computes the column-wise product of an array of input features, rescaling from 0-1.\"\"\"\n\n    def __init__(\n        self,\n        clamp: bool = MaxentConfig.clamp,\n        feature_range: Tuple[float, float] = (0.0, 1.0),\n    ):\n        self.clamp = clamp\n        self.feature_range = feature_range\n        self.estimator = None\n\n    def fit(self, x: ArrayLike) -&gt; \"ProductTransformer\":\n        \"\"\"Compute the minimum and maximum for scaling.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                The data used to compute the per-feature minimum and maximum\n                used for later scaling along the features axis.\n\n        Returns:\n            self. Returns the transformer with fitted parameters.\n        \"\"\"\n        self.estimator = MinMaxScaler(clip=self.clamp, feature_range=self.feature_range)\n        self.estimator.fit(column_product(np.array(x)))\n\n        return self\n\n    def transform(self, x: ArrayLike) -&gt; np.ndarray:\n        \"\"\"Scale covariates according to the feature range.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                Input data that will be transformed.\n\n        Returns:\n            ndarray with transformed data.\n        \"\"\"\n        return self.estimator.transform(column_product(np.array(x)))\n\n    def fit_transform(self, x: ArrayLike) -&gt; np.ndarray:\n        \"\"\"Fits scaler to x and returns transformed features.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                Input data to fit the scaler and to transform.\n\n        Returns:\n            ndarray with transformed data.\n        \"\"\"\n        self.fit(x)\n        return self.transform(x)\n</code></pre>"},{"location":"module/features/#elapid.features.ProductTransformer.fit","title":"<code>fit(x)</code>","text":"<p>Compute the minimum and maximum for scaling.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.</p> required <p>Returns:</p> Type Description <code>ProductTransformer</code> <p>self. Returns the transformer with fitted parameters.</p> Source code in <code>elapid/features.py</code> <pre><code>def fit(self, x: ArrayLike) -&gt; \"ProductTransformer\":\n    \"\"\"Compute the minimum and maximum for scaling.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            The data used to compute the per-feature minimum and maximum\n            used for later scaling along the features axis.\n\n    Returns:\n        self. Returns the transformer with fitted parameters.\n    \"\"\"\n    self.estimator = MinMaxScaler(clip=self.clamp, feature_range=self.feature_range)\n    self.estimator.fit(column_product(np.array(x)))\n\n    return self\n</code></pre>"},{"location":"module/features/#elapid.features.ProductTransformer.fit_transform","title":"<code>fit_transform(x)</code>","text":"<p>Fits scaler to x and returns transformed features.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ndarray with transformed data.</p> Source code in <code>elapid/features.py</code> <pre><code>def fit_transform(self, x: ArrayLike) -&gt; np.ndarray:\n    \"\"\"Fits scaler to x and returns transformed features.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            Input data to fit the scaler and to transform.\n\n    Returns:\n        ndarray with transformed data.\n    \"\"\"\n    self.fit(x)\n    return self.transform(x)\n</code></pre>"},{"location":"module/features/#elapid.features.ProductTransformer.transform","title":"<code>transform(x)</code>","text":"<p>Scale covariates according to the feature range.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) Input data that will be transformed.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ndarray with transformed data.</p> Source code in <code>elapid/features.py</code> <pre><code>def transform(self, x: ArrayLike) -&gt; np.ndarray:\n    \"\"\"Scale covariates according to the feature range.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            Input data that will be transformed.\n\n    Returns:\n        ndarray with transformed data.\n    \"\"\"\n    return self.estimator.transform(column_product(np.array(x)))\n</code></pre>"},{"location":"module/features/#elapid.features.QuadraticTransformer","title":"<code>QuadraticTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Applies quadtratic feature transformations and rescales features from 0-1.</p> Source code in <code>elapid/features.py</code> <pre><code>class QuadraticTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Applies quadtratic feature transformations and rescales features from 0-1.\"\"\"\n\n    def __init__(\n        self,\n        clamp: bool = MaxentConfig.clamp,\n        feature_range: Tuple[float, float] = (0.0, 1.0),\n    ):\n        self.clamp = clamp\n        self.feature_range = feature_range\n        self.estimator = None\n\n    def fit(self, x: ArrayLike) -&gt; \"QuadraticTransformer\":\n        \"\"\"Compute the minimum and maximum for scaling.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                The data used to compute the per-feature minimum and maximum\n                used for later scaling along the features axis.\n\n        Returns:\n            self. Returns the transformer with fitted parameters.\n        \"\"\"\n        self.estimator = MinMaxScaler(clip=self.clamp, feature_range=self.feature_range)\n        self.estimator.fit(np.array(x) ** 2)\n\n        return self\n\n    def transform(self, x: ArrayLike) -&gt; np.ndarray:\n        \"\"\"Scale covariates according to the feature range.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                Input data that will be transformed.\n\n        Returns:\n            ndarray with transformed data.\n        \"\"\"\n        return self.estimator.transform(np.array(x) ** 2)\n\n    def fit_transform(self, x: ArrayLike) -&gt; np.ndarray:\n        \"\"\"Fits scaler to x and returns transformed features.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                Input data to fit the scaler and to transform.\n\n        Returns:\n            ndarray with transformed data.\n        \"\"\"\n        self.fit(x)\n        return self.estimator.transform(np.array(x) ** 2)\n\n    def inverse_transform(self, x: ArrayLike) -&gt; np.ndarray:\n        \"\"\"Revert from transformed features to original covariate values.\n\n        Args:\n            x: array-like of shape (n_xamples, n_features)\n                Transformed feature data to convert to covariate data.\n\n        Returns:\n            ndarray with unscaled covariate values.\n        \"\"\"\n        return self.estimator.inverse_transform(np.array(x)) ** 0.5\n</code></pre>"},{"location":"module/features/#elapid.features.QuadraticTransformer.fit","title":"<code>fit(x)</code>","text":"<p>Compute the minimum and maximum for scaling.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.</p> required <p>Returns:</p> Type Description <code>QuadraticTransformer</code> <p>self. Returns the transformer with fitted parameters.</p> Source code in <code>elapid/features.py</code> <pre><code>def fit(self, x: ArrayLike) -&gt; \"QuadraticTransformer\":\n    \"\"\"Compute the minimum and maximum for scaling.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            The data used to compute the per-feature minimum and maximum\n            used for later scaling along the features axis.\n\n    Returns:\n        self. Returns the transformer with fitted parameters.\n    \"\"\"\n    self.estimator = MinMaxScaler(clip=self.clamp, feature_range=self.feature_range)\n    self.estimator.fit(np.array(x) ** 2)\n\n    return self\n</code></pre>"},{"location":"module/features/#elapid.features.QuadraticTransformer.fit_transform","title":"<code>fit_transform(x)</code>","text":"<p>Fits scaler to x and returns transformed features.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ndarray with transformed data.</p> Source code in <code>elapid/features.py</code> <pre><code>def fit_transform(self, x: ArrayLike) -&gt; np.ndarray:\n    \"\"\"Fits scaler to x and returns transformed features.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            Input data to fit the scaler and to transform.\n\n    Returns:\n        ndarray with transformed data.\n    \"\"\"\n    self.fit(x)\n    return self.estimator.transform(np.array(x) ** 2)\n</code></pre>"},{"location":"module/features/#elapid.features.QuadraticTransformer.inverse_transform","title":"<code>inverse_transform(x)</code>","text":"<p>Revert from transformed features to original covariate values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_xamples, n_features) Transformed feature data to convert to covariate data.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ndarray with unscaled covariate values.</p> Source code in <code>elapid/features.py</code> <pre><code>def inverse_transform(self, x: ArrayLike) -&gt; np.ndarray:\n    \"\"\"Revert from transformed features to original covariate values.\n\n    Args:\n        x: array-like of shape (n_xamples, n_features)\n            Transformed feature data to convert to covariate data.\n\n    Returns:\n        ndarray with unscaled covariate values.\n    \"\"\"\n    return self.estimator.inverse_transform(np.array(x)) ** 0.5\n</code></pre>"},{"location":"module/features/#elapid.features.QuadraticTransformer.transform","title":"<code>transform(x)</code>","text":"<p>Scale covariates according to the feature range.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) Input data that will be transformed.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ndarray with transformed data.</p> Source code in <code>elapid/features.py</code> <pre><code>def transform(self, x: ArrayLike) -&gt; np.ndarray:\n    \"\"\"Scale covariates according to the feature range.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            Input data that will be transformed.\n\n    Returns:\n        ndarray with transformed data.\n    \"\"\"\n    return self.estimator.transform(np.array(x) ** 2)\n</code></pre>"},{"location":"module/features/#elapid.features.ThresholdTransformer","title":"<code>ThresholdTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Apply binary thresholds across evenly-spaced bins for each covariate.</p> Source code in <code>elapid/features.py</code> <pre><code>class ThresholdTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Apply binary thresholds across evenly-spaced bins for each covariate.\"\"\"\n\n    def __init__(self, n_thresholds: int = MaxentConfig.n_threshold_features):\n        self.n_thresholds = n_thresholds\n        self.mins_ = None\n        self.maxs_ = None\n        self.threshold_indices_ = None\n\n    def fit(self, x: ArrayLike) -&gt; \"ThresholdTransformer\":\n        \"\"\"Compute the minimum and maximum for scaling.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                The data used to compute the per-feature minimum and maximum\n                used for later scaling along the features axis.\n\n        Returns:\n            self. Returns the transformer with fitted parameters.\n        \"\"\"\n        x = np.array(x)\n        self.mins_ = x.min(axis=0)\n        self.maxs_ = x.max(axis=0)\n        self.threshold_indices_ = np.linspace(self.mins_, self.maxs_, self.n_thresholds)\n\n        return self\n\n    def transform(self, x: ArrayLike) -&gt; np.ndarray:\n        \"\"\"Scale covariates according to the feature range.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                Input data that will be transformed.\n\n        Returns:\n            ndarray with transformed data.\n        \"\"\"\n        x = np.array(x)\n        xarr = repeat_array(x, len(self.threshold_indices_), axis=-1)\n        tarr = repeat_array(self.threshold_indices_.transpose(), len(x), axis=0)\n        thresh = (xarr &gt; tarr).reshape(x.shape[0], -1)\n        return thresh.astype(np.uint8)\n\n    def fit_transform(self, x: ArrayLike) -&gt; np.ndarray:\n        \"\"\"Fits scaler to x and returns transformed features.\n\n        Args:\n            x: array-like of shape (n_samples, n_features)\n                Input data to fit the scaler and to transform.\n\n        Returns:\n            ndarray with transformed data.\n        \"\"\"\n        self.fit(x)\n        return self.transform(x)\n</code></pre>"},{"location":"module/features/#elapid.features.ThresholdTransformer.fit","title":"<code>fit(x)</code>","text":"<p>Compute the minimum and maximum for scaling.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.</p> required <p>Returns:</p> Type Description <code>ThresholdTransformer</code> <p>self. Returns the transformer with fitted parameters.</p> Source code in <code>elapid/features.py</code> <pre><code>def fit(self, x: ArrayLike) -&gt; \"ThresholdTransformer\":\n    \"\"\"Compute the minimum and maximum for scaling.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            The data used to compute the per-feature minimum and maximum\n            used for later scaling along the features axis.\n\n    Returns:\n        self. Returns the transformer with fitted parameters.\n    \"\"\"\n    x = np.array(x)\n    self.mins_ = x.min(axis=0)\n    self.maxs_ = x.max(axis=0)\n    self.threshold_indices_ = np.linspace(self.mins_, self.maxs_, self.n_thresholds)\n\n    return self\n</code></pre>"},{"location":"module/features/#elapid.features.ThresholdTransformer.fit_transform","title":"<code>fit_transform(x)</code>","text":"<p>Fits scaler to x and returns transformed features.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) Input data to fit the scaler and to transform.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ndarray with transformed data.</p> Source code in <code>elapid/features.py</code> <pre><code>def fit_transform(self, x: ArrayLike) -&gt; np.ndarray:\n    \"\"\"Fits scaler to x and returns transformed features.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            Input data to fit the scaler and to transform.\n\n    Returns:\n        ndarray with transformed data.\n    \"\"\"\n    self.fit(x)\n    return self.transform(x)\n</code></pre>"},{"location":"module/features/#elapid.features.ThresholdTransformer.transform","title":"<code>transform(x)</code>","text":"<p>Scale covariates according to the feature range.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) Input data that will be transformed.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ndarray with transformed data.</p> Source code in <code>elapid/features.py</code> <pre><code>def transform(self, x: ArrayLike) -&gt; np.ndarray:\n    \"\"\"Scale covariates according to the feature range.\n\n    Args:\n        x: array-like of shape (n_samples, n_features)\n            Input data that will be transformed.\n\n    Returns:\n        ndarray with transformed data.\n    \"\"\"\n    x = np.array(x)\n    xarr = repeat_array(x, len(self.threshold_indices_), axis=-1)\n    tarr = repeat_array(self.threshold_indices_.transpose(), len(x), axis=0)\n    thresh = (xarr &gt; tarr).reshape(x.shape[0], -1)\n    return thresh.astype(np.uint8)\n</code></pre>"},{"location":"module/features/#elapid.features.column_product","title":"<code>column_product(array)</code>","text":"<p>Computes the column-wise product of a 2D array.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>array-like of shape (n_samples, n_features)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ndarray with of shape (n_samples, factorial(n_features-1))</p> Source code in <code>elapid/features.py</code> <pre><code>def column_product(array: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Computes the column-wise product of a 2D array.\n\n    Args:\n        array: array-like of shape (n_samples, n_features)\n\n    Returns:\n        ndarray with of shape (n_samples, factorial(n_features-1))\n    \"\"\"\n    nrows, ncols = array.shape\n\n    if ncols == 1:\n        return array\n    else:\n        products = []\n        for xstart in range(0, ncols - 1):\n            products.append(array[:, xstart].reshape(nrows, 1) * array[:, xstart + 1 :])\n        return np.concatenate(products, axis=1)\n</code></pre>"},{"location":"module/features/#elapid.features.compute_lambdas","title":"<code>compute_lambdas(y, weights, reg, n_lambdas=MaxentConfig.n_lambdas)</code>","text":"<p>Computes lambda parameter values for elastic lasso fits.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ArrayLike</code> <p>array-like of shape (n_samples,) with binary presence/background (1/0) values</p> required <code>weights</code> <code>ArrayLike</code> <p>per-sample model weights</p> required <code>reg</code> <code>ArrayLike</code> <p>per-feature regularization coefficients</p> required <code>n_lambdas</code> <code>int</code> <p>number of lambda values to estimate</p> <code>n_lambdas</code> <p>Returns:</p> Name Type Description <code>lambdas</code> <code>ndarray</code> <p>Array of lambda scores of length n_lambda</p> Source code in <code>elapid/features.py</code> <pre><code>def compute_lambdas(\n    y: ArrayLike, weights: ArrayLike, reg: ArrayLike, n_lambdas: int = MaxentConfig.n_lambdas\n) -&gt; np.ndarray:\n    \"\"\"Computes lambda parameter values for elastic lasso fits.\n\n    Args:\n        y: array-like of shape (n_samples,) with binary presence/background (1/0) values\n        weights: per-sample model weights\n        reg: per-feature regularization coefficients\n        n_lambdas: number of lambda values to estimate\n\n    Returns:\n        lambdas: Array of lambda scores of length n_lambda\n    \"\"\"\n    n_presence = np.sum(y)\n    mean_regularization = np.mean(reg)\n    total_weight = np.sum(weights)\n    seed_range = np.linspace(4, 0, n_lambdas)\n    lambdas = 10 ** (seed_range) * mean_regularization * (n_presence / total_weight)\n\n    return lambdas\n</code></pre>"},{"location":"module/features/#elapid.features.compute_regularization","title":"<code>compute_regularization(y, z, feature_labels, beta_multiplier=MaxentConfig.beta_multiplier, beta_lqp=MaxentConfig.beta_lqp, beta_threshold=MaxentConfig.beta_threshold, beta_hinge=MaxentConfig.beta_hinge, beta_categorical=MaxentConfig.beta_hinge)</code>","text":"<p>Computes variable regularization values for all feature data.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ArrayLike</code> <p>array-like of shape (n_samples,) with binary presence/background (1/0) values</p> required <code>z</code> <code>ndarray</code> <p>model features (transformations applied to covariates)</p> required <code>feature_labels</code> <code>List[str]</code> <p>list of length n_features, with labels identifying each column's feature type with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"]</p> required <code>beta_multiplier</code> <code>float</code> <p>scaler for all regularization parameters. higher values exclude more features</p> <code>beta_multiplier</code> <code>beta_lqp</code> <code>float</code> <p>scaler for linear, quadratic and product feature regularization</p> <code>beta_lqp</code> <code>beta_threshold</code> <code>float</code> <p>scaler for threshold feature regularization</p> <code>beta_threshold</code> <code>beta_hinge</code> <code>float</code> <p>scaler for hinge feature regularization</p> <code>beta_hinge</code> <code>beta_categorical</code> <code>float</code> <p>scaler for categorical feature regularization</p> <code>beta_hinge</code> <p>Returns:</p> Name Type Description <code>max_reg</code> <code>ndarray</code> <p>Array with per-feature regularization parameters</p> Source code in <code>elapid/features.py</code> <pre><code>def compute_regularization(\n    y: ArrayLike,\n    z: np.ndarray,\n    feature_labels: List[str],\n    beta_multiplier: float = MaxentConfig.beta_multiplier,\n    beta_lqp: float = MaxentConfig.beta_lqp,\n    beta_threshold: float = MaxentConfig.beta_threshold,\n    beta_hinge: float = MaxentConfig.beta_hinge,\n    beta_categorical: float = MaxentConfig.beta_hinge,\n) -&gt; np.ndarray:\n    \"\"\"Computes variable regularization values for all feature data.\n\n    Args:\n        y: array-like of shape (n_samples,) with binary presence/background (1/0) values\n        z: model features (transformations applied to covariates)\n        feature_labels: list of length n_features, with labels identifying each column's feature type\n            with options [\"linear\", \"quadratic\", \"product\", \"threshold\", \"hinge\", \"categorical\"]\n        beta_multiplier: scaler for all regularization parameters. higher values exclude more features\n        beta_lqp: scaler for linear, quadratic and product feature regularization\n        beta_threshold: scaler for threshold feature regularization\n        beta_hinge: scaler for hinge feature regularization\n        beta_categorical: scaler for categorical feature regularization\n\n    Returns:\n        max_reg: Array with per-feature regularization parameters\n    \"\"\"\n    # compute regularization based on presence-only locations\n    z1 = z[y == 1]\n    nrows, ncols = z1.shape\n    labels = np.array(feature_labels)\n    nlabels = len(feature_labels)\n\n    assert nlabels == ncols, f\"number of feature_labels ({nlabels}) must match number of features ({ncols})\"\n\n    # create arrays to store the regularization params\n    base_regularization = np.zeros(ncols)\n    hinge_regularization = np.zeros(ncols)\n    threshold_regularization = np.zeros(ncols)\n\n    # use a different reg table based on the features set\n    if \"product\" in labels:\n        table_lqp = RegularizationConfig.product\n    elif \"quadratic\" in labels:\n        table_lqp = RegularizationConfig.quadratic\n    else:\n        table_lqp = RegularizationConfig.linear\n\n    if \"linear\" in labels:\n        linear_idxs = labels == \"linear\"\n        fr_max, fr_min = table_lqp\n        multiplier = beta_lqp\n        ap = np.interp(nrows, fr_max, fr_min)\n        reg = multiplier * ap / np.sqrt(nrows)\n        base_regularization[linear_idxs] = reg\n\n    if \"quadratic\" in labels:\n        quadratic_idxs = labels == \"quadratic\"\n        fr_max, fr_min = table_lqp\n        multiplier = beta_lqp\n        ap = np.interp(nrows, fr_max, fr_min)\n        reg = multiplier * ap / np.sqrt(nrows)\n        base_regularization[quadratic_idxs] = reg\n\n    if \"product\" in labels:\n        product_idxs = labels == \"product\"\n        fr_max, fr_min = table_lqp\n        multiplier = beta_lqp\n        ap = np.interp(nrows, fr_max, fr_min)\n        reg = multiplier * ap / np.sqrt(nrows)\n        base_regularization[product_idxs] = reg\n\n    if \"threshold\" in labels:\n        threshold_idxs = labels == \"threshold\"\n        fr_max, fr_min = RegularizationConfig.threshold\n        multiplier = beta_threshold\n        ap = np.interp(nrows, fr_max, fr_min)\n        reg = multiplier * ap / np.sqrt(nrows)\n        base_regularization[threshold_idxs] = reg\n\n        # increase regularization for uniform threshlold values\n        all_zeros = np.all(z1 == 0, axis=0)\n        all_ones = np.all(z1 == 1, axis=0)\n        threshold_regularization[all_zeros] = 1\n        threshold_regularization[all_ones] = 1\n\n    if \"hinge\" in labels:\n        hinge_idxs = labels == \"hinge\"\n        fr_max, fr_min = RegularizationConfig.hinge\n        multiplier = beta_hinge\n        ap = np.interp(nrows, fr_max, fr_min)\n        reg = multiplier * ap / np.sqrt(nrows)\n        base_regularization[hinge_idxs] = reg\n\n        # increase regularization for extreme hinge values\n        hinge_std = np.std(z1[:, hinge_idxs], ddof=1, axis=0)\n        hinge_sqrt = np.zeros(len(hinge_std)) + (1 / np.sqrt(nrows))\n        std = np.max((hinge_std, hinge_sqrt), axis=0)\n        hinge_regularization[hinge_idxs] = (0.5 * std) / np.sqrt(nrows)\n\n    if \"categorical\" in labels:\n        categorical_idxs = labels == \"categorical\"\n        fr_max, fr_min = RegularizationConfig.categorical\n        multiplier = beta_categorical\n        ap = np.interp(nrows, fr_max, fr_min)\n        reg = multiplier * ap / np.sqrt(nrows)\n        base_regularization[categorical_idxs] = reg\n\n    # compute the maximum regularization based on a few different approaches\n    default_regularization = 0.001 * (np.max(z, axis=0) - np.min(z, axis=0))\n    variance_regularization = np.std(z1, ddof=1, axis=0) * base_regularization\n    max_regularization = np.max(\n        (default_regularization, variance_regularization, hinge_regularization, threshold_regularization), axis=0\n    )\n\n    # apply the final scaling factor\n    max_regularization *= beta_multiplier\n\n    return max_regularization\n</code></pre>"},{"location":"module/features/#elapid.features.compute_weights","title":"<code>compute_weights(y, pbr=100)</code>","text":"<p>Compute Maxent-format per-sample model weights.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ArrayLike</code> <p>array-like of shape (n_samples,) with binary presence/background (1/0) values</p> required <code>pbr</code> <code>int</code> <p>presence-to-background weight ratio. pbr=100 sets background samples to 1/100 weight of presence samples.</p> <code>100</code> <p>Returns:</p> Name Type Description <code>weights</code> <code>ndarray</code> <p>array with glmnet-formatted sample weights</p> Source code in <code>elapid/features.py</code> <pre><code>def compute_weights(y: ArrayLike, pbr: int = 100) -&gt; np.ndarray:\n    \"\"\"Compute Maxent-format per-sample model weights.\n\n    Args:\n        y: array-like of shape (n_samples,) with binary presence/background (1/0) values\n        pbr: presence-to-background weight ratio. pbr=100 sets background samples to 1/100 weight of presence samples.\n\n    Returns:\n        weights: array with glmnet-formatted sample weights\n    \"\"\"\n    weights = np.array(y + (1 - y) * pbr)\n    return weights\n</code></pre>"},{"location":"module/features/#elapid.features.left_hinge","title":"<code>left_hinge(x, mn, mx)</code>","text":"<p>Computes hinge transformation values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>Array-like of covariate values</p> required <code>mn</code> <code>float</code> <p>Minimum covariate value to fit hinges to</p> required <code>mx</code> <code>float</code> <p>Maximum covariate value to fit hinges to</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of hinge features</p> Source code in <code>elapid/features.py</code> <pre><code>def left_hinge(x: ArrayLike, mn: float, mx: float) -&gt; np.ndarray:\n    \"\"\"Computes hinge transformation values.\n\n    Args:\n        x: Array-like of covariate values\n        mn: Minimum covariate value to fit hinges to\n        mx: Maximum covariate value to fit hinges to\n\n    Returns:\n        Array of hinge features\n    \"\"\"\n    rng = repeat_array(mx, mn.shape[-1], axis=1) - mn\n    valid = rng &gt; 0\n    hinge = np.clip(np.divide((x - mn), rng, where=valid), 0, 1)\n\n    return hinge\n</code></pre>"},{"location":"module/features/#elapid.features.right_hinge","title":"<code>right_hinge(x, mn, mx)</code>","text":"<p>Computes hinge transformation values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>Array-like of covariate values</p> required <code>mn</code> <code>float</code> <p>Minimum covariate value to fit hinges to</p> required <code>mx</code> <code>float</code> <p>Maximum covariate value to fit hinges to</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of hinge features</p> Source code in <code>elapid/features.py</code> <pre><code>def right_hinge(x: ArrayLike, mn: float, mx: float) -&gt; np.ndarray:\n    \"\"\"Computes hinge transformation values.\n\n    Args:\n        x: Array-like of covariate values\n        mn: Minimum covariate value to fit hinges to\n        mx: Maximum covariate value to fit hinges to\n\n    Returns:\n        Array of hinge features\n    \"\"\"\n    mnr = repeat_array(mn, mx.shape[-1], axis=1)\n    rng = mx - mnr\n    valid = rng &gt; 0\n    hinge = np.clip(np.divide((x - mnr), rng, where=valid), 0, 1)\n\n    return hinge\n</code></pre>"},{"location":"module/geo/","title":"elapid.geo","text":"<p>Geospatial data operations like reading/writing/indexing raster and vector data.</p>"},{"location":"module/geo/#elapid.geo.annotate","title":"<code>annotate(points, raster_paths, labels=None, drop_na=True, quiet=False)</code>","text":"<p>Read raster values for each point in a vector and append as new columns.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Union[str, GeoSeries, GeoDataFrame]</code> <p>path to a point-format vector, OR GeoDataFrame with point locations, OR GeoSeries (e.g., gdf['geometry']) with point locations</p> required <code>raster_paths</code> <code>Union[str, list]</code> <p>raster paths to extract pixel values from.</p> required <code>labels</code> <code>list</code> <p>band name labels. number of labels should match the total number of bands across all raster_paths.</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>drop all records with no-data values.</p> <code>True</code> <code>quiet</code> <code>bool</code> <p>silence progress bar output.</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame annotated with the pixel values from each raster</p> Source code in <code>elapid/geo.py</code> <pre><code>def annotate(\n    points: Union[str, gpd.GeoSeries, gpd.GeoDataFrame],\n    raster_paths: Union[str, list],\n    labels: list = None,\n    drop_na: bool = True,\n    quiet: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Read raster values for each point in a vector and append as new columns.\n\n    Args:\n        points: path to a point-format vector, OR\n            GeoDataFrame with point locations, OR\n            GeoSeries (e.g., gdf['geometry']) with point locations\n        raster_paths: raster paths to extract pixel values from.\n        labels: band name labels. number of labels should match the\n            total number of bands across all raster_paths.\n        drop_na: drop all records with no-data values.\n        quiet: silence progress bar output.\n\n    Returns:\n        GeoDataFrame annotated with the pixel values from each raster\n    \"\"\"\n    # format the inputs\n    raster_paths = to_iterable(raster_paths)\n    labels = format_band_labels(raster_paths, labels)\n\n    # read raster values based on the points dtype\n    if isinstance(points, gpd.GeoSeries):\n        points = points.reset_index(drop=True)\n        gdf = annotate_geoseries(\n            points,\n            raster_paths,\n            labels=labels,\n            drop_na=drop_na,\n            quiet=quiet,\n        )\n\n    elif isinstance(points, gpd.GeoDataFrame) or isinstance(points, pd.DataFrame):\n        points = points.reset_index(drop=True)\n        gdf = annotate_geoseries(\n            points.geometry,\n            raster_paths,\n            labels=labels,\n            drop_na=drop_na,\n            quiet=quiet,\n        )\n\n        # append annotations to the input dataframe\n        gdf = pd.concat([points, gdf.drop([\"geometry\"], axis=1, errors=\"ignore\")], axis=1)\n\n    elif os.path.isfile(points):\n        gdf = annotate_vector(points, raster_paths, labels=labels, drop_na=drop_na, quiet=quiet)\n\n    else:\n        raise TypeError(\"points arg must be a valid path, GeoDataFrame, or GeoSeries\")\n\n    if drop_na:\n        try:\n            valid = gdf[\"valid\"] == 1\n            gdf = gdf[valid].drop(columns=\"valid\").dropna().reset_index(drop=True)\n        except KeyError:\n            pass\n\n    return gdf\n</code></pre>"},{"location":"module/geo/#elapid.geo.annotate_geoseries","title":"<code>annotate_geoseries(points, raster_paths, labels=None, drop_na=True, dtype=None, quiet=False)</code>","text":"<p>Reads and stores pixel values from rasters using point locations.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>GeoSeries</code> <p>GeoSeries with point locations.</p> required <code>raster_paths</code> <code>list</code> <p>rasters to extract pixel values from.</p> required <code>labels</code> <code>list</code> <p>band labels. must match the total number of bands for all raster_paths.</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>drop records with no-data values.</p> <code>True</code> <code>dtype</code> <code>str</code> <p>output column data type. uses the first raster's dtype by default.</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>silence progress bar output.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>gdf</code> <code>(GeoDataFrame, ndarray)</code> <p>GeoDataFrame annotated with the pixel values from each raster</p> Source code in <code>elapid/geo.py</code> <pre><code>def annotate_geoseries(\n    points: gpd.GeoSeries,\n    raster_paths: list,\n    labels: list = None,\n    drop_na: bool = True,\n    dtype: str = None,\n    quiet: bool = False,\n) -&gt; (gpd.GeoDataFrame, np.ndarray):\n    \"\"\"Reads and stores pixel values from rasters using point locations.\n\n    Args:\n        points: GeoSeries with point locations.\n        raster_paths: rasters to extract pixel values from.\n        labels: band labels. must match the total number of bands for all raster_paths.\n        drop_na: drop records with no-data values.\n        dtype: output column data type. uses the first raster's dtype by default.\n        quiet: silence progress bar output.\n\n    Returns:\n        gdf: GeoDataFrame annotated with the pixel values from each raster\n    \"\"\"\n    # format the inputs\n    raster_paths = to_iterable(raster_paths)\n    labels = format_band_labels(raster_paths, labels)\n\n    # get the dataset dimensions\n    n_rasters = len(raster_paths)\n\n    # create arrays and flags for updating\n    raster_values = []\n    valid_idxs = []\n    nodata_flag = False\n\n    # annotate each point with the pixel values for each raster\n    for raster_idx, raster_path in tqdm(\n        enumerate(raster_paths), desc=\"Raster\", total=n_rasters, disable=quiet, **tqdm_opts\n    ):\n        with rio.open(raster_path, \"r\") as src:\n            # reproject points to match raster and convert to a dataframe\n            if not crs_match(points.crs, src.crs):\n                points = points.to_crs(src.crs)\n\n            # use the first rasters dtype for the output array if not set\n            if raster_idx == 0 and dtype is None:\n                dtype = src.dtypes[0]\n\n            # get the raster row/col indices for each point and the respective read windows\n            xys = [(point.x, point.y) for point in points]\n\n            # read each pixel value\n            n_points = len(points)\n            samples_iter = list(\n                tqdm(\n                    src.sample(xys, masked=False),\n                    desc=\"Sample\",\n                    total=n_points,\n                    leave=False,\n                    disable=quiet,\n                    **tqdm_opts,\n                )\n            )\n            samples = np.array(samples_iter, dtype=dtype)\n            raster_values.append(samples)\n\n            # identify nodata points to remove later\n            if drop_na and src.nodata is not None:\n                nodata_flag = True\n                valid_idxs.append(samples[:, 0] != src.nodata)\n\n    # merge the arrays from each raster\n    values = np.concatenate(raster_values, axis=1, dtype=dtype)\n\n    if nodata_flag:\n        valid = np.all(valid_idxs, axis=0).reshape(-1, 1)\n        values = np.concatenate([values, valid], axis=1, dtype=dtype)\n        labels.append(\"valid\")\n        # values = values[valid, :]\n        # points = points.iloc[valid]\n        # points.index = range(valid.sum())\n\n    # convert to a geodataframe\n    gdf = gpd.GeoDataFrame(values, geometry=points.geometry, columns=labels)\n\n    return gdf\n</code></pre>"},{"location":"module/geo/#elapid.geo.annotate_vector","title":"<code>annotate_vector(vector_path, raster_paths, labels=None, drop_na=True, quiet=False)</code>","text":"<p>Reads and stores pixel values from rasters using a point-format vector file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>path to a vector file (shp, geojson, etc)</p> required <code>raster_paths</code> <code>list</code> <p>raster paths to extract pixel values from</p> required <code>labels</code> <code>list</code> <p>band name labels. should match the total number of bands across all raster_paths</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>drop all records with no-data values</p> <code>True</code> <code>quiet</code> <code>bool</code> <p>silence progress bar output.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame annotated with the pixel values from each raster</p> Source code in <code>elapid/geo.py</code> <pre><code>def annotate_vector(\n    vector_path: str,\n    raster_paths: list,\n    labels: list = None,\n    drop_na: bool = True,\n    quiet: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Reads and stores pixel values from rasters using a point-format vector file.\n\n    Args:\n        vector_path: path to a vector file (shp, geojson, etc)\n        raster_paths: raster paths to extract pixel values from\n        labels: band name labels. should match the total number of bands across all raster_paths\n        drop_na: drop all records with no-data values\n        quiet: silence progress bar output.\n\n    Returns:\n        gdf: GeoDataFrame annotated with the pixel values from each raster\n    \"\"\"\n    # format the inputs\n    raster_paths = to_iterable(raster_paths)\n    labels = format_band_labels(raster_paths, labels)\n\n    gdf = gpd.read_file(vector_path)\n    raster_df = annotate_geoseries(gdf.geometry, raster_paths, labels, drop_na)\n    gdf = pd.concat([gdf, raster_df.drop([\"geometry\"], axis=1, errors=\"ignore\")], axis=1)\n\n    return gdf\n</code></pre>"},{"location":"module/geo/#elapid.geo.apply_model_to_array","title":"<code>apply_model_to_array(model, array, nodata, nodata_idx, count=1, dtype='float32', predict_proba=False, **kwargs)</code>","text":"<p>Applies a model to an array of covariates.</p> <p>Covariate array should be of shape (nbands, nrows, ncols).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseEstimator</code> <p>object with a <code>model.predict()</code> function</p> required <code>array</code> <code>ndarray</code> <p>array of shape (nbands, nrows, ncols) with pixel values</p> required <code>nodata</code> <code>float</code> <p>numeric nodata value to apply to the output array</p> required <code>nodata_idx</code> <code>int</code> <p>array of bools with shape (nbands, nrows, ncols) containing nodata locations</p> required <code>count</code> <code>int</code> <p>number of bands in the prediction output</p> <code>1</code> <code>dtype</code> <code>str</code> <p>prediction array dtype</p> <code>'float32'</code> <code>predict_proba</code> <code>bool</code> <p>use model.predict_proba() instead of model.predict()</p> <code>False</code> <code>**kwargs</code> <p>additonal keywords to pass to model.predict()</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ypred_window</code> <code>ndarray</code> <p>Array of shape (nrows, ncols) with model predictions</p> Source code in <code>elapid/geo.py</code> <pre><code>def apply_model_to_array(\n    model: BaseEstimator,\n    array: np.ndarray,\n    nodata: float,\n    nodata_idx: int,\n    count: int = 1,\n    dtype: str = \"float32\",\n    predict_proba: bool = False,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Applies a model to an array of covariates.\n\n    Covariate array should be of shape (nbands, nrows, ncols).\n\n    Args:\n        model: object with a `model.predict()` function\n        array: array of shape (nbands, nrows, ncols) with pixel values\n        nodata: numeric nodata value to apply to the output array\n        nodata_idx: array of bools with shape (nbands, nrows, ncols) containing nodata locations\n        count: number of bands in the prediction output\n        dtype: prediction array dtype\n        predict_proba: use model.predict_proba() instead of model.predict()\n        **kwargs: additonal keywords to pass to model.predict()\n\n    Returns:\n        ypred_window: Array of shape (nrows, ncols) with model predictions\n    \"\"\"\n    # only apply to valid pixels\n    valid = ~nodata_idx.any(axis=0)\n    covariates = array[:, valid].transpose()\n    ypred = model.predict(covariates, **kwargs) if not predict_proba else model.predict_proba(covariates, **kwargs)\n\n    # reshape to the original window size\n    rows, cols = valid.shape\n    ypred_window = np.zeros((count, rows, cols), dtype=dtype) + nodata\n    ypred_window[:, valid] = ypred.transpose()\n\n    return ypred_window\n</code></pre>"},{"location":"module/geo/#elapid.geo.apply_model_to_rasters","title":"<code>apply_model_to_rasters(model, raster_paths, output_path, resampling=rio.enums.Resampling.average, count=1, dtype='float32', nodata=-9999, driver='GTiff', compress='deflate', bigtiff=True, template_idx=0, windowed=True, predict_proba=False, ignore_sklearn=True, quiet=False, **kwargs)</code>","text":"<p>Applies a trained model to a list of raster datasets.</p> <p>The list and band order of the rasters must match the order of the covariates used to train the model. It reads each dataset block-by-block, applies the model, and writes gridded predictions. If the raster datasets are not consistent (different extents, resolutions, etc.), it wll re-project the data on the fly, with the grid size, extent and projection based on a 'template' raster.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseEstimator</code> <p>object with a model.predict() function</p> required <code>raster_paths</code> <code>list</code> <p>raster paths of covariates to apply the model to</p> required <code>output_path</code> <code>str</code> <p>path to the output file to create</p> required <code>resampling</code> <code>Enum</code> <p>resampling algorithm to apply to on-the-fly reprojection from rasterio.enums.Resampling</p> <code>average</code> <code>count</code> <code>int</code> <p>number of bands in the prediction output</p> <code>1</code> <code>dtype</code> <code>str</code> <p>the output raster data type</p> <code>'float32'</code> <code>nodata</code> <code>float</code> <p>output nodata value</p> <code>-9999</code> <code>driver</code> <code>str</code> <p>output raster format from rasterio.drivers.raster_driver_extensions()</p> <code>'GTiff'</code> <code>compress</code> <code>str</code> <p>compression to apply to the output file</p> <code>'deflate'</code> <code>bigtiff</code> <code>bool</code> <p>specify the output file as a bigtiff (for rasters &gt; 2GB)</p> <code>True</code> <code>template_idx</code> <code>int</code> <p>index of the raster file to use as a template. template_idx=0 sets the first raster as template</p> <code>0</code> <code>windowed</code> <code>bool</code> <p>apply the model using windowed read/write slower, but more memory efficient</p> <code>True</code> <code>predict_proba</code> <code>bool</code> <p>use model.predict_proba() instead of model.predict()</p> <code>False</code> <code>ignore_sklearn</code> <code>bool</code> <p>silence sklearn warning messages</p> <code>True</code> <code>quiet</code> <code>bool</code> <p>silence progress bar output</p> <code>False</code> <code>**kwargs</code> <p>additonal keywords to pass to model.predict()</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>saves model predictions to disk.</p> Source code in <code>elapid/geo.py</code> <pre><code>def apply_model_to_rasters(\n    model: BaseEstimator,\n    raster_paths: list,\n    output_path: str,\n    resampling: rio.enums.Enum = rio.enums.Resampling.average,\n    count: int = 1,\n    dtype: str = \"float32\",\n    nodata: float = -9999,\n    driver: str = \"GTiff\",\n    compress: str = \"deflate\",\n    bigtiff: bool = True,\n    template_idx: int = 0,\n    windowed: bool = True,\n    predict_proba: bool = False,\n    ignore_sklearn: bool = True,\n    quiet: bool = False,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Applies a trained model to a list of raster datasets.\n\n    The list and band order of the rasters must match the order of the covariates\n    used to train the model. It reads each dataset block-by-block, applies\n    the model, and writes gridded predictions. If the raster datasets are not\n    consistent (different extents, resolutions, etc.), it wll re-project the data\n    on the fly, with the grid size, extent and projection based on a 'template'\n    raster.\n\n    Args:\n        model: object with a model.predict() function\n        raster_paths: raster paths of covariates to apply the model to\n        output_path: path to the output file to create\n        resampling: resampling algorithm to apply to on-the-fly reprojection\n            from rasterio.enums.Resampling\n        count: number of bands in the prediction output\n        dtype: the output raster data type\n        nodata: output nodata value\n        driver: output raster format\n            from rasterio.drivers.raster_driver_extensions()\n        compress: compression to apply to the output file\n        bigtiff: specify the output file as a bigtiff (for rasters &gt; 2GB)\n        template_idx: index of the raster file to use as a template.\n            template_idx=0 sets the first raster as template\n        windowed: apply the model using windowed read/write\n            slower, but more memory efficient\n        predict_proba: use model.predict_proba() instead of model.predict()\n        ignore_sklearn: silence sklearn warning messages\n        quiet: silence progress bar output\n        **kwargs: additonal keywords to pass to model.predict()\n\n    Returns:\n        None: saves model predictions to disk.\n    \"\"\"\n    # make sure the raster_paths are iterable\n    raster_paths = to_iterable(raster_paths)\n\n    # get and set template parameters\n    windows, dst_profile = create_output_raster_profile(\n        raster_paths,\n        template_idx,\n        count=count,\n        windowed=windowed,\n        nodata=nodata,\n        compress=compress,\n        driver=driver,\n        bigtiff=bigtiff,\n    )\n\n    # get the bands and indexes for each covariate raster\n    nbands, band_idx = get_raster_band_indexes(raster_paths)\n\n    # check whether the raster paths are aligned to determine how the data are read\n    aligned = check_raster_alignment(raster_paths)\n\n    # set a dummy nodata variable if none is set\n    # (acutal nodata reads handled by rasterios src.read(masked=True) method)\n    nodata = nodata or 0\n\n    # turn off sklearn warnings\n    if ignore_sklearn:\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n\n    # open all rasters to read from later\n    srcs = [rio.open(raster_path) for raster_path in raster_paths]\n\n    # use warped VRT reads to align all rasters pixel-pixel if not aligned\n    if not aligned:\n        vrt_options = {\n            \"resampling\": resampling,\n            \"transform\": dst_profile[\"transform\"],\n            \"crs\": dst_profile[\"crs\"],\n            \"height\": dst_profile[\"height\"],\n            \"width\": dst_profile[\"width\"],\n        }\n        srcs = [rio.vrt.WarpedVRT(src, **vrt_options) for src in srcs]\n\n    # read and reproject blocks from each data source and write predictions to disk\n    with rio.open(output_path, \"w\", **dst_profile) as dst:\n        for window in tqdm(windows, desc=\"Window\", disable=quiet, **tqdm_opts):\n            # create stacked arrays to handle multi-raster, multi-band inputs\n            # that may have different nodata locations\n            covariates = np.zeros((nbands, window.height, window.width), dtype=np.float32)\n            nodata_idx = np.ones_like(covariates, dtype=bool)\n\n            try:\n                for i, src in enumerate(srcs):\n                    data = src.read(window=window, masked=True)\n                    covariates[band_idx[i] : band_idx[i + 1]] = data\n                    nodata_idx[band_idx[i] : band_idx[i + 1]] = data.mask\n\n                    # skip blocks full of no-data\n                    if data.mask.all():\n                        raise NoDataException()\n\n                predictions = apply_model_to_array(\n                    model,\n                    covariates,\n                    nodata,\n                    nodata_idx,\n                    count=count,\n                    dtype=dtype,\n                    predict_proba=predict_proba,\n                    **kwargs,\n                )\n                dst.write(predictions, window=window)\n\n            except NoDataException:\n                continue\n</code></pre>"},{"location":"module/geo/#elapid.geo.crs_match","title":"<code>crs_match(crs1, crs2)</code>","text":"<p>Evaluates whether two coordinate reference systems are the same.</p> <p>Parameters:</p> Name Type Description Default <code>crs1</code> <code>CRSType</code> <p>the first CRS, from a rasterio dataset, a GeoDataFrame, or a string with projection parameters.</p> required <code>crs2</code> <code>CRSType</code> <p>the second CRS, from the same sources above.</p> required <p>Returns:</p> Name Type Description <code>matches</code> <code>bool</code> <p>Boolean for whether the CRS match.</p> Source code in <code>elapid/geo.py</code> <pre><code>def crs_match(crs1: CRSType, crs2: CRSType) -&gt; bool:\n    \"\"\"Evaluates whether two coordinate reference systems are the same.\n\n    Args:\n        crs1: the first CRS, from a rasterio dataset, a GeoDataFrame, or a string with projection parameters.\n        crs2: the second CRS, from the same sources above.\n\n    Returns:\n        matches: Boolean for whether the CRS match.\n    \"\"\"\n    # normalize string inputs via rasterio\n    if type(crs1) is str:\n        crs1 = string_to_crs(crs1)\n    if type(crs2) is str:\n        crs2 = string_to_crs(crs2)\n\n    matches = crs1 == crs2\n\n    return matches\n</code></pre>"},{"location":"module/geo/#elapid.geo.distance_weights","title":"<code>distance_weights(points, n_neighbors=-1, center='median', cpu_count=-1)</code>","text":"<p>Compute sample weights based on the distance between points.</p> <p>Assigns higher scores to isolated points, lower scores to clustered points.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Vector</code> <p>point-format GeoSeries or GeoDataFrame</p> required <code>n_neighbors</code> <code>int</code> <p>compute weights based on average distance to the nearest n_neighbors set to -1 to compute the distance to all neighbors.</p> <code>-1</code> <code>center</code> <code>str</code> <p>rescale the weights to center the mean or median of the array on 1 accepts either 'mean' or 'median' as input. pass None to ignore.</p> <code>'median'</code> <code>cpu_count</code> <code>int</code> <p>number of cpus to use for estimation. -1 uses all cores</p> <code>-1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>array of shape (len(points),) with scaled sample weights. Scaling is performed by dividing by the maximum value, preserving the relative scale of differences between the min and max distance.</p> Source code in <code>elapid/geo.py</code> <pre><code>def distance_weights(points: Vector, n_neighbors: int = -1, center: str = \"median\", cpu_count: int = -1) -&gt; np.ndarray:\n    \"\"\"Compute sample weights based on the distance between points.\n\n    Assigns higher scores to isolated points, lower scores to clustered points.\n\n    Args:\n        points: point-format GeoSeries or GeoDataFrame\n        n_neighbors: compute weights based on average distance to the nearest n_neighbors\n            set to -1 to compute the distance to all neighbors.\n        center: rescale the weights to center the mean or median of the array on 1\n            accepts either 'mean' or 'median' as input.\n            pass None to ignore.\n        cpu_count: number of cpus to use for estimation.\n            -1 uses all cores\n\n    Returns:\n        array of shape (len(points),) with scaled sample weights. Scaling\n            is performed by dividing by the maximum value, preserving the\n            relative scale of differences between the min and max distance.\n    \"\"\"\n    distances = nearest_point_distance(points, n_neighbors=n_neighbors, cpu_count=cpu_count)\n    weights = distances / distances.max()\n\n    if center is not None:\n        if center.lower() == \"mean\":\n            weights /= weights.mean()\n\n        elif center.lower() == \"median\":\n            weights /= np.median(weights)\n\n    return weights\n</code></pre>"},{"location":"module/geo/#elapid.geo.nearest_point_distance","title":"<code>nearest_point_distance(points1, points2=None, n_neighbors=1, cpu_count=-1)</code>","text":"<p>Compute the average euclidean distance to the nearest point in a series.</p> <p>Parameters:</p> Name Type Description Default <code>points1</code> <code>Vector</code> <p>return the closest distance from these points</p> required <code>points2</code> <code>Vector</code> <p>return the closest distance to these points if None, compute the distance to the nearest points in the points1 series</p> <code>None</code> <code>n_neighbors</code> <code>int</code> <p>compute the average distance to the nearest n_neighbors. set to -1 to compute the distance to all neighbors.</p> <code>1</code> <code>cpu_count</code> <code>int</code> <p>number of cpus to use for estimation. -1 uses all cores</p> <code>-1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>array of shape (len(points),) with the distance to each point's nearest neighbor</p> Source code in <code>elapid/geo.py</code> <pre><code>def nearest_point_distance(\n    points1: Vector, points2: Vector = None, n_neighbors: int = 1, cpu_count: int = -1\n) -&gt; np.ndarray:\n    \"\"\"Compute the average euclidean distance to the nearest point in a series.\n\n    Args:\n        points1: return the closest distance *from* these points\n        points2: return the closest distance *to* these points\n            if None, compute the distance to the nearest points\n            in the points1 series\n        n_neighbors: compute the average distance to the nearest n_neighbors.\n            set to -1 to compute the distance to all neighbors.\n        cpu_count: number of cpus to use for estimation.\n            -1 uses all cores\n\n    Returns:\n        array of shape (len(points),) with the distance to\n            each point's nearest neighbor\n    \"\"\"\n    if points1.crs.is_geographic:\n        warnings.warn(\"Computing distances using geographic coordinates is bad\")\n\n    pta1 = np.array(list(zip(points1.geometry.x, points1.geometry.y)))\n    k_offset = 1\n\n    if points2 is None:\n        pta2 = pta1\n        k_offset += 1\n\n    else:\n        pta2 = np.array(list(zip(points2.geometry.x, points2.geometry.y)))\n        if not crs_match(points1.crs, points2.crs):\n            warnings.warn(\"CRS mismatch between points\")\n\n    if n_neighbors &lt; 1:\n        n_neighbors = len(pta2) - k_offset\n\n    tree = KDTree(pta1)\n    k = np.arange(n_neighbors) + k_offset\n    distance, idx = tree.query(pta2, k=k, workers=cpu_count)\n\n    return distance.mean(axis=1)\n</code></pre>"},{"location":"module/geo/#elapid.geo.parse_crs_string","title":"<code>parse_crs_string(string)</code>","text":"<p>Parses a string to determine the CRS/spatial projection format.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>a string with CRS/projection data.</p> required <p>Returns:</p> Name Type Description <code>crs_type</code> <code>str</code> <p>Str in [\"wkt\", \"proj4\", \"epsg\", \"string\"].</p> Source code in <code>elapid/geo.py</code> <pre><code>def parse_crs_string(string: str) -&gt; str:\n    \"\"\"Parses a string to determine the CRS/spatial projection format.\n\n    Args:\n        string: a string with CRS/projection data.\n\n    Returns:\n        crs_type: Str in [\"wkt\", \"proj4\", \"epsg\", \"string\"].\n    \"\"\"\n    if \"epsg:\" in string.lower():\n        return \"epsg\"\n    elif \"+proj\" in string:\n        return \"proj4\"\n    elif \"SPHEROID\" in string:\n        return \"wkt\"\n    else:\n        return \"string\"\n</code></pre>"},{"location":"module/geo/#elapid.geo.read_raster_from_polygon","title":"<code>read_raster_from_polygon(src, poly)</code>","text":"<p>Read valid pixel values from all locations inside a polygon     Uses the polygon as a mask in addition to the existing raster mask</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>DatasetReader</code> <p>an open rasterio dataset to read from</p> required <code>poly</code> <code>Union[Polygon, MultiPolygon]</code> <p>a shapely Polygon or MultiPolygon</p> required <p>Returns:</p> Type Description <code>MaskedArray</code> <p>masked array of shape (nbands, nrows, ncols)</p> Source code in <code>elapid/geo.py</code> <pre><code>def read_raster_from_polygon(src: rio.DatasetReader, poly: Union[Polygon, MultiPolygon]) -&gt; np.ma.MaskedArray:\n    \"\"\"Read valid pixel values from all locations inside a polygon\n        Uses the polygon as a mask in addition to the existing raster mask\n\n    Args:\n        src: an open rasterio dataset to read from\n        poly: a shapely Polygon or MultiPolygon\n\n    Returns:\n        masked array of shape (nbands, nrows, ncols)\n    \"\"\"\n    # get the read parameters\n    window = rio.windows.from_bounds(*poly.bounds, src.transform)\n    transform = rio.windows.transform(window, src.transform)\n\n    # get the data\n    data = src.read(window=window, masked=True, boundless=True)\n    bands, rows, cols = data.shape\n    poly_mask = geometry_mask([poly], transform=transform, out_shape=(rows, cols))\n\n    # update the mask\n    data[:, poly_mask] = np.ma.masked\n\n    return data\n</code></pre>"},{"location":"module/geo/#elapid.geo.sample_bias_file","title":"<code>sample_bias_file(raster_path, count, ignore_mask=False)</code>","text":"<p>Creates a semi-random geographic sampling of points weighted towards biased areas.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>raster bias file path to sample from. pixel values can be in arbitrary range, but must be odered low -&gt; high probability</p> required <code>count</code> <code>int</code> <p>total number of samples to generate</p> required <code>ignore_mask</code> <code>bool</code> <p>sample from the full extent of the raster instead of unmasked areas only</p> <code>False</code> <p>Returns:</p> Name Type Description <code>points</code> <code>GeoSeries</code> <p>Point geometry geoseries</p> Source code in <code>elapid/geo.py</code> <pre><code>def sample_bias_file(raster_path: str, count: int, ignore_mask: bool = False) -&gt; gpd.GeoSeries:\n    \"\"\"Creates a semi-random geographic sampling of points weighted towards biased areas.\n\n    Args:\n        raster_path: raster bias file path to sample from. pixel values can\n            be in arbitrary range, but must be odered low -&gt; high probability\n        count: total number of samples to generate\n        ignore_mask: sample from the full extent of the raster instead of unmasked areas only\n\n    Returns:\n        points: Point geometry geoseries\n    \"\"\"\n    with rio.open(raster_path) as src:\n        if src.nodata is None or ignore_mask:\n            data = src.read(1)\n            rows, cols = np.where(data)\n            values = data.flatten()\n            probabilities = normalize_sample_probabilities(values)\n            samples = np.random.choice(len(rows), size=count, p=probabilities)\n\n        else:\n            data = src.read(1, masked=True)\n            rows, cols = np.where(~data.mask)\n            values = data[rows, cols]\n            probabilities = normalize_sample_probabilities(values)\n            samples = np.random.choice(len(rows), size=count, p=probabilities)\n\n        xy = np.zeros((count, 2))\n        for i, sample in enumerate(samples):\n            xy[i] = src.xy(rows[sample], cols[sample])\n\n        points = xy_to_geoseries(xy[:, 0], xy[:, 1], crs=src.crs)\n        return points\n</code></pre>"},{"location":"module/geo/#elapid.geo.sample_geoseries","title":"<code>sample_geoseries(geoseries, count, overestimate=2)</code>","text":"<p>Creates random geographic point samples inside a polygon/multipolygon</p> <p>Parameters:</p> Name Type Description Default <code>geoseries</code> <code>GeoSeries</code> <p>geometry dataset (e.g. <code>gdf['geometry']</code>) with polygons/multipolygons</p> required <code>count</code> <code>int</code> <p>number of samples to generate</p> required <code>overestimate</code> <code>float</code> <p>scaler to generate extra samples to toss points outside of the polygon/inside it's bounds</p> <code>2</code> <p>Returns:</p> Name Type Description <code>points</code> <code>GeoSeries</code> <p>Point geometry geoseries</p> Source code in <code>elapid/geo.py</code> <pre><code>def sample_geoseries(geoseries: gpd.GeoSeries, count: int, overestimate: float = 2) -&gt; gpd.GeoSeries:\n    \"\"\"Creates random geographic point samples inside a polygon/multipolygon\n\n    Args:\n        geoseries: geometry dataset (e.g. `gdf['geometry']`) with polygons/multipolygons\n        count: number of samples to generate\n        overestimate: scaler to generate extra samples\n            to toss points outside of the polygon/inside it's bounds\n\n    Returns:\n        points: Point geometry geoseries\n    \"\"\"\n    if type(geoseries) is gpd.GeoDataFrame:\n        geoseries = geoseries.geometry\n\n    polygon = geoseries.union_all()\n    xmin, ymin, xmax, ymax = polygon.bounds\n    ratio = polygon.area / polygon.envelope.area\n\n    samples = np.random.uniform((xmin, ymin), (xmax, ymax), (int(count / ratio * overestimate), 2))\n    multipoint = MultiPoint(samples)\n    multipoint = multipoint.intersection(polygon)\n    sample_array = np.zeros((len(multipoint.geoms), 2))\n    for idx, point in enumerate(multipoint.geoms):\n        sample_array[idx] = (point.x, point.y)\n\n    xy = sample_array[np.random.choice(len(sample_array), count)]\n    points = xy_to_geoseries(xy[:, 0], xy[:, 1], crs=geoseries.crs)\n\n    return points\n</code></pre>"},{"location":"module/geo/#elapid.geo.sample_raster","title":"<code>sample_raster(raster_path, count, nodata=None, ignore_mask=False)</code>","text":"<p>Create a random geographic sample of points based on a raster's extent.</p> <p>Selects from unmasked locations if the rasters nodata value is set.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>raster file path to sample locations from</p> required <code>count</code> <code>int</code> <p>number of samples to generate</p> required <code>nodata</code> <code>float</code> <p>add pixels with this value to the nodata mask</p> <code>None</code> <code>ignore_mask</code> <code>bool</code> <p>sample from the full extent of the raster instead of unmasked areas only</p> <code>False</code> <p>Returns:</p> Name Type Description <code>points</code> <code>GeoSeries</code> <p>Point geometry geoseries</p> Source code in <code>elapid/geo.py</code> <pre><code>def sample_raster(raster_path: str, count: int, nodata: float = None, ignore_mask: bool = False) -&gt; gpd.GeoSeries:\n    \"\"\"Create a random geographic sample of points based on a raster's extent.\n\n    Selects from unmasked locations if the rasters nodata value is set.\n\n    Args:\n        raster_path: raster file path to sample locations from\n        count: number of samples to generate\n        nodata: add pixels with this value to the nodata mask\n        ignore_mask: sample from the full extent of the raster instead of unmasked areas only\n\n    Returns:\n        points: Point geometry geoseries\n    \"\"\"\n    # handle masked vs unmasked data differently\n    with rio.open(raster_path) as src:\n        if src.nodata is None or ignore_mask:\n            if nodata is None:\n                xmin, ymin, xmax, ymax = src.bounds\n                xy = np.random.uniform((xmin, ymin), (xmax, ymax), (count, 2))\n            else:\n                data = src.read(1)\n                mask = data != nodata\n                rows, cols = np.where(mask)\n                samples = np.random.randint(0, len(rows), count)\n                xy = np.zeros((count, 2))\n                for i, sample in enumerate(samples):\n                    xy[i] = src.xy(rows[sample], cols[sample])\n\n        else:\n            if nodata is None:\n                masked = src.read_masks(1)\n                rows, cols = np.where(masked == 255)\n            else:\n                data = src.read(1, masked=True)\n                data.mask += data.data == nodata\n                rows, cols = np.where(~data.mask)\n\n            samples = np.random.randint(0, len(rows), count)\n            xy = np.zeros((count, 2))\n            for i, sample in enumerate(samples):\n                xy[i] = src.xy(rows[sample], cols[sample])\n\n        points = xy_to_geoseries(xy[:, 0], xy[:, 1], crs=src.crs)\n        return points\n</code></pre>"},{"location":"module/geo/#elapid.geo.sample_vector","title":"<code>sample_vector(vector_path, count, overestimate=2)</code>","text":"<p>Creates a random geographic sampling of points inside of a polygon/multipolygon type vector file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>path to a vector file (shp, geojson, etc)</p> required <code>count</code> <code>int</code> <p>number of samples to generate</p> required <code>overestimate</code> <code>float</code> <p>scaler to generate extra samples to toss points outside of the polygon/inside it's bounds</p> <code>2</code> <p>Returns:</p> Name Type Description <code>points</code> <code>GeoSeries</code> <p>Point geometry geoseries</p> Source code in <code>elapid/geo.py</code> <pre><code>def sample_vector(vector_path: str, count: int, overestimate: float = 2) -&gt; gpd.GeoSeries:\n    \"\"\"Creates a random geographic sampling of points inside of a polygon/multipolygon type vector file.\n\n    Args:\n        vector_path: path to a vector file (shp, geojson, etc)\n        count: number of samples to generate\n        overestimate: scaler to generate extra samples to\n            toss points outside of the polygon/inside it's bounds\n\n    Returns:\n        points: Point geometry geoseries\n    \"\"\"\n    gdf = gpd.read_file(vector_path)\n    return sample_geoseries(gdf.geometry, count, overestimate=overestimate)\n</code></pre>"},{"location":"module/geo/#elapid.geo.stack_geodataframes","title":"<code>stack_geodataframes(presence, background, add_class_label=False, target_crs='presence')</code>","text":"<p>Concatenate geometries from two GeoSeries/GeoDataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>presence</code> <code>Vector</code> <p>presence geometry (y=1) locations</p> required <code>background</code> <code>Vector</code> <p>background geometry (y=0) locations</p> required <code>add_class_label</code> <code>bool</code> <p>add a column labeling the y value for each point</p> <code>False</code> <code>target_crs</code> <code>str</code> <p>if reprojection is necessary, use this variable's crs. valid options are \"presence\" and \"background\"</p> <code>'presence'</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>merged GeoDataFrame with all geometries projected to the same crs.</p> Source code in <code>elapid/geo.py</code> <pre><code>def stack_geodataframes(\n    presence: Vector, background: Vector, add_class_label: bool = False, target_crs: str = \"presence\"\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Concatenate geometries from two GeoSeries/GeoDataFrames.\n\n    Args:\n        presence: presence geometry (y=1) locations\n        background: background geometry (y=0) locations\n        add_class_label: add a column labeling the y value for each point\n        target_crs: if reprojection is necessary, use this variable's crs.\n            valid options are \"presence\" and \"background\"\n\n    Returns:\n        merged GeoDataFrame with all geometries projected to the same crs.\n    \"\"\"\n    validate_gpd(presence)\n    validate_gpd(background)\n\n    # cast to geodataframes\n    if isinstance(presence, gpd.GeoSeries):\n        presence = presence.to_frame(\"geometry\")\n    if isinstance(background, gpd.GeoSeries):\n        background = background.to_frame(\"geometry\")\n\n    # handle projection mismatch\n    crs = presence.crs\n    if crs_match(presence.crs, background.crs):\n        # explicitly set the two to exactly matching crs as geopandas\n        # throws errors if there's any mismatch at all\n        background.set_crs(presence.crs, inplace=True)\n    else:\n        if target_crs.lower() == \"presence\":\n            background.to_crs(crs, inplace=True)\n        elif target_crs.lower() == \"background\":\n            crs = background.crs\n            presence.to_crs(crs, inplace=True)\n        else:\n            raise NameError(f\"Unrecognized target_crs option: {target_crs}\")\n\n    if add_class_label:\n        presence[\"class\"] = 1\n        background[\"class\"] = 0\n\n    matching = [col for col in presence.columns if col in background.columns]\n    assert len(matching) &gt; 0, \"no matching columns found between data frames\"\n\n    merged = pd.concat((presence[matching], background[matching]), axis=0, ignore_index=True)\n    gdf = gpd.GeoDataFrame(merged, crs=crs)\n\n    return gdf\n</code></pre>"},{"location":"module/geo/#elapid.geo.string_to_crs","title":"<code>string_to_crs(string)</code>","text":"<p>Converts a crs/projection string to a pyproj-readable CRS object</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>a crs/projection string.</p> required <p>Returns:</p> Name Type Description <code>crs</code> <code>CRS</code> <p>the coordinate reference system</p> Source code in <code>elapid/geo.py</code> <pre><code>def string_to_crs(string: str) -&gt; rio.crs.CRS:\n    \"\"\"Converts a crs/projection string to a pyproj-readable CRS object\n\n    Args:\n        string: a crs/projection string.\n\n    Returns:\n        crs: the coordinate reference system\n    \"\"\"\n    crs_type = parse_crs_string(string)\n\n    if crs_type == \"epsg\":\n        auth, code = string.split(\":\")\n        crs = rio.crs.CRS.from_epsg(int(code))\n    elif crs_type == \"proj4\":\n        crs = rio.crs.CRS.from_proj4(string)\n    elif crs_type == \"wkt\":\n        crs = rio.crs.CRS.from_wkt(string)\n    else:\n        crs = rio.crs.CRS.from_string(string)\n\n    return crs\n</code></pre>"},{"location":"module/geo/#elapid.geo.validate_gpd","title":"<code>validate_gpd(geo)</code>","text":"<p>Validates whether an input is a GeoDataFrame or a GeoSeries.</p> <p>Parameters:</p> Name Type Description Default <code>geo</code> <code>Vector</code> <p>an input variable that should be in GeoPandas format</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>geo is not a GeoPandas dataframe or series</p> Source code in <code>elapid/geo.py</code> <pre><code>def validate_gpd(geo: Vector) -&gt; None:\n    \"\"\"Validates whether an input is a GeoDataFrame or a GeoSeries.\n\n    Args:\n        geo: an input variable that should be in GeoPandas format\n\n    Raises:\n        TypeError: geo is not a GeoPandas dataframe or series\n    \"\"\"\n    if not (isinstance(geo, gpd.GeoDataFrame) or isinstance(geo, gpd.GeoSeries)):\n        raise TypeError(\"Input must be a GeoDataFrame or GeoSeries\")\n</code></pre>"},{"location":"module/geo/#elapid.geo.validate_polygons","title":"<code>validate_polygons(geometry)</code>","text":"<p>Iterate over a geoseries to find rows with invalid geometry types.</p> <p>Parameters:</p> Name Type Description Default <code>geometry</code> <code>Vector</code> <p>a GeoSeries or GeoDataFrame with polygon geometries</p> required <p>Returns:</p> Type Description <code>Index</code> <p>an index of rows with valid polygon types</p> Source code in <code>elapid/geo.py</code> <pre><code>def validate_polygons(geometry: Vector) -&gt; pd.Index:\n    \"\"\"Iterate over a geoseries to find rows with invalid geometry types.\n\n    Args:\n        geometry: a GeoSeries or GeoDataFrame with polygon geometries\n\n    Returns:\n        an index of rows with valid polygon types\n    \"\"\"\n    if isinstance(geometry, gpd.GeoDataFrame):\n        geometry = geometry.geometry\n\n    index = []\n    for idx, geom in enumerate(geometry):\n        if not (isinstance(geom, Polygon) or isinstance(geom, MultiPolygon)):\n            index.append(idx)\n\n    if len(index) &gt; 0:\n        warnings.warn(\n            f\"Input geometry had {len(index)} invalid geometries. \"\n            \"These will be dropped, but with the original index preserved.\"\n        )\n        geometry.drop(index=index, inplace=True)\n\n    return geometry.index\n</code></pre>"},{"location":"module/geo/#elapid.geo.xy_to_geoseries","title":"<code>xy_to_geoseries(x, y, crs='epsg:4326')</code>","text":"<p>Converts x/y data into a geopandas geoseries.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[float, list, ndarray]</code> <p>1-D array-like of x location values</p> required <code>y</code> <code>Union[float, list, ndarray]</code> <p>1-D array-like of y location values</p> required <code>crs</code> <code>CRSType</code> <p>coordinate reference system. accepts pyproj.CRS / rio.crs.CRS objects or anything allowed by pyproj.CRS.from_user_input()</p> <code>'epsg:4326'</code> <p>Returns:</p> Name Type Description <code>gs</code> <code>GeoSeries</code> <p>Point geometry geoseries</p> Source code in <code>elapid/geo.py</code> <pre><code>def xy_to_geoseries(\n    x: Union[float, list, np.ndarray], y: Union[float, list, np.ndarray], crs: CRSType = \"epsg:4326\"\n) -&gt; gpd.GeoSeries:\n    \"\"\"Converts x/y data into a geopandas geoseries.\n\n    Args:\n        x: 1-D array-like of x location values\n        y: 1-D array-like of y location values\n        crs: coordinate reference system. accepts pyproj.CRS / rio.crs.CRS objects\n            or anything allowed by pyproj.CRS.from_user_input()\n\n    Returns:\n        gs: Point geometry geoseries\n    \"\"\"\n    # handle single x/y location values\n    x = to_iterable(x)\n    y = to_iterable(y)\n\n    points = [Point(x, y) for x, y in zip(x, y)]\n    gs = gpd.GeoSeries(points, crs=crs)\n\n    return gs\n</code></pre>"},{"location":"module/geo/#elapid.geo.zonal_stats","title":"<code>zonal_stats(polygons, raster_paths, labels=None, all_touched=True, mean=True, stdv=True, min=False, max=False, count=False, sum=False, skew=False, kurtosis=False, mode=False, all=False, percentiles=[], quiet=False)</code>","text":"<p>Compute raster summary stats for each polygon in a GeoSeries or GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>Vector</code> <p>GeoSeries or GeoDataFrame with polygon geometries.</p> required <code>raster_paths</code> <code>list</code> <p>list of paths to rasters to summarize</p> required <code>labels</code> <code>list</code> <p>band labels. must match the total number of bands for all raster_paths.</p> <code>None</code> <code>all_touched</code> <code>bool</code> <p>include all pixels that touch a polygon. set to False to only include pixels whose centers intersect the polygon</p> <code>True</code> <code>mean, min, max, count, sum, stdv, skew, kurtosis, mode</code> <p>set to True to compute these stats</p> required <code>all</code> <code>bool</code> <p>compute all of the above stats</p> <code>False</code> <code>percentiles</code> <code>list</code> <p>list of 0-100 percentile ranges to compute</p> <code>[]</code> <code>quiet</code> <code>bool</code> <p>silence progress bar output</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with zonal stats for each raster band in new columns. If <code>polygons</code> is a GeoDataFrame, the zonal stats columns are appended to the original input.</p> Source code in <code>elapid/geo.py</code> <pre><code>def zonal_stats(\n    polygons: Vector,\n    raster_paths: list,\n    labels: list = None,\n    all_touched: bool = True,\n    mean: bool = True,\n    stdv: bool = True,\n    min: bool = False,\n    max: bool = False,\n    count: bool = False,\n    sum: bool = False,\n    skew: bool = False,\n    kurtosis: bool = False,\n    mode: bool = False,\n    all: bool = False,\n    percentiles: list = [],\n    quiet: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Compute raster summary stats for each polygon in a GeoSeries or GeoDataFrame.\n\n    Args:\n        polygons: GeoSeries or GeoDataFrame with polygon geometries.\n        raster_paths: list of paths to rasters to summarize\n        labels: band labels. must match the total number of bands for all raster_paths.\n        all_touched: include all pixels that touch a polygon.\n            set to False to only include pixels whose centers intersect the polygon\n        mean, min, max, count, sum, stdv, skew, kurtosis, mode:\n            set to True to compute these stats\n        all: compute all of the above stats\n        percentiles: list of 0-100 percentile ranges to compute\n        quiet: silence progress bar output\n\n    Returns:\n        GeoDataFrame with zonal stats for each raster band in new columns.\n            If `polygons` is a GeoDataFrame, the zonal stats columns are appended\n            to the original input.\n    \"\"\"\n    # format the input geometries\n    validate_gpd(polygons)\n    valid_idx = validate_polygons(polygons)\n    polygons = polygons.iloc[valid_idx]\n    is_df = isinstance(polygons, gpd.GeoDataFrame)\n    polys = polygons.geometry if is_df else polygons\n\n    # format the input labels\n    raster_paths = to_iterable(raster_paths)\n    labels = format_band_labels(raster_paths, labels)\n\n    # get the bands and indexes for each covariate raster\n    nbands, band_idx = get_raster_band_indexes(raster_paths)\n\n    # get the stats methods to compute for each feature\n    stats_methods = get_raster_stats_methods(\n        mean=mean,\n        min=min,\n        max=max,\n        count=count,\n        sum=sum,\n        stdv=stdv,\n        skew=skew,\n        kurtosis=kurtosis,\n        mode=mode,\n        percentiles=percentiles,\n        all=all,\n    )\n\n    # create dataframes for each raster and concatenate at the end\n    raster_dfs = []\n\n    # run zonal stats raster-by-raster (instead of iterating first over geometries)\n    for r, raster in tqdm(enumerate(raster_paths), total=len(raster_paths), desc=\"Raster\", disable=quiet, **tqdm_opts):\n        # format the band labels\n        band_labels = labels[band_idx[r] : band_idx[r + 1]]\n        n_raster_bands = band_idx[r + 1] - band_idx[r]\n        stats_labels = []\n        for method in stats_methods:\n            stats_labels.append([f\"{band}_{method.name}\" for band in band_labels])\n\n        # open the raster for reading\n        with rio.open(raster, \"r\") as src:\n            # reproject the polygon data as necessary\n            if not crs_match(polys.crs, src.crs):\n                polys = polys.to_crs(src.crs)\n\n            # create output arrays to store each stat's output\n            stats_arrays = []\n            for method in stats_methods:\n                dtype = method.dtype or src.dtypes[0]\n                stats_arrays.append(np.zeros((len(polys), n_raster_bands), dtype=dtype))\n\n            # iterate over each geometry to read data and compute stats\n            for p, poly in tqdm(\n                enumerate(polys), total=len(polys), desc=\"Polygon\", leave=False, disable=quiet, **tqdm_opts\n            ):\n                data = read_raster_from_polygon(src, poly)\n                for method, array in zip(stats_methods, stats_arrays):\n                    array[p, :] = np.array(method.reduce(data)).astype(array.dtype)\n\n        # convert each stat's array into dataframes and merge them together\n        stats_dfs = [pd.DataFrame(array, columns=labels) for array, labels in zip(stats_arrays, stats_labels)]\n        raster_dfs.append(pd.concat(stats_dfs, axis=1))\n\n    # merge the outputs from each raster\n    if is_df:\n        merged = gpd.GeoDataFrame(pd.concat([polygons] + raster_dfs, axis=1), crs=polygons.crs)\n    else:\n        merged = gpd.GeoDataFrame(pd.concat(raster_dfs, axis=1), geometry=polygons, crs=polygons.crs)\n\n    return merged\n</code></pre>"},{"location":"module/models/","title":"elapid.models","text":"<p>Classes for training species distribution models.</p>"},{"location":"module/models/#elapid.models.EnsembleModel","title":"<code>EnsembleModel</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>SDMMixin</code></p> <p>Barebones estimator for ensembling multiple model predictions.</p> Source code in <code>elapid/models.py</code> <pre><code>class EnsembleModel(BaseEstimator, SDMMixin):\n    \"\"\"Barebones estimator for ensembling multiple model predictions.\"\"\"\n\n    models: list = None\n    reducer: str = None\n\n    def __init__(self, models: List[BaseEstimator], reducer: str = EnsembleConfig.reducer):\n        \"\"\"Create a model ensemble from a set of trained models.\n\n        Args:\n            models: iterable of models with `.predict()` and/or `.predict_proba()` methods\n            reducer: method for reducing/ensembling each model's predictions.\n                select from ['mean', 'median', 'mode']\n        \"\"\"\n        self.models = models\n        self.reducer = reducer\n\n    def reduce(self, preds: List[np.ndarray]) -&gt; np.ndarray:\n        \"\"\"Reduce multiple model predictions into ensemble prediction/probability scores.\n\n        Args:\n            preds: list of model predictions from .predict() or .predict_proba()\n\n        Returns:\n            array-like of shape (n_samples, n_classes) with model predictions\n        \"\"\"\n        reducer = self.reducer.lower()\n\n        if reducer == \"mean\":\n            reduced = np.nanmean(preds, axis=0)\n\n        elif reducer == \"median\":\n            reduced = np.nanmedian(preds, axis=0)\n\n        elif reducer == \"mode\":\n            try:\n                summary = scistats.mode(preds, axis=0, nan_policy=\"omit\", keepdims=False)\n                reduced = summary.mode\n            except TypeError:\n                summary = scistats.mode(preds, axis=0, nan_policy=\"omit\")\n                reduced = np.squeeze(summary.mode)\n\n        return reduced\n\n    def predict(self, x: ArrayLike) -&gt; np.ndarray:\n        \"\"\"Applies models to a set of covariates or features. Requires each model has been fit.\n\n        Args:\n            x: array-like of shape (n_samples, n_features) with covariate data\n\n        Returns:\n            array-like of shape (n_samples,) with model predictions\n        \"\"\"\n        preds = [model.predict(x) for model in self.models]\n        ensemble = self.reduce(preds)\n        return ensemble\n\n    def predict_proba(self, x: ArrayLike) -&gt; np.ndarray:\n        \"\"\"Compute prediction probability scores for each class.\n\n        Args:\n            x: array-like of shape (n_samples, n_features) with covariate data\n\n        Returns:\n            array-like of shape (n_samples, n_classes) with model predictions\n        \"\"\"\n        probas = [model.predict_proba(x) for model in self.models]\n        ensemble = self.reduce(probas)\n        return ensemble\n</code></pre>"},{"location":"module/models/#elapid.models.EnsembleModel.__init__","title":"<code>__init__(models, reducer=EnsembleConfig.reducer)</code>","text":"<p>Create a model ensemble from a set of trained models.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>List[BaseEstimator]</code> <p>iterable of models with <code>.predict()</code> and/or <code>.predict_proba()</code> methods</p> required <code>reducer</code> <code>str</code> <p>method for reducing/ensembling each model's predictions. select from ['mean', 'median', 'mode']</p> <code>reducer</code> Source code in <code>elapid/models.py</code> <pre><code>def __init__(self, models: List[BaseEstimator], reducer: str = EnsembleConfig.reducer):\n    \"\"\"Create a model ensemble from a set of trained models.\n\n    Args:\n        models: iterable of models with `.predict()` and/or `.predict_proba()` methods\n        reducer: method for reducing/ensembling each model's predictions.\n            select from ['mean', 'median', 'mode']\n    \"\"\"\n    self.models = models\n    self.reducer = reducer\n</code></pre>"},{"location":"module/models/#elapid.models.EnsembleModel.predict","title":"<code>predict(x)</code>","text":"<p>Applies models to a set of covariates or features. Requires each model has been fit.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) with covariate data</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>array-like of shape (n_samples,) with model predictions</p> Source code in <code>elapid/models.py</code> <pre><code>def predict(self, x: ArrayLike) -&gt; np.ndarray:\n    \"\"\"Applies models to a set of covariates or features. Requires each model has been fit.\n\n    Args:\n        x: array-like of shape (n_samples, n_features) with covariate data\n\n    Returns:\n        array-like of shape (n_samples,) with model predictions\n    \"\"\"\n    preds = [model.predict(x) for model in self.models]\n    ensemble = self.reduce(preds)\n    return ensemble\n</code></pre>"},{"location":"module/models/#elapid.models.EnsembleModel.predict_proba","title":"<code>predict_proba(x)</code>","text":"<p>Compute prediction probability scores for each class.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) with covariate data</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>array-like of shape (n_samples, n_classes) with model predictions</p> Source code in <code>elapid/models.py</code> <pre><code>def predict_proba(self, x: ArrayLike) -&gt; np.ndarray:\n    \"\"\"Compute prediction probability scores for each class.\n\n    Args:\n        x: array-like of shape (n_samples, n_features) with covariate data\n\n    Returns:\n        array-like of shape (n_samples, n_classes) with model predictions\n    \"\"\"\n    probas = [model.predict_proba(x) for model in self.models]\n    ensemble = self.reduce(probas)\n    return ensemble\n</code></pre>"},{"location":"module/models/#elapid.models.EnsembleModel.reduce","title":"<code>reduce(preds)</code>","text":"<p>Reduce multiple model predictions into ensemble prediction/probability scores.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>List[ndarray]</code> <p>list of model predictions from .predict() or .predict_proba()</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>array-like of shape (n_samples, n_classes) with model predictions</p> Source code in <code>elapid/models.py</code> <pre><code>def reduce(self, preds: List[np.ndarray]) -&gt; np.ndarray:\n    \"\"\"Reduce multiple model predictions into ensemble prediction/probability scores.\n\n    Args:\n        preds: list of model predictions from .predict() or .predict_proba()\n\n    Returns:\n        array-like of shape (n_samples, n_classes) with model predictions\n    \"\"\"\n    reducer = self.reducer.lower()\n\n    if reducer == \"mean\":\n        reduced = np.nanmean(preds, axis=0)\n\n    elif reducer == \"median\":\n        reduced = np.nanmedian(preds, axis=0)\n\n    elif reducer == \"mode\":\n        try:\n            summary = scistats.mode(preds, axis=0, nan_policy=\"omit\", keepdims=False)\n            reduced = summary.mode\n        except TypeError:\n            summary = scistats.mode(preds, axis=0, nan_policy=\"omit\")\n            reduced = np.squeeze(summary.mode)\n\n    return reduced\n</code></pre>"},{"location":"module/models/#elapid.models.MaxentModel","title":"<code>MaxentModel</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>SDMMixin</code></p> <p>Model estimator for Maxent-style species distribution models.</p> Source code in <code>elapid/models.py</code> <pre><code>class MaxentModel(BaseEstimator, SDMMixin):\n    \"\"\"Model estimator for Maxent-style species distribution models.\"\"\"\n\n    def __init__(\n        self,\n        feature_types: Union[list, str] = MaxentConfig.feature_types,\n        tau: float = MaxentConfig.tau,\n        transform: float = MaxentConfig.transform,\n        clamp: bool = MaxentConfig.clamp,\n        scorer: str = MaxentConfig.scorer,\n        beta_multiplier: float = MaxentConfig.beta_multiplier,\n        beta_lqp: float = MaxentConfig.beta_lqp,\n        beta_hinge: float = MaxentConfig.beta_hinge,\n        beta_threshold: float = MaxentConfig.beta_lqp,\n        beta_categorical: float = MaxentConfig.beta_categorical,\n        n_hinge_features: int = MaxentConfig.n_hinge_features,\n        n_threshold_features: int = MaxentConfig.n_threshold_features,\n        convergence_tolerance: float = MaxentConfig.tolerance,\n        use_lambdas: str = MaxentConfig.use_lambdas,\n        n_lambdas: int = MaxentConfig.n_lambdas,\n        class_weights: Union[str, float] = MaxentConfig.class_weights,\n        n_cpus: int = NCPUS,\n        use_sklearn: bool = FORCE_SKLEARN,\n        random_state: int = None,\n    ):\n        \"\"\"Create a maxent model object.\n\n        Args:\n            feature_types: maxent feature types to fit. must be in string \"lqphta\" or\n                list [\"linear\", \"quadratic\", \"product\", \"hinge\", \"threshold\", \"auto\"]\n            tau: maxent prevalence value for scaling logistic output\n            transform: maxent model transformation type. select from\n                [\"raw\", \"logistic\", \"cloglog\"].\n            clamp: set features to min/max range from training during prediction\n            scorer: sklearn scoring function for model training\n            beta_multiplier: scaler for all regularization parameters.\n                higher values drop more coeffiecients\n            beta_lqp: linear, quadratic and product feature regularization scaler\n            beta_hinge: hinge feature regularization scaler\n            beta_threshold: threshold feature regularization scaler\n            beta_categorical: categorical feature regularization scaler\n            n_hinge_features: the number of hinge features to fit in feature transformation\n            n_threshold_features: the number of thresholds to fit in feature transformation\n            convergence_tolerance: model convergence tolerance level\n            use_lambdas: guide for which model lambdas to select (either \"best\" or \"last\")\n            n_lambdas: number of lamba values to fit models with\n            class_weights: strategy for weighting presence samples.\n                pass \"balanced\" to compute the ratio based on sample frequency\n                or pass a float for the presence:background weight ratio\n                the R `maxnet` package uses a value of 100 as default.\n                set to None to ignore.\n            n_cpus: threads to use during model training\n            use_sklearn: force using `sklearn` for fitting logistic regression.\n                turned off by default to use `glmnet` for fitting.\n                this feature was turned on to support Windows users\n                who install the package without a fortran compiler.\n            random_state: Add random seed for reproducibility. Will be applied to both sklearn and glmnet.\n        \"\"\"\n        self.feature_types = feature_types\n        self.tau = tau\n        self.transform = transform\n        self.clamp = clamp\n        self.scorer = scorer\n        self.beta_multiplier = beta_multiplier\n        self.beta_hinge = beta_hinge\n        self.beta_lqp = beta_lqp\n        self.beta_threshold = beta_threshold\n        self.beta_categorical = beta_categorical\n        self.n_hinge_features = n_hinge_features\n        self.n_threshold_features = n_threshold_features\n        self.convergence_tolerance = convergence_tolerance\n        self.n_cpus = n_cpus\n        self.use_lambdas = use_lambdas\n        self.n_lambdas = n_lambdas\n        self.class_weights = class_weights\n        self.use_sklearn = use_sklearn\n        self.random_state = random_state\n\n        # computed during model fitting\n        self.initialized_ = False\n        self.estimator = None\n        self.preprocessor = None\n        self.transformer = None\n        self.regularization_ = None\n        self.lambdas_ = None\n        self.beta_scores_ = None\n        self.entropy_ = 0.0\n        self.alpha_ = 0.0\n\n    def fit(\n        self,\n        x: ArrayLike,\n        y: ArrayLike,\n        sample_weight: ArrayLike = None,\n        categorical: List[int] = None,\n        labels: list = None,\n        preprocessor: BaseEstimator = None,\n    ) -&gt; None:\n        \"\"\"Trains a maxent model using a set of covariates and presence/background points.\n\n        Args:\n            x: array of shape (n_samples, n_features) with covariate data\n            y: array of shape (n_samples,) with binary presence/background (1/0) values\n            sample_weight: array of weights assigned to each sample with shape (n_samples,).\n                this is modified by the `class_weights` model parameter unless\n                you set `class_weights=None`.\n            categorical: indices for which columns are categorical\n            labels: covariate labels. ignored if x is a pandas DataFrame\n            preprocessor: an `sklearn` transformer with a .transform() and/or\n                a .fit_transform() method. Some examples include a PCA() object or a\n                RobustScaler().\n        \"\"\"\n\n        # clear state variables\n        self.alpha_ = 0.0\n        self.entropy_ = 0.0\n\n        # format the input data\n        y = format_occurrence_data(y)\n\n        # apply preprocessing\n        if preprocessor is not None:\n            self.preprocessor = preprocessor\n            try:\n                x = self.preprocessor.transform(x)\n            except NotFittedError:\n                x = self.preprocessor.fit_transform(x)\n\n        # fit the feature transformer\n        self.feature_types = validate_feature_types(self.feature_types)\n        self.transformer = MaxentFeatureTransformer(\n            feature_types=self.feature_types,\n            clamp=self.clamp,\n            n_hinge_features=self.n_hinge_features,\n            n_threshold_features=self.n_threshold_features,\n        )\n        features = self.transformer.fit_transform(x, categorical=categorical, labels=labels)\n        feature_labels = self.transformer.feature_names_\n\n        # compute class weights\n        if self.class_weights is not None:\n            pbr = len(y) / y.sum() if self.class_weights == \"balanced\" else self.class_weights\n            class_weight = compute_weights(y, pbr=pbr)\n\n            # scale the sample weight\n            if sample_weight is None:\n                sample_weight = class_weight\n            else:\n                sample_weight *= class_weight\n\n        # model fitting with sklearn\n        if self.use_sklearn:\n            C = estimate_C_from_betas(self.beta_multiplier)\n            self.initialize_sklearn_model(C)\n            self.estimator.fit(features, y, sample_weight=sample_weight)\n            self.beta_scores_ = self.estimator.coef_[0]\n\n        # model fitting with glmnet\n        else:\n            # set feature regularization parameters\n            self.regularization_ = compute_regularization(\n                y,\n                features,\n                feature_labels=feature_labels,\n                beta_multiplier=self.beta_multiplier,\n                beta_lqp=self.beta_lqp,\n                beta_threshold=self.beta_threshold,\n                beta_hinge=self.beta_hinge,\n                beta_categorical=self.beta_categorical,\n            )\n\n            # get model lambda scores to initialize the glm\n            self.lambdas_ = compute_lambdas(y, sample_weight, self.regularization_, n_lambdas=self.n_lambdas)\n\n            # model fitting\n            self.initialize_glmnet_model(lambdas=self.lambdas_)\n            self.estimator.fit(\n                features,\n                y,\n                sample_weight=sample_weight,\n                relative_penalties=self.regularization_,\n            )\n\n            # get the beta values based on which lambda selection method to use\n            if self.use_lambdas == \"last\":\n                self.beta_scores_ = self.estimator.coef_path_[0, :, -1]\n            elif self.use_lambdas == \"best\":\n                self.beta_scores_ = self.estimator.coef_path_[0, :, self.estimator.lambda_max_inx_]\n\n        # store initialization state\n        self.initialized_ = True\n\n        # apply maxent-specific transformations\n        class_transform = self.get_params()[\"transform\"]\n        self.set_params(transform=\"raw\")\n        raw = self.predict(x[y == 0])\n        self.set_params(transform=class_transform)\n\n        # alpha is a normalizing constant that ensures that f1(z) integrates (sums) to 1\n        self.alpha_ = maxent_alpha(raw)\n\n        # the distance from f(z) is the relative entropy of f1(z) WRT f(z)\n        self.entropy_ = maxent_entropy(raw)\n\n        return self\n\n    def predict(self, x: ArrayLike) -&gt; ArrayLike:\n        \"\"\"Apply a model to a set of covariates or features. Requires that a model has been fit.\n\n        Args:\n            x: array-like of shape (n_samples, n_features) with covariate data\n\n        Returns:\n            predictions: array-like of shape (n_samples,) with model predictions\n        \"\"\"\n        if not self.initialized_:\n            raise NotFittedError(\"Model must be fit first\")\n\n        # feature transformations\n        x = x if self.preprocessor is None else self.preprocessor.transform(x)\n        features = x if self.transformer is None else self.transformer.transform(x)\n\n        # apply the model\n        engma = np.matmul(features, self.beta_scores_) + self.alpha_\n\n        # scale based on the transform type\n        if self.transform == \"raw\":\n            return maxent_raw_transform(engma)\n\n        elif self.transform == \"logistic\":\n            return maxent_logistic_transform(engma, self.entropy_, self.tau)\n\n        elif self.transform == \"cloglog\":\n            return maxent_cloglog_transform(engma, self.entropy_)\n\n    def predict_proba(self, x: ArrayLike) -&gt; ArrayLike:\n        \"\"\"Compute prediction probability scores for the 0/1 classes.\n\n        Args:\n            x: array-like of shape (n_samples, n_features) with covariate data\n\n        Returns:\n            predictions: array-like of shape (n_samples, 2) with model predictions\n        \"\"\"\n        ypred = self.predict(x).reshape(-1, 1)\n        predictions = np.hstack((1 - ypred, ypred))\n\n        return predictions\n\n    def fit_predict(\n        self,\n        x: ArrayLike,\n        y: ArrayLike,\n        categorical: list = None,\n        labels: list = None,\n        preprocessor: BaseEstimator = None,\n    ) -&gt; ArrayLike:\n        \"\"\"Trains and applies a model to x/y data.\n\n        Args:\n            x: array-like of shape (n_samples, n_features) with covariate data\n            y: array-like of shape (n_samples,) with binary presence/background (1/0) values\n            categorical: column indices indicating which columns are categorical\n            labels: Covariate labels. Ignored if x is a pandas DataFrame\n            preprocessor: an `sklearn` transformer with a .transform() and/or\n                a .fit_transform() method. Some examples include a PCA() object or a\n                RobustScaler().\n\n        Returns:\n            predictions: Array-like of shape (n_samples,) with model predictions\n        \"\"\"\n        self.fit(x, y, categorical=categorical, labels=labels, preprocessor=preprocessor)\n        predictions = self.predict(x)\n\n        return predictions\n\n    def initialize_glmnet_model(\n        self,\n        lambdas: np.array,\n        alpha: float = 1,\n        standardize: bool = False,\n        fit_intercept: bool = True,\n    ) -&gt; None:\n        \"\"\"Creates the Logistic Regression with elastic net penalty model object.\n\n        Args:\n            lambdas: array of model lambda values. get from elapid.features.compute_lambdas()\n            alpha: elasticnet mixing parameter. alpha=1 for lasso, alpha=0 for ridge\n            standardize: specify coefficient normalization\n            fit_intercept: include an intercept parameter\n        \"\"\"\n        self.estimator = LogitNet(\n            alpha=alpha,\n            lambda_path=lambdas,\n            standardize=standardize,\n            fit_intercept=fit_intercept,\n            scoring=self.scorer,\n            n_jobs=self.n_cpus,\n            tol=self.convergence_tolerance,\n            random_state=self.random_state,\n        )\n\n    def initialize_sklearn_model(self, C: float, fit_intercept: bool = True) -&gt; None:\n        \"\"\"Creates an sklearn Logisticregression estimator with L1 penalties.\n\n        Args:\n            C: the regularization parameter\n            fit_intercept: include an intercept parameter\n        \"\"\"\n        self.estimator = LogisticRegression(\n            C=C,\n            fit_intercept=fit_intercept,\n            penalty=\"l1\",\n            solver=\"liblinear\",\n            tol=self.convergence_tolerance,\n            max_iter=self.n_lambdas,\n            random_state=self.random_state\n        )\n</code></pre>"},{"location":"module/models/#elapid.models.MaxentModel.__init__","title":"<code>__init__(feature_types=MaxentConfig.feature_types, tau=MaxentConfig.tau, transform=MaxentConfig.transform, clamp=MaxentConfig.clamp, scorer=MaxentConfig.scorer, beta_multiplier=MaxentConfig.beta_multiplier, beta_lqp=MaxentConfig.beta_lqp, beta_hinge=MaxentConfig.beta_hinge, beta_threshold=MaxentConfig.beta_lqp, beta_categorical=MaxentConfig.beta_categorical, n_hinge_features=MaxentConfig.n_hinge_features, n_threshold_features=MaxentConfig.n_threshold_features, convergence_tolerance=MaxentConfig.tolerance, use_lambdas=MaxentConfig.use_lambdas, n_lambdas=MaxentConfig.n_lambdas, class_weights=MaxentConfig.class_weights, n_cpus=NCPUS, use_sklearn=FORCE_SKLEARN, random_state=None)</code>","text":"<p>Create a maxent model object.</p> <p>Parameters:</p> Name Type Description Default <code>feature_types</code> <code>Union[list, str]</code> <p>maxent feature types to fit. must be in string \"lqphta\" or list [\"linear\", \"quadratic\", \"product\", \"hinge\", \"threshold\", \"auto\"]</p> <code>feature_types</code> <code>tau</code> <code>float</code> <p>maxent prevalence value for scaling logistic output</p> <code>tau</code> <code>transform</code> <code>float</code> <p>maxent model transformation type. select from [\"raw\", \"logistic\", \"cloglog\"].</p> <code>transform</code> <code>clamp</code> <code>bool</code> <p>set features to min/max range from training during prediction</p> <code>clamp</code> <code>scorer</code> <code>str</code> <p>sklearn scoring function for model training</p> <code>scorer</code> <code>beta_multiplier</code> <code>float</code> <p>scaler for all regularization parameters. higher values drop more coeffiecients</p> <code>beta_multiplier</code> <code>beta_lqp</code> <code>float</code> <p>linear, quadratic and product feature regularization scaler</p> <code>beta_lqp</code> <code>beta_hinge</code> <code>float</code> <p>hinge feature regularization scaler</p> <code>beta_hinge</code> <code>beta_threshold</code> <code>float</code> <p>threshold feature regularization scaler</p> <code>beta_lqp</code> <code>beta_categorical</code> <code>float</code> <p>categorical feature regularization scaler</p> <code>beta_categorical</code> <code>n_hinge_features</code> <code>int</code> <p>the number of hinge features to fit in feature transformation</p> <code>n_hinge_features</code> <code>n_threshold_features</code> <code>int</code> <p>the number of thresholds to fit in feature transformation</p> <code>n_threshold_features</code> <code>convergence_tolerance</code> <code>float</code> <p>model convergence tolerance level</p> <code>tolerance</code> <code>use_lambdas</code> <code>str</code> <p>guide for which model lambdas to select (either \"best\" or \"last\")</p> <code>use_lambdas</code> <code>n_lambdas</code> <code>int</code> <p>number of lamba values to fit models with</p> <code>n_lambdas</code> <code>class_weights</code> <code>Union[str, float]</code> <p>strategy for weighting presence samples. pass \"balanced\" to compute the ratio based on sample frequency or pass a float for the presence:background weight ratio the R <code>maxnet</code> package uses a value of 100 as default. set to None to ignore.</p> <code>class_weights</code> <code>n_cpus</code> <code>int</code> <p>threads to use during model training</p> <code>NCPUS</code> <code>use_sklearn</code> <code>bool</code> <p>force using <code>sklearn</code> for fitting logistic regression. turned off by default to use <code>glmnet</code> for fitting. this feature was turned on to support Windows users who install the package without a fortran compiler.</p> <code>FORCE_SKLEARN</code> <code>random_state</code> <code>int</code> <p>Add random seed for reproducibility. Will be applied to both sklearn and glmnet.</p> <code>None</code> Source code in <code>elapid/models.py</code> <pre><code>def __init__(\n    self,\n    feature_types: Union[list, str] = MaxentConfig.feature_types,\n    tau: float = MaxentConfig.tau,\n    transform: float = MaxentConfig.transform,\n    clamp: bool = MaxentConfig.clamp,\n    scorer: str = MaxentConfig.scorer,\n    beta_multiplier: float = MaxentConfig.beta_multiplier,\n    beta_lqp: float = MaxentConfig.beta_lqp,\n    beta_hinge: float = MaxentConfig.beta_hinge,\n    beta_threshold: float = MaxentConfig.beta_lqp,\n    beta_categorical: float = MaxentConfig.beta_categorical,\n    n_hinge_features: int = MaxentConfig.n_hinge_features,\n    n_threshold_features: int = MaxentConfig.n_threshold_features,\n    convergence_tolerance: float = MaxentConfig.tolerance,\n    use_lambdas: str = MaxentConfig.use_lambdas,\n    n_lambdas: int = MaxentConfig.n_lambdas,\n    class_weights: Union[str, float] = MaxentConfig.class_weights,\n    n_cpus: int = NCPUS,\n    use_sklearn: bool = FORCE_SKLEARN,\n    random_state: int = None,\n):\n    \"\"\"Create a maxent model object.\n\n    Args:\n        feature_types: maxent feature types to fit. must be in string \"lqphta\" or\n            list [\"linear\", \"quadratic\", \"product\", \"hinge\", \"threshold\", \"auto\"]\n        tau: maxent prevalence value for scaling logistic output\n        transform: maxent model transformation type. select from\n            [\"raw\", \"logistic\", \"cloglog\"].\n        clamp: set features to min/max range from training during prediction\n        scorer: sklearn scoring function for model training\n        beta_multiplier: scaler for all regularization parameters.\n            higher values drop more coeffiecients\n        beta_lqp: linear, quadratic and product feature regularization scaler\n        beta_hinge: hinge feature regularization scaler\n        beta_threshold: threshold feature regularization scaler\n        beta_categorical: categorical feature regularization scaler\n        n_hinge_features: the number of hinge features to fit in feature transformation\n        n_threshold_features: the number of thresholds to fit in feature transformation\n        convergence_tolerance: model convergence tolerance level\n        use_lambdas: guide for which model lambdas to select (either \"best\" or \"last\")\n        n_lambdas: number of lamba values to fit models with\n        class_weights: strategy for weighting presence samples.\n            pass \"balanced\" to compute the ratio based on sample frequency\n            or pass a float for the presence:background weight ratio\n            the R `maxnet` package uses a value of 100 as default.\n            set to None to ignore.\n        n_cpus: threads to use during model training\n        use_sklearn: force using `sklearn` for fitting logistic regression.\n            turned off by default to use `glmnet` for fitting.\n            this feature was turned on to support Windows users\n            who install the package without a fortran compiler.\n        random_state: Add random seed for reproducibility. Will be applied to both sklearn and glmnet.\n    \"\"\"\n    self.feature_types = feature_types\n    self.tau = tau\n    self.transform = transform\n    self.clamp = clamp\n    self.scorer = scorer\n    self.beta_multiplier = beta_multiplier\n    self.beta_hinge = beta_hinge\n    self.beta_lqp = beta_lqp\n    self.beta_threshold = beta_threshold\n    self.beta_categorical = beta_categorical\n    self.n_hinge_features = n_hinge_features\n    self.n_threshold_features = n_threshold_features\n    self.convergence_tolerance = convergence_tolerance\n    self.n_cpus = n_cpus\n    self.use_lambdas = use_lambdas\n    self.n_lambdas = n_lambdas\n    self.class_weights = class_weights\n    self.use_sklearn = use_sklearn\n    self.random_state = random_state\n\n    # computed during model fitting\n    self.initialized_ = False\n    self.estimator = None\n    self.preprocessor = None\n    self.transformer = None\n    self.regularization_ = None\n    self.lambdas_ = None\n    self.beta_scores_ = None\n    self.entropy_ = 0.0\n    self.alpha_ = 0.0\n</code></pre>"},{"location":"module/models/#elapid.models.MaxentModel.fit","title":"<code>fit(x, y, sample_weight=None, categorical=None, labels=None, preprocessor=None)</code>","text":"<p>Trains a maxent model using a set of covariates and presence/background points.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array of shape (n_samples, n_features) with covariate data</p> required <code>y</code> <code>ArrayLike</code> <p>array of shape (n_samples,) with binary presence/background (1/0) values</p> required <code>sample_weight</code> <code>ArrayLike</code> <p>array of weights assigned to each sample with shape (n_samples,). this is modified by the <code>class_weights</code> model parameter unless you set <code>class_weights=None</code>.</p> <code>None</code> <code>categorical</code> <code>List[int]</code> <p>indices for which columns are categorical</p> <code>None</code> <code>labels</code> <code>list</code> <p>covariate labels. ignored if x is a pandas DataFrame</p> <code>None</code> <code>preprocessor</code> <code>BaseEstimator</code> <p>an <code>sklearn</code> transformer with a .transform() and/or a .fit_transform() method. Some examples include a PCA() object or a RobustScaler().</p> <code>None</code> Source code in <code>elapid/models.py</code> <pre><code>def fit(\n    self,\n    x: ArrayLike,\n    y: ArrayLike,\n    sample_weight: ArrayLike = None,\n    categorical: List[int] = None,\n    labels: list = None,\n    preprocessor: BaseEstimator = None,\n) -&gt; None:\n    \"\"\"Trains a maxent model using a set of covariates and presence/background points.\n\n    Args:\n        x: array of shape (n_samples, n_features) with covariate data\n        y: array of shape (n_samples,) with binary presence/background (1/0) values\n        sample_weight: array of weights assigned to each sample with shape (n_samples,).\n            this is modified by the `class_weights` model parameter unless\n            you set `class_weights=None`.\n        categorical: indices for which columns are categorical\n        labels: covariate labels. ignored if x is a pandas DataFrame\n        preprocessor: an `sklearn` transformer with a .transform() and/or\n            a .fit_transform() method. Some examples include a PCA() object or a\n            RobustScaler().\n    \"\"\"\n\n    # clear state variables\n    self.alpha_ = 0.0\n    self.entropy_ = 0.0\n\n    # format the input data\n    y = format_occurrence_data(y)\n\n    # apply preprocessing\n    if preprocessor is not None:\n        self.preprocessor = preprocessor\n        try:\n            x = self.preprocessor.transform(x)\n        except NotFittedError:\n            x = self.preprocessor.fit_transform(x)\n\n    # fit the feature transformer\n    self.feature_types = validate_feature_types(self.feature_types)\n    self.transformer = MaxentFeatureTransformer(\n        feature_types=self.feature_types,\n        clamp=self.clamp,\n        n_hinge_features=self.n_hinge_features,\n        n_threshold_features=self.n_threshold_features,\n    )\n    features = self.transformer.fit_transform(x, categorical=categorical, labels=labels)\n    feature_labels = self.transformer.feature_names_\n\n    # compute class weights\n    if self.class_weights is not None:\n        pbr = len(y) / y.sum() if self.class_weights == \"balanced\" else self.class_weights\n        class_weight = compute_weights(y, pbr=pbr)\n\n        # scale the sample weight\n        if sample_weight is None:\n            sample_weight = class_weight\n        else:\n            sample_weight *= class_weight\n\n    # model fitting with sklearn\n    if self.use_sklearn:\n        C = estimate_C_from_betas(self.beta_multiplier)\n        self.initialize_sklearn_model(C)\n        self.estimator.fit(features, y, sample_weight=sample_weight)\n        self.beta_scores_ = self.estimator.coef_[0]\n\n    # model fitting with glmnet\n    else:\n        # set feature regularization parameters\n        self.regularization_ = compute_regularization(\n            y,\n            features,\n            feature_labels=feature_labels,\n            beta_multiplier=self.beta_multiplier,\n            beta_lqp=self.beta_lqp,\n            beta_threshold=self.beta_threshold,\n            beta_hinge=self.beta_hinge,\n            beta_categorical=self.beta_categorical,\n        )\n\n        # get model lambda scores to initialize the glm\n        self.lambdas_ = compute_lambdas(y, sample_weight, self.regularization_, n_lambdas=self.n_lambdas)\n\n        # model fitting\n        self.initialize_glmnet_model(lambdas=self.lambdas_)\n        self.estimator.fit(\n            features,\n            y,\n            sample_weight=sample_weight,\n            relative_penalties=self.regularization_,\n        )\n\n        # get the beta values based on which lambda selection method to use\n        if self.use_lambdas == \"last\":\n            self.beta_scores_ = self.estimator.coef_path_[0, :, -1]\n        elif self.use_lambdas == \"best\":\n            self.beta_scores_ = self.estimator.coef_path_[0, :, self.estimator.lambda_max_inx_]\n\n    # store initialization state\n    self.initialized_ = True\n\n    # apply maxent-specific transformations\n    class_transform = self.get_params()[\"transform\"]\n    self.set_params(transform=\"raw\")\n    raw = self.predict(x[y == 0])\n    self.set_params(transform=class_transform)\n\n    # alpha is a normalizing constant that ensures that f1(z) integrates (sums) to 1\n    self.alpha_ = maxent_alpha(raw)\n\n    # the distance from f(z) is the relative entropy of f1(z) WRT f(z)\n    self.entropy_ = maxent_entropy(raw)\n\n    return self\n</code></pre>"},{"location":"module/models/#elapid.models.MaxentModel.fit_predict","title":"<code>fit_predict(x, y, categorical=None, labels=None, preprocessor=None)</code>","text":"<p>Trains and applies a model to x/y data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) with covariate data</p> required <code>y</code> <code>ArrayLike</code> <p>array-like of shape (n_samples,) with binary presence/background (1/0) values</p> required <code>categorical</code> <code>list</code> <p>column indices indicating which columns are categorical</p> <code>None</code> <code>labels</code> <code>list</code> <p>Covariate labels. Ignored if x is a pandas DataFrame</p> <code>None</code> <code>preprocessor</code> <code>BaseEstimator</code> <p>an <code>sklearn</code> transformer with a .transform() and/or a .fit_transform() method. Some examples include a PCA() object or a RobustScaler().</p> <code>None</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>ArrayLike</code> <p>Array-like of shape (n_samples,) with model predictions</p> Source code in <code>elapid/models.py</code> <pre><code>def fit_predict(\n    self,\n    x: ArrayLike,\n    y: ArrayLike,\n    categorical: list = None,\n    labels: list = None,\n    preprocessor: BaseEstimator = None,\n) -&gt; ArrayLike:\n    \"\"\"Trains and applies a model to x/y data.\n\n    Args:\n        x: array-like of shape (n_samples, n_features) with covariate data\n        y: array-like of shape (n_samples,) with binary presence/background (1/0) values\n        categorical: column indices indicating which columns are categorical\n        labels: Covariate labels. Ignored if x is a pandas DataFrame\n        preprocessor: an `sklearn` transformer with a .transform() and/or\n            a .fit_transform() method. Some examples include a PCA() object or a\n            RobustScaler().\n\n    Returns:\n        predictions: Array-like of shape (n_samples,) with model predictions\n    \"\"\"\n    self.fit(x, y, categorical=categorical, labels=labels, preprocessor=preprocessor)\n    predictions = self.predict(x)\n\n    return predictions\n</code></pre>"},{"location":"module/models/#elapid.models.MaxentModel.initialize_glmnet_model","title":"<code>initialize_glmnet_model(lambdas, alpha=1, standardize=False, fit_intercept=True)</code>","text":"<p>Creates the Logistic Regression with elastic net penalty model object.</p> <p>Parameters:</p> Name Type Description Default <code>lambdas</code> <code>array</code> <p>array of model lambda values. get from elapid.features.compute_lambdas()</p> required <code>alpha</code> <code>float</code> <p>elasticnet mixing parameter. alpha=1 for lasso, alpha=0 for ridge</p> <code>1</code> <code>standardize</code> <code>bool</code> <p>specify coefficient normalization</p> <code>False</code> <code>fit_intercept</code> <code>bool</code> <p>include an intercept parameter</p> <code>True</code> Source code in <code>elapid/models.py</code> <pre><code>def initialize_glmnet_model(\n    self,\n    lambdas: np.array,\n    alpha: float = 1,\n    standardize: bool = False,\n    fit_intercept: bool = True,\n) -&gt; None:\n    \"\"\"Creates the Logistic Regression with elastic net penalty model object.\n\n    Args:\n        lambdas: array of model lambda values. get from elapid.features.compute_lambdas()\n        alpha: elasticnet mixing parameter. alpha=1 for lasso, alpha=0 for ridge\n        standardize: specify coefficient normalization\n        fit_intercept: include an intercept parameter\n    \"\"\"\n    self.estimator = LogitNet(\n        alpha=alpha,\n        lambda_path=lambdas,\n        standardize=standardize,\n        fit_intercept=fit_intercept,\n        scoring=self.scorer,\n        n_jobs=self.n_cpus,\n        tol=self.convergence_tolerance,\n        random_state=self.random_state,\n    )\n</code></pre>"},{"location":"module/models/#elapid.models.MaxentModel.initialize_sklearn_model","title":"<code>initialize_sklearn_model(C, fit_intercept=True)</code>","text":"<p>Creates an sklearn Logisticregression estimator with L1 penalties.</p> <p>Parameters:</p> Name Type Description Default <code>C</code> <code>float</code> <p>the regularization parameter</p> required <code>fit_intercept</code> <code>bool</code> <p>include an intercept parameter</p> <code>True</code> Source code in <code>elapid/models.py</code> <pre><code>def initialize_sklearn_model(self, C: float, fit_intercept: bool = True) -&gt; None:\n    \"\"\"Creates an sklearn Logisticregression estimator with L1 penalties.\n\n    Args:\n        C: the regularization parameter\n        fit_intercept: include an intercept parameter\n    \"\"\"\n    self.estimator = LogisticRegression(\n        C=C,\n        fit_intercept=fit_intercept,\n        penalty=\"l1\",\n        solver=\"liblinear\",\n        tol=self.convergence_tolerance,\n        max_iter=self.n_lambdas,\n        random_state=self.random_state\n    )\n</code></pre>"},{"location":"module/models/#elapid.models.MaxentModel.predict","title":"<code>predict(x)</code>","text":"<p>Apply a model to a set of covariates or features. Requires that a model has been fit.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) with covariate data</p> required <p>Returns:</p> Name Type Description <code>predictions</code> <code>ArrayLike</code> <p>array-like of shape (n_samples,) with model predictions</p> Source code in <code>elapid/models.py</code> <pre><code>def predict(self, x: ArrayLike) -&gt; ArrayLike:\n    \"\"\"Apply a model to a set of covariates or features. Requires that a model has been fit.\n\n    Args:\n        x: array-like of shape (n_samples, n_features) with covariate data\n\n    Returns:\n        predictions: array-like of shape (n_samples,) with model predictions\n    \"\"\"\n    if not self.initialized_:\n        raise NotFittedError(\"Model must be fit first\")\n\n    # feature transformations\n    x = x if self.preprocessor is None else self.preprocessor.transform(x)\n    features = x if self.transformer is None else self.transformer.transform(x)\n\n    # apply the model\n    engma = np.matmul(features, self.beta_scores_) + self.alpha_\n\n    # scale based on the transform type\n    if self.transform == \"raw\":\n        return maxent_raw_transform(engma)\n\n    elif self.transform == \"logistic\":\n        return maxent_logistic_transform(engma, self.entropy_, self.tau)\n\n    elif self.transform == \"cloglog\":\n        return maxent_cloglog_transform(engma, self.entropy_)\n</code></pre>"},{"location":"module/models/#elapid.models.MaxentModel.predict_proba","title":"<code>predict_proba(x)</code>","text":"<p>Compute prediction probability scores for the 0/1 classes.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) with covariate data</p> required <p>Returns:</p> Name Type Description <code>predictions</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, 2) with model predictions</p> Source code in <code>elapid/models.py</code> <pre><code>def predict_proba(self, x: ArrayLike) -&gt; ArrayLike:\n    \"\"\"Compute prediction probability scores for the 0/1 classes.\n\n    Args:\n        x: array-like of shape (n_samples, n_features) with covariate data\n\n    Returns:\n        predictions: array-like of shape (n_samples, 2) with model predictions\n    \"\"\"\n    ypred = self.predict(x).reshape(-1, 1)\n    predictions = np.hstack((1 - ypred, ypred))\n\n    return predictions\n</code></pre>"},{"location":"module/models/#elapid.models.NicheEnvelopeModel","title":"<code>NicheEnvelopeModel</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>SDMMixin</code>, <code>FeaturesMixin</code></p> <p>Model estimator for niche envelope-style models.</p> Source code in <code>elapid/models.py</code> <pre><code>class NicheEnvelopeModel(BaseEstimator, SDMMixin, FeaturesMixin):\n    \"\"\"Model estimator for niche envelope-style models.\"\"\"\n\n    def __init__(\n        self,\n        percentile_range: Tuple[float, float] = NicheEnvelopeConfig.percentile_range,\n        overlay: str = NicheEnvelopeConfig.overlay,\n    ):\n        \"\"\"Create a niche envelope model estimator.\n\n        Args:\n            percentile_range: covariate values within this range are flagged as suitable habitat\n                using a narrow range like [10, 90] drops more areas from suitability maps\n                while [0, 100] creates an envelope around the full range of observed\n                covariates at all y==1 locations.\n            overlay: niche envelope overlap type.\n                select from [\"average\", \"intersection\", \"union\"]\n        \"\"\"\n        self.percentile_range = percentile_range\n        self.overlay = overlay\n        self.feature_mins_ = None\n        self.feature_maxs_ = None\n        self.categorical_estimator = None\n        self.categorical_ = None\n        self.continuous_ = None\n        self.categorical_pd_ = None\n        self.continuous_pd_ = None\n        self.in_categorical_ = None\n\n    def fit(self, x: ArrayLike, y: ArrayLike, categorical: list = None, labels: list = None) -&gt; None:\n        \"\"\"Fits a niche envelope model using a set of covariates and presence/background points.\n\n        Args:\n            x: array-like of shape (n_samples, n_features) with covariate data\n            y: array-like of shape (n_samples,) with binary presence/background (1/0) values\n            categorical: indices for which columns are categorical\n            labels: covariate labels. ignored if x is a pandas DataFrame\n        \"\"\"\n        # format the input x/y data\n        self._format_labels_and_dtypes(x, categorical=categorical, labels=labels)\n        con, cat = self._format_covariate_data(x)\n        y = format_occurrence_data(y)\n\n        # estimate the feature range of the continuous data\n        self.feature_mins_ = np.percentile(con[y == 1], self.percentile_range[0], axis=0)\n        self.feature_maxs_ = np.percentile(con[y == 1], self.percentile_range[1], axis=0)\n\n        # one-hot encode the categorical data and label the classes with\n        if cat is not None:\n            self.categorical_estimator = CategoricalTransformer()\n            ohe = self.categorical_estimator.fit_transform(cat)\n            self.in_categorical_ = np.any(ohe[y == 1], axis=0)\n\n        return self\n\n    def predict(self, x: ArrayLike) -&gt; np.ndarray:\n        \"\"\"Applies a model to a set of covariates or features. Requires that a model has been fit.\n\n        Args:\n            x: array-like of shape (n_samples, n_features) with covariate data\n\n        Returns:\n            array-like of shape (n_samples,) with model predictions\n        \"\"\"\n        overlay = self.overlay.lower()\n        overlay_options = [\"average\", \"intersection\", \"union\"]\n        assert overlay in overlay_options, f\"overlay must be one of {', '.join(overlay_options)}\"\n\n        # format the input data\n        con, cat = self._format_covariate_data(x)\n        nrows, ncols = x.shape\n\n        # any value within the transformed range is considered within the envelope\n        in_range = (con &gt;= self.feature_mins_) * (con &lt;= self.feature_maxs_)\n\n        # map the class locations where the species has been observed\n        if cat is not None:\n            ohe = self.categorical_estimator.transform(cat)\n            should_be_here = ohe[:, self.in_categorical_].any(axis=1).reshape((nrows, 1))\n            shouldnt_be_here = (~ohe[:, ~self.in_categorical_].any(axis=1)).reshape((nrows, 1))\n            in_range = np.concatenate((in_range, should_be_here, shouldnt_be_here), axis=1)\n\n        # comput envelope based on the overlay method\n        if overlay == \"average\":\n            ypred = np.mean(in_range, axis=1, dtype=\"float32\")\n\n        elif overlay == \"intersection\":\n            ypred = np.all(in_range, axis=1).astype(\"uint8\")\n\n        elif overlay == \"union\":\n            ypred = np.any(in_range, axis=1).astype(\"uint8\")\n\n        return ypred\n\n    def predict_proba(self, x: ArrayLike) -&gt; ArrayLike:\n        \"\"\"Compute prediction probability scores for the 0/1 classes.\n\n        Args:\n            x: array-like of shape (n_samples, n_features) with covariate data\n\n        Returns:\n            predictions: array-like of shape (n_samples, 2) with model predictions\n        \"\"\"\n        ypred = self.predict(x).reshape(-1, 1)\n        predictions = np.hstack((1 - ypred, ypred))\n\n        return predictions\n\n    def fit_predict(self, x: ArrayLike, y: ArrayLike, categorical: list = None, labels: list = None) -&gt; np.ndarray:\n        \"\"\"Trains and applies a model to x/y data.\n\n        Args:\n            x: array-like of shape (n_samples, n_features) with covariate data\n            y: array-like of shape (n_samples,) with binary presence/background (1/0) values\n            categorical: column indices indicating which columns are categorical\n            labels: Covariate labels. Ignored if x is a pandas DataFrame\n\n        Returns:\n            array-like of shape (n_samples,) with model predictions\n        \"\"\"\n        self.fit(x, y, categorical=categorical, labels=labels)\n        return self.predict(x)\n</code></pre>"},{"location":"module/models/#elapid.models.NicheEnvelopeModel.__init__","title":"<code>__init__(percentile_range=NicheEnvelopeConfig.percentile_range, overlay=NicheEnvelopeConfig.overlay)</code>","text":"<p>Create a niche envelope model estimator.</p> <p>Parameters:</p> Name Type Description Default <code>percentile_range</code> <code>Tuple[float, float]</code> <p>covariate values within this range are flagged as suitable habitat using a narrow range like [10, 90] drops more areas from suitability maps while [0, 100] creates an envelope around the full range of observed covariates at all y==1 locations.</p> <code>percentile_range</code> <code>overlay</code> <code>str</code> <p>niche envelope overlap type. select from [\"average\", \"intersection\", \"union\"]</p> <code>overlay</code> Source code in <code>elapid/models.py</code> <pre><code>def __init__(\n    self,\n    percentile_range: Tuple[float, float] = NicheEnvelopeConfig.percentile_range,\n    overlay: str = NicheEnvelopeConfig.overlay,\n):\n    \"\"\"Create a niche envelope model estimator.\n\n    Args:\n        percentile_range: covariate values within this range are flagged as suitable habitat\n            using a narrow range like [10, 90] drops more areas from suitability maps\n            while [0, 100] creates an envelope around the full range of observed\n            covariates at all y==1 locations.\n        overlay: niche envelope overlap type.\n            select from [\"average\", \"intersection\", \"union\"]\n    \"\"\"\n    self.percentile_range = percentile_range\n    self.overlay = overlay\n    self.feature_mins_ = None\n    self.feature_maxs_ = None\n    self.categorical_estimator = None\n    self.categorical_ = None\n    self.continuous_ = None\n    self.categorical_pd_ = None\n    self.continuous_pd_ = None\n    self.in_categorical_ = None\n</code></pre>"},{"location":"module/models/#elapid.models.NicheEnvelopeModel.fit","title":"<code>fit(x, y, categorical=None, labels=None)</code>","text":"<p>Fits a niche envelope model using a set of covariates and presence/background points.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) with covariate data</p> required <code>y</code> <code>ArrayLike</code> <p>array-like of shape (n_samples,) with binary presence/background (1/0) values</p> required <code>categorical</code> <code>list</code> <p>indices for which columns are categorical</p> <code>None</code> <code>labels</code> <code>list</code> <p>covariate labels. ignored if x is a pandas DataFrame</p> <code>None</code> Source code in <code>elapid/models.py</code> <pre><code>def fit(self, x: ArrayLike, y: ArrayLike, categorical: list = None, labels: list = None) -&gt; None:\n    \"\"\"Fits a niche envelope model using a set of covariates and presence/background points.\n\n    Args:\n        x: array-like of shape (n_samples, n_features) with covariate data\n        y: array-like of shape (n_samples,) with binary presence/background (1/0) values\n        categorical: indices for which columns are categorical\n        labels: covariate labels. ignored if x is a pandas DataFrame\n    \"\"\"\n    # format the input x/y data\n    self._format_labels_and_dtypes(x, categorical=categorical, labels=labels)\n    con, cat = self._format_covariate_data(x)\n    y = format_occurrence_data(y)\n\n    # estimate the feature range of the continuous data\n    self.feature_mins_ = np.percentile(con[y == 1], self.percentile_range[0], axis=0)\n    self.feature_maxs_ = np.percentile(con[y == 1], self.percentile_range[1], axis=0)\n\n    # one-hot encode the categorical data and label the classes with\n    if cat is not None:\n        self.categorical_estimator = CategoricalTransformer()\n        ohe = self.categorical_estimator.fit_transform(cat)\n        self.in_categorical_ = np.any(ohe[y == 1], axis=0)\n\n    return self\n</code></pre>"},{"location":"module/models/#elapid.models.NicheEnvelopeModel.fit_predict","title":"<code>fit_predict(x, y, categorical=None, labels=None)</code>","text":"<p>Trains and applies a model to x/y data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) with covariate data</p> required <code>y</code> <code>ArrayLike</code> <p>array-like of shape (n_samples,) with binary presence/background (1/0) values</p> required <code>categorical</code> <code>list</code> <p>column indices indicating which columns are categorical</p> <code>None</code> <code>labels</code> <code>list</code> <p>Covariate labels. Ignored if x is a pandas DataFrame</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>array-like of shape (n_samples,) with model predictions</p> Source code in <code>elapid/models.py</code> <pre><code>def fit_predict(self, x: ArrayLike, y: ArrayLike, categorical: list = None, labels: list = None) -&gt; np.ndarray:\n    \"\"\"Trains and applies a model to x/y data.\n\n    Args:\n        x: array-like of shape (n_samples, n_features) with covariate data\n        y: array-like of shape (n_samples,) with binary presence/background (1/0) values\n        categorical: column indices indicating which columns are categorical\n        labels: Covariate labels. Ignored if x is a pandas DataFrame\n\n    Returns:\n        array-like of shape (n_samples,) with model predictions\n    \"\"\"\n    self.fit(x, y, categorical=categorical, labels=labels)\n    return self.predict(x)\n</code></pre>"},{"location":"module/models/#elapid.models.NicheEnvelopeModel.predict","title":"<code>predict(x)</code>","text":"<p>Applies a model to a set of covariates or features. Requires that a model has been fit.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) with covariate data</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>array-like of shape (n_samples,) with model predictions</p> Source code in <code>elapid/models.py</code> <pre><code>def predict(self, x: ArrayLike) -&gt; np.ndarray:\n    \"\"\"Applies a model to a set of covariates or features. Requires that a model has been fit.\n\n    Args:\n        x: array-like of shape (n_samples, n_features) with covariate data\n\n    Returns:\n        array-like of shape (n_samples,) with model predictions\n    \"\"\"\n    overlay = self.overlay.lower()\n    overlay_options = [\"average\", \"intersection\", \"union\"]\n    assert overlay in overlay_options, f\"overlay must be one of {', '.join(overlay_options)}\"\n\n    # format the input data\n    con, cat = self._format_covariate_data(x)\n    nrows, ncols = x.shape\n\n    # any value within the transformed range is considered within the envelope\n    in_range = (con &gt;= self.feature_mins_) * (con &lt;= self.feature_maxs_)\n\n    # map the class locations where the species has been observed\n    if cat is not None:\n        ohe = self.categorical_estimator.transform(cat)\n        should_be_here = ohe[:, self.in_categorical_].any(axis=1).reshape((nrows, 1))\n        shouldnt_be_here = (~ohe[:, ~self.in_categorical_].any(axis=1)).reshape((nrows, 1))\n        in_range = np.concatenate((in_range, should_be_here, shouldnt_be_here), axis=1)\n\n    # comput envelope based on the overlay method\n    if overlay == \"average\":\n        ypred = np.mean(in_range, axis=1, dtype=\"float32\")\n\n    elif overlay == \"intersection\":\n        ypred = np.all(in_range, axis=1).astype(\"uint8\")\n\n    elif overlay == \"union\":\n        ypred = np.any(in_range, axis=1).astype(\"uint8\")\n\n    return ypred\n</code></pre>"},{"location":"module/models/#elapid.models.NicheEnvelopeModel.predict_proba","title":"<code>predict_proba(x)</code>","text":"<p>Compute prediction probability scores for the 0/1 classes.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, n_features) with covariate data</p> required <p>Returns:</p> Name Type Description <code>predictions</code> <code>ArrayLike</code> <p>array-like of shape (n_samples, 2) with model predictions</p> Source code in <code>elapid/models.py</code> <pre><code>def predict_proba(self, x: ArrayLike) -&gt; ArrayLike:\n    \"\"\"Compute prediction probability scores for the 0/1 classes.\n\n    Args:\n        x: array-like of shape (n_samples, n_features) with covariate data\n\n    Returns:\n        predictions: array-like of shape (n_samples, 2) with model predictions\n    \"\"\"\n    ypred = self.predict(x).reshape(-1, 1)\n    predictions = np.hstack((1 - ypred, ypred))\n\n    return predictions\n</code></pre>"},{"location":"module/models/#elapid.models.SDMMixin","title":"<code>SDMMixin</code>","text":"<p>Mixin class for SDM classifiers.</p> Source code in <code>elapid/models.py</code> <pre><code>class SDMMixin:\n    \"\"\"Mixin class for SDM classifiers.\"\"\"\n\n    _estimator_type = \"classifier\"\n    classes_ = [0, 1]\n\n    def score(self, x: ArrayLike, y: ArrayLike, sample_weight: ArrayLike = None) -&gt; float:\n        \"\"\"Return the mean AUC score on the given test data and labels.\n\n        Args:\n            x: test samples. array-like of shape (n_samples, n_features).\n            y: presence/absence labels. array-like of shape (n_samples,).\n            sample_weight: array-like of shape (n_samples,)\n\n        Returns:\n            AUC score of `self.predict(x)` w.r.t. `y`.\n        \"\"\"\n        return roc_auc_score(y, self.predict(x), sample_weight=sample_weight)\n\n    def _more_tags(self):\n        return {\"requires_y\": True}\n\n    def permutation_importance_scores(\n        self,\n        x: ArrayLike,\n        y: ArrayLike,\n        sample_weight: ArrayLike = None,\n        n_repeats: int = 10,\n        n_jobs: int = -1,\n    ) -&gt; np.ndarray:\n        \"\"\"Compute a generic feature importance score by modifying feature values\n            and computing the relative change in model performance.\n\n        Permutation importance measures how much a model score decreases when a\n            single feature value is randomly shuffled. This score doesn't reflect\n            the intrinsic predictive value of a feature by itself, but how important\n            feature is for a particular model.\n\n        Args:\n            x: test samples. array-like of shape (n_samples, n_features).\n            y: presence/absence labels. array-like of shape (n_samples,).\n            sample_weight: array-like of shape (n_samples,)\n            n_repeats: number of permutation iterations.\n            n_jobs: number of parallel compute tasks. set to -1 for all cpus.\n\n        Returns:\n            importances: an array of shape (n_features, n_repeats).\n        \"\"\"\n        pi = permutation_importance(self, x, y, sample_weight=sample_weight, n_jobs=n_jobs, n_repeats=n_repeats)\n\n        return pi.importances\n\n    def permutation_importance_plot(\n        self,\n        x: ArrayLike,\n        y: ArrayLike,\n        sample_weight: ArrayLike = None,\n        n_repeats: int = 10,\n        labels: list = None,\n        **kwargs,\n    ) -&gt; Tuple[plt.Figure, plt.Axes]:\n        \"\"\"Create a box plot with bootstrapped permutation importance scores for each covariate.\n\n        Permutation importance measures how much a model score decreases when a\n            single feature value is randomly shuffled. This score doesn't reflect\n            the intrinsic predictive value of a feature by itself, but how important\n            feature is for a particular model.\n\n        It is often appropriate to compute permuation importance scores using both\n            training and validation sets. Large differences between the two may\n            indicate overfitting.\n\n        This implementation does not necessarily match the implementation in Maxent.\n            These scores may be difficult to interpret if there is a high degree\n            of covariance between features or if the model estimator includes any\n            non-linear feature transformations (e.g. 'hinge' features).\n\n        Reference:\n            https://scikit-learn.org/stable/modules/permutation_importance.html\n\n        Args:\n            x: evaluation features. array-like of shape (n_samples, n_features).\n            y: presence/absence labels. array-like of shape (n_samples,).\n            sample_weight: array-like of shape (n_samples,)\n            n_repeats: number of permutation iterations.\n            labels: list of band names to label the plots.\n            **kwargs: additional arguments to pass to `plt.subplots()`.\n\n        Returns:\n            fig, ax: matplotlib subplot figure and axes.\n        \"\"\"\n        importance = self.permutation_importance_scores(x, y, sample_weight=sample_weight, n_repeats=n_repeats)\n        rank_order = importance.mean(axis=-1).argsort()\n\n        if labels is None:\n            try:\n                labels = x.columns.tolist()\n            except AttributeError:\n                labels = make_band_labels(x.shape[-1])\n        labels = [labels[idx] for idx in rank_order]\n\n        plot_defaults = {\"dpi\": 150, \"figsize\": (5, 4)}\n        plot_defaults.update(**kwargs)\n        fig, ax = plt.subplots(**plot_defaults)\n        ax.boxplot(\n            importance[rank_order].T,\n            vert=False,\n            labels=labels,\n        )\n        fig.tight_layout()\n\n        return fig, ax\n\n    def partial_dependence_scores(\n        self,\n        x: ArrayLike,\n        percentiles: tuple = (0.025, 0.975),\n        n_bins: int = 100,\n        categorical_features: tuple = [None],\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Compute partial dependence scores for each feature.\n\n        Args:\n            x: evaluation features. array-like of shape (n_samples, n_features).\n                used to constrain the range of values to evaluate for each feature.\n            percentiles: lower and upper percentiles used to set the range to plot.\n            n_bins: the number of bins spanning the lower-upper percentile range.\n            categorical_features: a 0-based index of which features are categorical.\n\n        Returns:\n            bins, mean, stdv: the binned feature values and the mean/stdv of responses.\n        \"\"\"\n        ncols = x.shape[1]\n        mean = np.zeros((ncols, n_bins))\n        stdv = np.zeros_like(mean)\n        bins = np.zeros_like(mean)\n\n        for idx in range(ncols):\n            if idx in categorical_features:\n                continue\n            pd = partial_dependence(\n                self,\n                x,\n                [idx],\n                percentiles=percentiles,\n                grid_resolution=n_bins,\n                kind=\"individual\",\n            )\n            mean[idx] = pd[\"individual\"][0].mean(axis=0)\n            stdv[idx] = pd[\"individual\"][0].std(axis=0)\n            bins[idx] = pd[\"grid_values\"][0]\n\n        return bins, mean, stdv\n\n    def partial_dependence_plot(\n        self,\n        x: ArrayLike,\n        percentiles: tuple = (0.025, 0.975),\n        n_bins: int = 50,\n        categorical_features: tuple = None,\n        labels: list = None,\n        **kwargs,\n    ) -&gt; Tuple[plt.Figure, plt.Axes]:\n        \"\"\"Plot the response of an estimator across the range of feature values.\n\n        Args:\n            x: evaluation features. array-like of shape (n_samples, n_features).\n                used to constrain the range of values to evaluate for each feature.\n            percentiles: lower and upper percentiles used to set the range to plot.\n            n_bins: the number of bins spanning the lower-upper percentile range.\n            categorical_features: a 0-based index of which features are categorical.\n            labels: list of band names to label the plots.\n            **kwargs: additional arguments to pass to `plt.subplots()`.\n\n        Returns:\n            fig, ax: matplotlib subplot figure and axes.\n        \"\"\"\n        # skip categorical features for now\n        if categorical_features is None:\n            try:\n                categorical_features = self.transformer.categorical_ or [None]\n            except AttributeError:\n                categorical_features = [None]\n\n        bins, mean, stdv = self.partial_dependence_scores(\n            x, percentiles=percentiles, n_bins=n_bins, categorical_features=categorical_features\n        )\n\n        if labels is None:\n            try:\n                labels = x.columns.tolist()\n            except AttributeError:\n                labels = make_band_labels(x.shape[-1])\n\n        ncols = x.shape[1]\n        figx = int(np.ceil(np.sqrt(ncols)))\n        figy = int(np.ceil(ncols / figx))\n        fig, ax = plt.subplots(figx, figy, **kwargs)\n        ax = ax.flatten()\n\n        for idx in range(ncols):\n            ax[idx].fill_between(bins[idx], mean[idx] - stdv[idx], mean[idx] + stdv[idx], alpha=0.25)\n            ax[idx].plot(bins[idx], mean[idx])\n            ax[idx].set_title(labels[idx])\n\n        # turn off empty plots\n        for axi in ax:\n            if not axi.lines:\n                axi.set_visible(False)\n\n        fig.tight_layout()\n\n        return fig, ax\n</code></pre>"},{"location":"module/models/#elapid.models.SDMMixin.partial_dependence_plot","title":"<code>partial_dependence_plot(x, percentiles=(0.025, 0.975), n_bins=50, categorical_features=None, labels=None, **kwargs)</code>","text":"<p>Plot the response of an estimator across the range of feature values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>evaluation features. array-like of shape (n_samples, n_features). used to constrain the range of values to evaluate for each feature.</p> required <code>percentiles</code> <code>tuple</code> <p>lower and upper percentiles used to set the range to plot.</p> <code>(0.025, 0.975)</code> <code>n_bins</code> <code>int</code> <p>the number of bins spanning the lower-upper percentile range.</p> <code>50</code> <code>categorical_features</code> <code>tuple</code> <p>a 0-based index of which features are categorical.</p> <code>None</code> <code>labels</code> <code>list</code> <p>list of band names to label the plots.</p> <code>None</code> <code>**kwargs</code> <p>additional arguments to pass to <code>plt.subplots()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Figure, Axes]</code> <p>fig, ax: matplotlib subplot figure and axes.</p> Source code in <code>elapid/models.py</code> <pre><code>def partial_dependence_plot(\n    self,\n    x: ArrayLike,\n    percentiles: tuple = (0.025, 0.975),\n    n_bins: int = 50,\n    categorical_features: tuple = None,\n    labels: list = None,\n    **kwargs,\n) -&gt; Tuple[plt.Figure, plt.Axes]:\n    \"\"\"Plot the response of an estimator across the range of feature values.\n\n    Args:\n        x: evaluation features. array-like of shape (n_samples, n_features).\n            used to constrain the range of values to evaluate for each feature.\n        percentiles: lower and upper percentiles used to set the range to plot.\n        n_bins: the number of bins spanning the lower-upper percentile range.\n        categorical_features: a 0-based index of which features are categorical.\n        labels: list of band names to label the plots.\n        **kwargs: additional arguments to pass to `plt.subplots()`.\n\n    Returns:\n        fig, ax: matplotlib subplot figure and axes.\n    \"\"\"\n    # skip categorical features for now\n    if categorical_features is None:\n        try:\n            categorical_features = self.transformer.categorical_ or [None]\n        except AttributeError:\n            categorical_features = [None]\n\n    bins, mean, stdv = self.partial_dependence_scores(\n        x, percentiles=percentiles, n_bins=n_bins, categorical_features=categorical_features\n    )\n\n    if labels is None:\n        try:\n            labels = x.columns.tolist()\n        except AttributeError:\n            labels = make_band_labels(x.shape[-1])\n\n    ncols = x.shape[1]\n    figx = int(np.ceil(np.sqrt(ncols)))\n    figy = int(np.ceil(ncols / figx))\n    fig, ax = plt.subplots(figx, figy, **kwargs)\n    ax = ax.flatten()\n\n    for idx in range(ncols):\n        ax[idx].fill_between(bins[idx], mean[idx] - stdv[idx], mean[idx] + stdv[idx], alpha=0.25)\n        ax[idx].plot(bins[idx], mean[idx])\n        ax[idx].set_title(labels[idx])\n\n    # turn off empty plots\n    for axi in ax:\n        if not axi.lines:\n            axi.set_visible(False)\n\n    fig.tight_layout()\n\n    return fig, ax\n</code></pre>"},{"location":"module/models/#elapid.models.SDMMixin.partial_dependence_scores","title":"<code>partial_dependence_scores(x, percentiles=(0.025, 0.975), n_bins=100, categorical_features=[None])</code>","text":"<p>Compute partial dependence scores for each feature.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>evaluation features. array-like of shape (n_samples, n_features). used to constrain the range of values to evaluate for each feature.</p> required <code>percentiles</code> <code>tuple</code> <p>lower and upper percentiles used to set the range to plot.</p> <code>(0.025, 0.975)</code> <code>n_bins</code> <code>int</code> <p>the number of bins spanning the lower-upper percentile range.</p> <code>100</code> <code>categorical_features</code> <code>tuple</code> <p>a 0-based index of which features are categorical.</p> <code>[None]</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>bins, mean, stdv: the binned feature values and the mean/stdv of responses.</p> Source code in <code>elapid/models.py</code> <pre><code>def partial_dependence_scores(\n    self,\n    x: ArrayLike,\n    percentiles: tuple = (0.025, 0.975),\n    n_bins: int = 100,\n    categorical_features: tuple = [None],\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Compute partial dependence scores for each feature.\n\n    Args:\n        x: evaluation features. array-like of shape (n_samples, n_features).\n            used to constrain the range of values to evaluate for each feature.\n        percentiles: lower and upper percentiles used to set the range to plot.\n        n_bins: the number of bins spanning the lower-upper percentile range.\n        categorical_features: a 0-based index of which features are categorical.\n\n    Returns:\n        bins, mean, stdv: the binned feature values and the mean/stdv of responses.\n    \"\"\"\n    ncols = x.shape[1]\n    mean = np.zeros((ncols, n_bins))\n    stdv = np.zeros_like(mean)\n    bins = np.zeros_like(mean)\n\n    for idx in range(ncols):\n        if idx in categorical_features:\n            continue\n        pd = partial_dependence(\n            self,\n            x,\n            [idx],\n            percentiles=percentiles,\n            grid_resolution=n_bins,\n            kind=\"individual\",\n        )\n        mean[idx] = pd[\"individual\"][0].mean(axis=0)\n        stdv[idx] = pd[\"individual\"][0].std(axis=0)\n        bins[idx] = pd[\"grid_values\"][0]\n\n    return bins, mean, stdv\n</code></pre>"},{"location":"module/models/#elapid.models.SDMMixin.permutation_importance_plot","title":"<code>permutation_importance_plot(x, y, sample_weight=None, n_repeats=10, labels=None, **kwargs)</code>","text":"<p>Create a box plot with bootstrapped permutation importance scores for each covariate.</p> <p>Permutation importance measures how much a model score decreases when a     single feature value is randomly shuffled. This score doesn't reflect     the intrinsic predictive value of a feature by itself, but how important     feature is for a particular model.</p> <p>It is often appropriate to compute permuation importance scores using both     training and validation sets. Large differences between the two may     indicate overfitting.</p> <p>This implementation does not necessarily match the implementation in Maxent.     These scores may be difficult to interpret if there is a high degree     of covariance between features or if the model estimator includes any     non-linear feature transformations (e.g. 'hinge' features).</p> Reference <p>scikit-learn.org/stable/modules/permutation_importance.html</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>evaluation features. array-like of shape (n_samples, n_features).</p> required <code>y</code> <code>ArrayLike</code> <p>presence/absence labels. array-like of shape (n_samples,).</p> required <code>sample_weight</code> <code>ArrayLike</code> <p>array-like of shape (n_samples,)</p> <code>None</code> <code>n_repeats</code> <code>int</code> <p>number of permutation iterations.</p> <code>10</code> <code>labels</code> <code>list</code> <p>list of band names to label the plots.</p> <code>None</code> <code>**kwargs</code> <p>additional arguments to pass to <code>plt.subplots()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Figure, Axes]</code> <p>fig, ax: matplotlib subplot figure and axes.</p> Source code in <code>elapid/models.py</code> <pre><code>def permutation_importance_plot(\n    self,\n    x: ArrayLike,\n    y: ArrayLike,\n    sample_weight: ArrayLike = None,\n    n_repeats: int = 10,\n    labels: list = None,\n    **kwargs,\n) -&gt; Tuple[plt.Figure, plt.Axes]:\n    \"\"\"Create a box plot with bootstrapped permutation importance scores for each covariate.\n\n    Permutation importance measures how much a model score decreases when a\n        single feature value is randomly shuffled. This score doesn't reflect\n        the intrinsic predictive value of a feature by itself, but how important\n        feature is for a particular model.\n\n    It is often appropriate to compute permuation importance scores using both\n        training and validation sets. Large differences between the two may\n        indicate overfitting.\n\n    This implementation does not necessarily match the implementation in Maxent.\n        These scores may be difficult to interpret if there is a high degree\n        of covariance between features or if the model estimator includes any\n        non-linear feature transformations (e.g. 'hinge' features).\n\n    Reference:\n        https://scikit-learn.org/stable/modules/permutation_importance.html\n\n    Args:\n        x: evaluation features. array-like of shape (n_samples, n_features).\n        y: presence/absence labels. array-like of shape (n_samples,).\n        sample_weight: array-like of shape (n_samples,)\n        n_repeats: number of permutation iterations.\n        labels: list of band names to label the plots.\n        **kwargs: additional arguments to pass to `plt.subplots()`.\n\n    Returns:\n        fig, ax: matplotlib subplot figure and axes.\n    \"\"\"\n    importance = self.permutation_importance_scores(x, y, sample_weight=sample_weight, n_repeats=n_repeats)\n    rank_order = importance.mean(axis=-1).argsort()\n\n    if labels is None:\n        try:\n            labels = x.columns.tolist()\n        except AttributeError:\n            labels = make_band_labels(x.shape[-1])\n    labels = [labels[idx] for idx in rank_order]\n\n    plot_defaults = {\"dpi\": 150, \"figsize\": (5, 4)}\n    plot_defaults.update(**kwargs)\n    fig, ax = plt.subplots(**plot_defaults)\n    ax.boxplot(\n        importance[rank_order].T,\n        vert=False,\n        labels=labels,\n    )\n    fig.tight_layout()\n\n    return fig, ax\n</code></pre>"},{"location":"module/models/#elapid.models.SDMMixin.permutation_importance_scores","title":"<code>permutation_importance_scores(x, y, sample_weight=None, n_repeats=10, n_jobs=-1)</code>","text":"<p>Compute a generic feature importance score by modifying feature values     and computing the relative change in model performance.</p> <p>Permutation importance measures how much a model score decreases when a     single feature value is randomly shuffled. This score doesn't reflect     the intrinsic predictive value of a feature by itself, but how important     feature is for a particular model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>test samples. array-like of shape (n_samples, n_features).</p> required <code>y</code> <code>ArrayLike</code> <p>presence/absence labels. array-like of shape (n_samples,).</p> required <code>sample_weight</code> <code>ArrayLike</code> <p>array-like of shape (n_samples,)</p> <code>None</code> <code>n_repeats</code> <code>int</code> <p>number of permutation iterations.</p> <code>10</code> <code>n_jobs</code> <code>int</code> <p>number of parallel compute tasks. set to -1 for all cpus.</p> <code>-1</code> <p>Returns:</p> Name Type Description <code>importances</code> <code>ndarray</code> <p>an array of shape (n_features, n_repeats).</p> Source code in <code>elapid/models.py</code> <pre><code>def permutation_importance_scores(\n    self,\n    x: ArrayLike,\n    y: ArrayLike,\n    sample_weight: ArrayLike = None,\n    n_repeats: int = 10,\n    n_jobs: int = -1,\n) -&gt; np.ndarray:\n    \"\"\"Compute a generic feature importance score by modifying feature values\n        and computing the relative change in model performance.\n\n    Permutation importance measures how much a model score decreases when a\n        single feature value is randomly shuffled. This score doesn't reflect\n        the intrinsic predictive value of a feature by itself, but how important\n        feature is for a particular model.\n\n    Args:\n        x: test samples. array-like of shape (n_samples, n_features).\n        y: presence/absence labels. array-like of shape (n_samples,).\n        sample_weight: array-like of shape (n_samples,)\n        n_repeats: number of permutation iterations.\n        n_jobs: number of parallel compute tasks. set to -1 for all cpus.\n\n    Returns:\n        importances: an array of shape (n_features, n_repeats).\n    \"\"\"\n    pi = permutation_importance(self, x, y, sample_weight=sample_weight, n_jobs=n_jobs, n_repeats=n_repeats)\n\n    return pi.importances\n</code></pre>"},{"location":"module/models/#elapid.models.SDMMixin.score","title":"<code>score(x, y, sample_weight=None)</code>","text":"<p>Return the mean AUC score on the given test data and labels.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>test samples. array-like of shape (n_samples, n_features).</p> required <code>y</code> <code>ArrayLike</code> <p>presence/absence labels. array-like of shape (n_samples,).</p> required <code>sample_weight</code> <code>ArrayLike</code> <p>array-like of shape (n_samples,)</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>AUC score of <code>self.predict(x)</code> w.r.t. <code>y</code>.</p> Source code in <code>elapid/models.py</code> <pre><code>def score(self, x: ArrayLike, y: ArrayLike, sample_weight: ArrayLike = None) -&gt; float:\n    \"\"\"Return the mean AUC score on the given test data and labels.\n\n    Args:\n        x: test samples. array-like of shape (n_samples, n_features).\n        y: presence/absence labels. array-like of shape (n_samples,).\n        sample_weight: array-like of shape (n_samples,)\n\n    Returns:\n        AUC score of `self.predict(x)` w.r.t. `y`.\n    \"\"\"\n    return roc_auc_score(y, self.predict(x), sample_weight=sample_weight)\n</code></pre>"},{"location":"module/models/#elapid.models.estimate_C_from_betas","title":"<code>estimate_C_from_betas(beta_multiplier)</code>","text":"<p>Convert the maxent-format beta_multiplier to an sklearn-format C regularization parameter.</p> <p>Parameters:</p> Name Type Description Default <code>beta_multiplier</code> <code>float</code> <p>the maxent beta regularization scaler</p> required <p>Returns:</p> Type Description <code>float</code> <p>a C factor approximating the level of regularization passed to glmnet</p> Source code in <code>elapid/models.py</code> <pre><code>def estimate_C_from_betas(beta_multiplier: float) -&gt; float:\n    \"\"\"Convert the maxent-format beta_multiplier to an sklearn-format C regularization parameter.\n\n    Args:\n        beta_multiplier: the maxent beta regularization scaler\n\n    Returns:\n        a C factor approximating the level of regularization passed to glmnet\n    \"\"\"\n    return 2 / (1 - np.exp(-beta_multiplier))\n</code></pre>"},{"location":"module/models/#elapid.models.format_occurrence_data","title":"<code>format_occurrence_data(y)</code>","text":"<p>Reads input y data and formats it to consistent 1d array dtypes.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ArrayLike</code> <p>array-like of shape (n_samples,) or (n_samples, 1)</p> required <p>Returns:</p> Type Description <code>ArrayLike</code> <p>formatted uint8 ndarray of shape (n_samples,)</p> <p>Raises:</p> Type Description <code>AxisError</code> <p>an array with 2 or more columns is passed</p> Source code in <code>elapid/models.py</code> <pre><code>def format_occurrence_data(y: ArrayLike) -&gt; ArrayLike:\n    \"\"\"Reads input y data and formats it to consistent 1d array dtypes.\n\n    Args:\n        y: array-like of shape (n_samples,) or (n_samples, 1)\n\n    Returns:\n        formatted uint8 ndarray of shape (n_samples,)\n\n    Raises:\n        np.AxisError: an array with 2 or more columns is passed\n    \"\"\"\n    if not isinstance(y, np.ndarray):\n        y = np.array(y)\n\n    if y.ndim &gt; 1:\n        if y.shape[1] &gt; 1 or y.ndim &gt; 2:\n            raise np.AxisError(f\"Multi-column y data passed of shape {y.shape}. Must be 1d or 1 column.\")\n        y = y.flatten()\n\n    return y.astype(\"uint8\")\n</code></pre>"},{"location":"module/models/#elapid.models.maxent_alpha","title":"<code>maxent_alpha(raw)</code>","text":"<p>Compute the sum-to-one alpha maxent model parameter.</p> <p>Parameters:</p> Name Type Description Default <code>raw</code> <code>ndarray</code> <p>uncalibrated maxent raw (exponential) model output</p> required <p>Returns:</p> Name Type Description <code>alpha</code> <code>float</code> <p>the output sum-to-one scaling factor</p> Source code in <code>elapid/models.py</code> <pre><code>def maxent_alpha(raw: np.ndarray) -&gt; float:\n    \"\"\"Compute the sum-to-one alpha maxent model parameter.\n\n    Args:\n        raw: uncalibrated maxent raw (exponential) model output\n\n    Returns:\n        alpha: the output sum-to-one scaling factor\n    \"\"\"\n    return -np.log(np.sum(raw))\n</code></pre>"},{"location":"module/models/#elapid.models.maxent_cloglog_transform","title":"<code>maxent_cloglog_transform(engma, entropy)</code>","text":"<p>Compute maxent's cumulative log-log suitability score</p> <p>Parameters:</p> Name Type Description Default <code>engma</code> <code>ndarray</code> <p>calibrated maxent linear model output</p> required <code>entropy</code> <code>float</code> <p>the calibrated model entropy score</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>the cloglog scores for each sample</p> Source code in <code>elapid/models.py</code> <pre><code>def maxent_cloglog_transform(engma: np.ndarray, entropy: float) -&gt; np.ndarray:\n    \"\"\"Compute maxent's cumulative log-log suitability score\n\n    Args:\n        engma: calibrated maxent linear model output\n        entropy: the calibrated model entropy score\n\n    Returns:\n        the cloglog scores for each sample\n    \"\"\"\n    return 1 - np.exp(-np.exp(engma) * np.exp(entropy))\n</code></pre>"},{"location":"module/models/#elapid.models.maxent_entropy","title":"<code>maxent_entropy(raw)</code>","text":"<p>Compute the maxent model entropy score for scaling the logistic output</p> <p>Parameters:</p> Name Type Description Default <code>raw</code> <code>ndarray</code> <p>uncalibrated maxent raw (exponential) model output</p> required <p>Returns:</p> Name Type Description <code>entropy</code> <code>float</code> <p>background distribution entropy score</p> Source code in <code>elapid/models.py</code> <pre><code>def maxent_entropy(raw: np.ndarray) -&gt; float:\n    \"\"\"Compute the maxent model entropy score for scaling the logistic output\n\n    Args:\n        raw: uncalibrated maxent raw (exponential) model output\n\n    Returns:\n        entropy: background distribution entropy score\n    \"\"\"\n    scaled = raw / np.sum(raw)\n    return -np.sum(scaled * np.log(scaled))\n</code></pre>"},{"location":"module/models/#elapid.models.maxent_logistic_transform","title":"<code>maxent_logistic_transform(engma, entropy, tau=MaxentConfig.tau)</code>","text":"<p>Compute maxent's logistic suitability score</p> <p>Parameters:</p> Name Type Description Default <code>engma</code> <code>ndarray</code> <p>calibrated maxent linear model output</p> required <code>entropy</code> <code>float</code> <p>the calibrated model entropy score</p> required <code>tau</code> <code>float</code> <p>the prevalence scaler. lower values indicate rarer species.</p> <code>tau</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>the tau-scaled logistic scores for each sample</p> Source code in <code>elapid/models.py</code> <pre><code>def maxent_logistic_transform(engma: np.ndarray, entropy: float, tau: float = MaxentConfig.tau) -&gt; np.ndarray:\n    \"\"\"Compute maxent's logistic suitability score\n\n    Args:\n        engma: calibrated maxent linear model output\n        entropy: the calibrated model entropy score\n        tau: the prevalence scaler. lower values indicate rarer species.\n\n    Returns:\n        the tau-scaled logistic scores for each sample\n    \"\"\"\n    # maxnet's (tau-free) logistic formulation:\n    # return 1 / (1 + np.exp(-entropy - engma))\n    # use java's formulation instead\n    logratio = np.exp(engma) * np.exp(entropy)\n    return (tau * logratio) / ((1 - tau) + (tau * logratio))\n</code></pre>"},{"location":"module/models/#elapid.models.maxent_raw_transform","title":"<code>maxent_raw_transform(engma)</code>","text":"<p>Compute maxent's raw suitability score</p> <p>Parameters:</p> Name Type Description Default <code>engma</code> <code>ndarray</code> <p>calibrated maxent linear model output</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>the log-linear raw scores for each sample</p> Source code in <code>elapid/models.py</code> <pre><code>def maxent_raw_transform(engma: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute maxent's raw suitability score\n\n    Args:\n        engma: calibrated maxent linear model output\n\n    Returns:\n        the log-linear raw scores for each sample\n    \"\"\"\n    return np.exp(engma)\n</code></pre>"},{"location":"module/stats/","title":"elapid.stats","text":"<p>Utilities for calculating zonal stats and other transformations</p>"},{"location":"module/stats/#elapid.stats.RasterStat","title":"<code>RasterStat</code>","text":"<p>Utility class to iterate over and apply reductions to multiband arrays</p> Source code in <code>elapid/stats.py</code> <pre><code>class RasterStat:\n    \"\"\"Utility class to iterate over and apply reductions to multiband arrays\"\"\"\n\n    def __init__(self, name: str, method: Callable, dtype: str = None, **kwargs):\n        \"\"\"Create a RasterStat object\n\n        Args:\n            name: the label to prepend to the output column\n            method: function to reduce a 2d ndarray to an (nbands,) shape array\n            dtype: the output array data type\n            **kwargs: additional arguments to pass to `method`\n        \"\"\"\n        self.name = name\n        self.method = method\n        self.dtype = dtype\n        self.kwargs = kwargs\n\n    def format(self, x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Format the array data into an array of shape [nbands, n_valid_pixels]\n\n        Args:\n            x: ndarray of shape (nbands, nrows, ncols) or (nbands, n_valid_pixels)\n\n        Returns:\n            2d ndarray\n        \"\"\"\n        if x.ndim == 3:\n            bands, rows, cols = x.shape\n            x = x.reshape((bands, rows * cols))\n        return x\n\n    def reduce(self, x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Reduce an array using the objects `method` function\n\n        Args:\n            x: ndarray of shape (nbands, n_valid_pixels)\n\n        Returns:\n            ndarray of shape (nbands,)\n        \"\"\"\n        return self.method(self.format(x), **self.kwargs)\n</code></pre>"},{"location":"module/stats/#elapid.stats.RasterStat.__init__","title":"<code>__init__(name, method, dtype=None, **kwargs)</code>","text":"<p>Create a RasterStat object</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the label to prepend to the output column</p> required <code>method</code> <code>Callable</code> <p>function to reduce a 2d ndarray to an (nbands,) shape array</p> required <code>dtype</code> <code>str</code> <p>the output array data type</p> <code>None</code> <code>**kwargs</code> <p>additional arguments to pass to <code>method</code></p> <code>{}</code> Source code in <code>elapid/stats.py</code> <pre><code>def __init__(self, name: str, method: Callable, dtype: str = None, **kwargs):\n    \"\"\"Create a RasterStat object\n\n    Args:\n        name: the label to prepend to the output column\n        method: function to reduce a 2d ndarray to an (nbands,) shape array\n        dtype: the output array data type\n        **kwargs: additional arguments to pass to `method`\n    \"\"\"\n    self.name = name\n    self.method = method\n    self.dtype = dtype\n    self.kwargs = kwargs\n</code></pre>"},{"location":"module/stats/#elapid.stats.RasterStat.format","title":"<code>format(x)</code>","text":"<p>Format the array data into an array of shape [nbands, n_valid_pixels]</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>ndarray of shape (nbands, nrows, ncols) or (nbands, n_valid_pixels)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>2d ndarray</p> Source code in <code>elapid/stats.py</code> <pre><code>def format(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Format the array data into an array of shape [nbands, n_valid_pixels]\n\n    Args:\n        x: ndarray of shape (nbands, nrows, ncols) or (nbands, n_valid_pixels)\n\n    Returns:\n        2d ndarray\n    \"\"\"\n    if x.ndim == 3:\n        bands, rows, cols = x.shape\n        x = x.reshape((bands, rows * cols))\n    return x\n</code></pre>"},{"location":"module/stats/#elapid.stats.RasterStat.reduce","title":"<code>reduce(x)</code>","text":"<p>Reduce an array using the objects <code>method</code> function</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>ndarray of shape (nbands, n_valid_pixels)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>ndarray of shape (nbands,)</p> Source code in <code>elapid/stats.py</code> <pre><code>def reduce(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Reduce an array using the objects `method` function\n\n    Args:\n        x: ndarray of shape (nbands, n_valid_pixels)\n\n    Returns:\n        ndarray of shape (nbands,)\n    \"\"\"\n    return self.method(self.format(x), **self.kwargs)\n</code></pre>"},{"location":"module/stats/#elapid.stats.get_raster_stats_methods","title":"<code>get_raster_stats_methods(mean=True, min=False, max=False, count=False, sum=False, stdv=False, skew=False, kurtosis=False, mode=False, percentiles=[], all=False)</code>","text":"<p>Return RasterStat configs for the requested stats calculations</p> Source code in <code>elapid/stats.py</code> <pre><code>def get_raster_stats_methods(\n    mean: bool = True,\n    min: bool = False,\n    max: bool = False,\n    count: bool = False,\n    sum: bool = False,\n    stdv: bool = False,\n    skew: bool = False,\n    kurtosis: bool = False,\n    mode: bool = False,\n    percentiles: list = [],\n    all: bool = False,\n) -&gt; List[RasterStat]:\n    \"\"\"Return RasterStat configs for the requested stats calculations\"\"\"\n    methods = []\n\n    if mean or all:\n        methods.append(RasterStat(name=\"mean\", method=raster_mean, dtype=\"float32\"))\n\n    if min or all:\n        methods.append(RasterStat(name=\"min\", method=raster_min))\n\n    if max or all:\n        methods.append(RasterStat(name=\"max\", method=raster_max))\n\n    if count or all:\n        methods.append(RasterStat(name=\"count\", method=raster_count, dtype=\"int16\"))\n\n    if sum or all:\n        methods.append(RasterStat(name=\"sum\", method=raster_sum))\n\n    if stdv or all:\n        methods.append(RasterStat(name=\"stdv\", method=raster_stdv))\n\n    if skew or all:\n        methods.append(RasterStat(name=\"skew\", method=raster_skew))\n\n    if kurtosis or all:\n        methods.append(RasterStat(name=\"kurt\", method=raster_kurtosis))\n\n    if mode or all:\n        methods.append(RasterStat(name=\"mode\", method=raster_mode))\n\n    if len(percentiles) &gt; 0:\n        for percentile in percentiles:\n            methods.append(RasterStat(name=f\"{percentile}pct\", method=raster_percentile, pctile=percentile))\n\n    return methods\n</code></pre>"},{"location":"module/stats/#elapid.stats.normalize_sample_probabilities","title":"<code>normalize_sample_probabilities(values)</code>","text":"<p>Compute scaled probability scores for an array of samples.</p> <p>These normalized probabilities sum to 1.0 and are designed to be used     as the <code>p</code> parameter for determining sample probabilities in     np.random.choice</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>ArrayLike</code> <p>numeric array to be treated as sample probabilities. probabilities will be scaled linearly based on the input range. setting <code>values = np.array([2, 1, 1])</code> returns (0.5, 0.25, 0.25)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>sample probabiility scores</p> Source code in <code>elapid/stats.py</code> <pre><code>def normalize_sample_probabilities(values: ArrayLike) -&gt; np.ndarray:\n    \"\"\"Compute scaled probability scores for an array of samples.\n\n    These normalized probabilities sum to 1.0 and are designed to be used\n        as the `p` parameter for determining sample probabilities in\n        np.random.choice\n\n    Args:\n        values: numeric array to be treated as sample probabilities.\n            probabilities will be scaled linearly based on the input range.\n            setting `values = np.array([2, 1, 1])` returns (0.5, 0.25, 0.25)\n\n    Returns:\n        sample probabiility scores\n    \"\"\"\n    return values / np.linalg.norm(values, ord=1)\n</code></pre>"},{"location":"module/train_test_split/","title":"elapid.train_test_split","text":"<p>Methods for geographlically splitting data into train/test splits</p>"},{"location":"module/train_test_split/#elapid.train_test_split.BufferedLeaveOneOut","title":"<code>BufferedLeaveOneOut</code>","text":"<p>               Bases: <code>BaseCrossValidator</code></p> <p>Leave-one-out CV that excludes training points within a buffered distance.</p> Source code in <code>elapid/train_test_split.py</code> <pre><code>class BufferedLeaveOneOut(BaseCrossValidator):\n    \"\"\"Leave-one-out CV that excludes training points within a buffered distance.\"\"\"\n\n    def __init__(self, distance: float):\n        \"\"\"Buffered leave-one-out cross-validation strategy.\n\n        Drops points from the training data based on a buffered distance\n            to the left-out test point(s). Implemented from Ploton et al. 2020,\n            https://www.nature.com/articles/s41467-020-18321-y\n\n        Args:\n            distance: drop training data points within this distance of test data.\n        \"\"\"\n        self.distance = distance\n\n    def _group_idxs(\n        self, points: Vector, class_label: str = None, groups: str = None, count: bool = False\n    ) -&gt; List[int]:\n        \"\"\"Get test indices for grouped train/test splits.\"\"\"\n        if class_label is not None:\n            in_class = points[class_label] == 1\n            points = points.iloc[in_class]\n\n        unique = points[groups].unique()\n        if count:\n            return len(unique)\n\n        all_idxs = np.arange(len(points))\n        test_idxs = []\n        for group in unique:\n            in_group = points[groups] == group\n            test_idxs.append(all_idxs[in_group])\n\n        return test_idxs\n\n    def _point_idxs(self, points: Vector, class_label: str = None, count: bool = False) -&gt; List[int]:\n        \"\"\"Get test indices for single point train/test splits.\"\"\"\n        if class_label is None:\n            if count:\n                return len(points)\n            else:\n                return range(len(points))\n\n        else:\n            in_class = points[class_label] == 1\n            if count:\n                return in_class.sum()\n            else:\n                return np.where(in_class)[0]\n\n    def _iter_test_indices(self, points: Vector, class_label: str = None, groups: str = None, y: None = None):\n        \"\"\"Generate indices for test data samples.\"\"\"\n        if groups is None:\n            test_idxs = self._point_idxs(points, class_label)\n\n        else:\n            test_idxs = self._group_idxs(points, class_label, groups)\n\n        for indices in test_idxs:\n            yield indices\n\n    def _iter_test_masks(self, points: Vector, class_label: str = None, groups: str = None):\n        \"\"\"Generates boolean masks corresponding to test sets.\"\"\"\n        for test_index in self._iter_test_indices(points, class_label, groups):\n            test_mask = np.zeros(_num_samples(points), dtype=bool)\n            test_mask[test_index] = True\n            yield test_mask\n\n    def split(self, points: Vector, class_label: str = None, groups: str = None) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Split point data into train/test folds and return their array indices.\n\n        Default behaviour is to perform leave-one-out cross-validation, meaning\n            there will be as many train/test splits as there are samples.\n            To run leave-one-out splits for each y==1 sample, use the\n            `class_label` parameter to define which column includes the class\n            to leave out. To run a grouped leave-one-out, use the `groups`\n            parameter to define which column includes unique IDs to group by.\n\n        Args:\n            points: point-format GeoSeries or GeoDataFrame.\n            class_label: column to specify presence locations (y==1).\n            groups: column to group train/test splits by.\n\n        Yields:\n            (train_idxs, test_idxs) the train/test splits for each fold.\n        \"\"\"\n        n_samples = len(points)\n        indices = np.arange(n_samples)\n        for test_index in self._iter_test_masks(points, class_label, groups):\n            train_idx = indices[np.logical_not(test_index)]\n            test_idx = indices[test_index]\n            train_pts = points.iloc[train_idx]\n            test_pts = points.iloc[test_idx]\n            distances = nearest_point_distance(test_pts, train_pts)\n            in_range = distances &gt; self.distance\n            buffered_train_idx = train_idx[in_range]\n            yield buffered_train_idx, test_idx\n\n    def get_n_splits(self, points: Vector, class_label: str = None, groups: str = None) -&gt; int:\n        \"\"\"Return the number of splitting iterations in the cross-validator.\n\n        Args:\n            points: point-format GeoSeries or GeoDataFrame.\n            class_label: column to specify presence locations (y==1).\n            groups: column to group train/test splits by.\n\n        Returns:\n            Splitting iteration count.\n        \"\"\"\n        if groups is None:\n            return self._point_idxs(points, class_label, count=True)\n        else:\n            return self._group_idxs(points, class_label, groups, count=True)\n</code></pre>"},{"location":"module/train_test_split/#elapid.train_test_split.BufferedLeaveOneOut.__init__","title":"<code>__init__(distance)</code>","text":"<p>Buffered leave-one-out cross-validation strategy.</p> <p>Drops points from the training data based on a buffered distance     to the left-out test point(s). Implemented from Ploton et al. 2020,     www.nature.com/articles/s41467-020-18321-y</p> <p>Parameters:</p> Name Type Description Default <code>distance</code> <code>float</code> <p>drop training data points within this distance of test data.</p> required Source code in <code>elapid/train_test_split.py</code> <pre><code>def __init__(self, distance: float):\n    \"\"\"Buffered leave-one-out cross-validation strategy.\n\n    Drops points from the training data based on a buffered distance\n        to the left-out test point(s). Implemented from Ploton et al. 2020,\n        https://www.nature.com/articles/s41467-020-18321-y\n\n    Args:\n        distance: drop training data points within this distance of test data.\n    \"\"\"\n    self.distance = distance\n</code></pre>"},{"location":"module/train_test_split/#elapid.train_test_split.BufferedLeaveOneOut.get_n_splits","title":"<code>get_n_splits(points, class_label=None, groups=None)</code>","text":"<p>Return the number of splitting iterations in the cross-validator.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Vector</code> <p>point-format GeoSeries or GeoDataFrame.</p> required <code>class_label</code> <code>str</code> <p>column to specify presence locations (y==1).</p> <code>None</code> <code>groups</code> <code>str</code> <p>column to group train/test splits by.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Splitting iteration count.</p> Source code in <code>elapid/train_test_split.py</code> <pre><code>def get_n_splits(self, points: Vector, class_label: str = None, groups: str = None) -&gt; int:\n    \"\"\"Return the number of splitting iterations in the cross-validator.\n\n    Args:\n        points: point-format GeoSeries or GeoDataFrame.\n        class_label: column to specify presence locations (y==1).\n        groups: column to group train/test splits by.\n\n    Returns:\n        Splitting iteration count.\n    \"\"\"\n    if groups is None:\n        return self._point_idxs(points, class_label, count=True)\n    else:\n        return self._group_idxs(points, class_label, groups, count=True)\n</code></pre>"},{"location":"module/train_test_split/#elapid.train_test_split.BufferedLeaveOneOut.split","title":"<code>split(points, class_label=None, groups=None)</code>","text":"<p>Split point data into train/test folds and return their array indices.</p> <p>Default behaviour is to perform leave-one-out cross-validation, meaning     there will be as many train/test splits as there are samples.     To run leave-one-out splits for each y==1 sample, use the     <code>class_label</code> parameter to define which column includes the class     to leave out. To run a grouped leave-one-out, use the <code>groups</code>     parameter to define which column includes unique IDs to group by.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Vector</code> <p>point-format GeoSeries or GeoDataFrame.</p> required <code>class_label</code> <code>str</code> <p>column to specify presence locations (y==1).</p> <code>None</code> <code>groups</code> <code>str</code> <p>column to group train/test splits by.</p> <code>None</code> <p>Yields:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>(train_idxs, test_idxs) the train/test splits for each fold.</p> Source code in <code>elapid/train_test_split.py</code> <pre><code>def split(self, points: Vector, class_label: str = None, groups: str = None) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Split point data into train/test folds and return their array indices.\n\n    Default behaviour is to perform leave-one-out cross-validation, meaning\n        there will be as many train/test splits as there are samples.\n        To run leave-one-out splits for each y==1 sample, use the\n        `class_label` parameter to define which column includes the class\n        to leave out. To run a grouped leave-one-out, use the `groups`\n        parameter to define which column includes unique IDs to group by.\n\n    Args:\n        points: point-format GeoSeries or GeoDataFrame.\n        class_label: column to specify presence locations (y==1).\n        groups: column to group train/test splits by.\n\n    Yields:\n        (train_idxs, test_idxs) the train/test splits for each fold.\n    \"\"\"\n    n_samples = len(points)\n    indices = np.arange(n_samples)\n    for test_index in self._iter_test_masks(points, class_label, groups):\n        train_idx = indices[np.logical_not(test_index)]\n        test_idx = indices[test_index]\n        train_pts = points.iloc[train_idx]\n        test_pts = points.iloc[test_idx]\n        distances = nearest_point_distance(test_pts, train_pts)\n        in_range = distances &gt; self.distance\n        buffered_train_idx = train_idx[in_range]\n        yield buffered_train_idx, test_idx\n</code></pre>"},{"location":"module/train_test_split/#elapid.train_test_split.GeographicKFold","title":"<code>GeographicKFold</code>","text":"<p>               Bases: <code>BaseCrossValidator</code></p> <p>Compute geographically-clustered train/test folds using KMeans clustering</p> Source code in <code>elapid/train_test_split.py</code> <pre><code>class GeographicKFold(BaseCrossValidator):\n    \"\"\"Compute geographically-clustered train/test folds using KMeans clustering\"\"\"\n\n    def __init__(self, n_splits: int = 4, random_state: int = None):\n        \"\"\"Cluster x/y points into separate cross-validation folds.\n\n        Args:\n            n_splits: Number of geographic clusters to split the data into.\n            random_state: Random seed for KMeans clustering.\n        \"\"\"\n        self.n_splits = n_splits\n        self.random_state = random_state\n\n    def _iter_test_indices(self, points: Vector, y: None = None, groups: None = None):\n        \"\"\"Generate indices for test data samples.\"\"\"\n        kmeans = KMeans(n_clusters=self.n_splits, random_state=self.random_state)\n        xy = np.array(list(zip(points.geometry.x, points.geometry.y)))\n        kmeans.fit(xy)\n        clusters = kmeans.predict(xy)\n        indices = np.arange(len(xy))\n        for cluster in range(self.n_splits):\n            test = clusters == cluster\n            yield indices[test]\n\n    def split(self, points: Vector) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Split point data into geographically-clustered train/test folds and\n            return their array indices.\n\n        Args:\n            points: point-format GeoSeries or GeoDataFrame.\n\n        Yields:\n            (train_idxs, test_idxs) the train/test splits for each geo fold.\n        \"\"\"\n        for train, test in super().split(points):\n            yield train, test\n\n    def get_n_splits(self) -&gt; int:\n        \"\"\"Return the number of splitting iterations in the cross-validator.\n\n        Returns:\n            Splitting iteration count.\n        \"\"\"\n        return self.n_splits\n</code></pre>"},{"location":"module/train_test_split/#elapid.train_test_split.GeographicKFold.__init__","title":"<code>__init__(n_splits=4, random_state=None)</code>","text":"<p>Cluster x/y points into separate cross-validation folds.</p> <p>Parameters:</p> Name Type Description Default <code>n_splits</code> <code>int</code> <p>Number of geographic clusters to split the data into.</p> <code>4</code> <code>random_state</code> <code>int</code> <p>Random seed for KMeans clustering.</p> <code>None</code> Source code in <code>elapid/train_test_split.py</code> <pre><code>def __init__(self, n_splits: int = 4, random_state: int = None):\n    \"\"\"Cluster x/y points into separate cross-validation folds.\n\n    Args:\n        n_splits: Number of geographic clusters to split the data into.\n        random_state: Random seed for KMeans clustering.\n    \"\"\"\n    self.n_splits = n_splits\n    self.random_state = random_state\n</code></pre>"},{"location":"module/train_test_split/#elapid.train_test_split.GeographicKFold.get_n_splits","title":"<code>get_n_splits()</code>","text":"<p>Return the number of splitting iterations in the cross-validator.</p> <p>Returns:</p> Type Description <code>int</code> <p>Splitting iteration count.</p> Source code in <code>elapid/train_test_split.py</code> <pre><code>def get_n_splits(self) -&gt; int:\n    \"\"\"Return the number of splitting iterations in the cross-validator.\n\n    Returns:\n        Splitting iteration count.\n    \"\"\"\n    return self.n_splits\n</code></pre>"},{"location":"module/train_test_split/#elapid.train_test_split.GeographicKFold.split","title":"<code>split(points)</code>","text":"<p>Split point data into geographically-clustered train/test folds and     return their array indices.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Vector</code> <p>point-format GeoSeries or GeoDataFrame.</p> required <p>Yields:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>(train_idxs, test_idxs) the train/test splits for each geo fold.</p> Source code in <code>elapid/train_test_split.py</code> <pre><code>def split(self, points: Vector) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Split point data into geographically-clustered train/test folds and\n        return their array indices.\n\n    Args:\n        points: point-format GeoSeries or GeoDataFrame.\n\n    Yields:\n        (train_idxs, test_idxs) the train/test splits for each geo fold.\n    \"\"\"\n    for train, test in super().split(points):\n        yield train, test\n</code></pre>"},{"location":"module/train_test_split/#elapid.train_test_split.checkerboard_split","title":"<code>checkerboard_split(points, grid_size, buffer=0, bounds=None)</code>","text":"<p>Create train/test splits with a spatially-gridded checkerboard.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Vector</code> <p>point-format GeoSeries or GeoDataFrame</p> required <code>grid_size</code> <code>float</code> <p>the height and width of each checkerboard side to split data using. Should match the units of the points CRS (i.e. grid_size=1000 is a 1km grid for UTM data)</p> required <code>buffer</code> <code>float</code> <p>add an x/y buffer around the initial checkerboard bounds</p> <code>0</code> <code>bounds</code> <code>Tuple[float, float, float, float]</code> <p>instead of deriving the checkerboard bounds from <code>points</code>, use this tuple of [xmin, ymin, xmax, ymax] values.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[GeoDataFrame, GeoDataFrame]</code> <p>(train_points, test_points) split using a checkerboard grid.</p> Source code in <code>elapid/train_test_split.py</code> <pre><code>def checkerboard_split(\n    points: Vector, grid_size: float, buffer: float = 0, bounds: Tuple[float, float, float, float] = None\n) -&gt; Tuple[gpd.GeoDataFrame, gpd.GeoDataFrame]:\n    \"\"\"Create train/test splits with a spatially-gridded checkerboard.\n\n    Args:\n        points: point-format GeoSeries or GeoDataFrame\n        grid_size: the height and width of each checkerboard side to split\n            data using. Should match the units of the points CRS\n            (i.e. grid_size=1000 is a 1km grid for UTM data)\n        buffer: add an x/y buffer around the initial checkerboard bounds\n        bounds: instead of deriving the checkerboard bounds from `points`,\n            use this tuple of [xmin, ymin, xmax, ymax] values.\n\n    Returns:\n        (train_points, test_points) split using a checkerboard grid.\n    \"\"\"\n    if isinstance(points, gpd.GeoSeries):\n        points = points.to_frame(\"geometry\")\n\n    bounds = points.total_bounds if bounds is None else bounds\n    xmin, ymin, xmax, ymax = bounds\n\n    x0s = np.arange(xmin - buffer, xmax + buffer + grid_size, grid_size)\n    y0s = np.arange(ymin - buffer, ymax + buffer + grid_size, grid_size)\n\n    train_cells = []\n    test_cells = []\n    for idy, y0 in enumerate(y0s):\n        offset = 0 if idy % 2 == 0 else 1\n        for idx, x0 in enumerate(x0s):\n            cell = box(x0, y0, x0 + grid_size, y0 + grid_size)\n            cell_type = 0 if (idx + offset) % 2 == 0 else 1\n            if cell_type == 0:\n                train_cells.append(cell)\n            else:\n                test_cells.append(cell)\n\n    grid_crs = points.crs\n    train_grid = gpd.GeoDataFrame(geometry=train_cells, crs=grid_crs)\n    test_grid = gpd.GeoDataFrame(geometry=test_cells, crs=grid_crs)\n    train_points = (\n        gpd.sjoin(points, train_grid, how=\"left\", predicate=\"within\")\n        .dropna()\n        .drop(columns=\"index_right\")\n        .reset_index(drop=True)\n    )\n    test_points = (\n        gpd.sjoin(points, test_grid, how=\"left\", predicate=\"within\")\n        .dropna()\n        .drop(columns=\"index_right\")\n        .reset_index(drop=True)\n    )\n\n    return train_points, test_points\n</code></pre>"},{"location":"module/types/","title":"elapid.types","text":"<p>Custom maxent and typing data types.</p>"},{"location":"module/types/#elapid.types.to_iterable","title":"<code>to_iterable(var)</code>","text":"<p>Checks and converts variables to an iterable type.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>Any</code> <p>the input variable to check and convert.</p> required <p>Returns:</p> Type Description <code>list</code> <p><code>var</code> wrapped in a list.</p> Source code in <code>elapid/types.py</code> <pre><code>def to_iterable(var: Any) -&gt; list:\n    \"\"\"Checks and converts variables to an iterable type.\n\n    Args:\n        var: the input variable to check and convert.\n\n    Returns:\n        `var` wrapped in a list.\n    \"\"\"\n    if not hasattr(var, \"__iter__\"):\n        return [var]\n    elif isinstance(var, (str)):\n        return [var]\n    else:\n        return var\n</code></pre>"},{"location":"module/types/#elapid.types.validate_boolean","title":"<code>validate_boolean(var)</code>","text":"<p>Evaluates whether an argument is boolean True/False</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>Any</code> <p>the input argument to validate</p> required <p>Returns:</p> Name Type Description <code>var</code> <code>bool</code> <p>the value if it passes validation</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p><code>var</code> was not boolean</p> Source code in <code>elapid/types.py</code> <pre><code>def validate_boolean(var: Any) -&gt; bool:\n    \"\"\"Evaluates whether an argument is boolean True/False\n\n    Args:\n        var: the input argument to validate\n\n    Returns:\n        var: the value if it passes validation\n\n    Raises:\n        AssertionError: `var` was not boolean\n    \"\"\"\n    assert isinstance(var, bool), \"Argument must be True/False\"\n    return var\n</code></pre>"},{"location":"module/types/#elapid.types.validate_feature_types","title":"<code>validate_feature_types(features)</code>","text":"<p>Ensures the feature classes passed are maxent-legible</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Union[str, list]</code> <p>List or string that must be in [\"linear\", \"quadratic\", \"product\", \"hinge\", \"threshold\", \"auto\"] or string \"lqphta\"</p> required <p>Returns:</p> Name Type Description <code>valid_features</code> <code>list</code> <p>List of formatted valid feature values</p> Source code in <code>elapid/types.py</code> <pre><code>def validate_feature_types(features: Union[str, list]) -&gt; list:\n    \"\"\"Ensures the feature classes passed are maxent-legible\n\n    Args:\n        features: List or string that must be in [\"linear\", \"quadratic\", \"product\",\n            \"hinge\", \"threshold\", \"auto\"] or string \"lqphta\"\n\n    Returns:\n        valid_features: List of formatted valid feature values\n    \"\"\"\n    valid_list = get_feature_types(return_string=False)\n    valid_string = get_feature_types(return_string=True)\n    valid_features = list()\n\n    # ensure the string features are valid, and translate to a standard feature list\n    if type(features) is str:\n        for feature in features:\n            if feature == \"a\":\n                return valid_list\n            assert feature in valid_string, \"Invalid feature passed: {}\".format(feature)\n            if feature == \"l\":\n                valid_features.append(\"linear\")\n            elif feature == \"q\":\n                valid_features.append(\"quadratic\")\n            elif feature == \"p\":\n                valid_features.append(\"product\")\n            elif feature == \"h\":\n                valid_features.append(\"hinge\")\n            elif feature == \"t\":\n                valid_features.append(\"threshold\")\n\n    # or ensure the list features are valid\n    elif type(features) is list:\n        for feature in features:\n            if feature == \"auto\":\n                return valid_list\n            assert feature in valid_list, \"Invalid feature passed: {}\".format(feature)\n            valid_features.append(feature)\n\n    return valid_features\n</code></pre>"},{"location":"module/types/#elapid.types.validate_numeric_scalar","title":"<code>validate_numeric_scalar(var)</code>","text":"<p>Evaluates whether an argument is a single numeric value.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>Any</code> <p>the input argument to validate</p> required <p>Returns:</p> Name Type Description <code>var</code> <code>bool</code> <p>the value if it passes validation</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p><code>var</code> was not numeric.</p> Source code in <code>elapid/types.py</code> <pre><code>def validate_numeric_scalar(var: Any) -&gt; bool:\n    \"\"\"Evaluates whether an argument is a single numeric value.\n\n    Args:\n        var: the input argument to validate\n\n    Returns:\n        var: the value if it passes validation\n\n    Raises:\n        AssertionError: `var` was not numeric.\n    \"\"\"\n    assert isinstance(var, (int, float)), \"Argument must be single numeric value\"\n    return var\n</code></pre>"},{"location":"module/utils/","title":"elapid.utils","text":"<p>Backend helper and convenience functions.</p>"},{"location":"module/utils/#elapid.utils.check_raster_alignment","title":"<code>check_raster_alignment(raster_paths)</code>","text":"<p>Checks whether the extent, resolution and projection of multiple rasters match exactly.</p> <p>Parameters:</p> Name Type Description Default <code>raster_paths</code> <code>list</code> <p>a list of raster covariate paths</p> required <p>Returns:</p> Type Description <code>bool</code> <p>whether all rasters align</p> Source code in <code>elapid/utils.py</code> <pre><code>def check_raster_alignment(raster_paths: list) -&gt; bool:\n    \"\"\"Checks whether the extent, resolution and projection of multiple rasters match exactly.\n\n    Args:\n        raster_paths: a list of raster covariate paths\n\n    Returns:\n        whether all rasters align\n    \"\"\"\n    first = raster_paths[0]\n    rest = raster_paths[1:]\n\n    with rio.open(first) as src:\n        res = src.res\n        bounds = src.bounds\n        transform = src.transform\n\n    for path in rest:\n        with rio.open(path) as src:\n            if src.res != res or src.bounds != bounds or src.transform != transform:\n                return False\n\n    return True\n</code></pre>"},{"location":"module/utils/#elapid.utils.count_raster_bands","title":"<code>count_raster_bands(raster_paths)</code>","text":"<p>Returns the total number of bands from a list of rasters.</p> <p>Parameters:</p> Name Type Description Default <code>raster_paths</code> <code>list</code> <p>List of raster data file paths.</p> required <p>Returns:</p> Name Type Description <code>n_bands</code> <code>int</code> <p>total band count.</p> Source code in <code>elapid/utils.py</code> <pre><code>def count_raster_bands(raster_paths: list) -&gt; int:\n    \"\"\"Returns the total number of bands from a list of rasters.\n\n    Args:\n        raster_paths: List of raster data file paths.\n\n    Returns:\n        n_bands: total band count.\n    \"\"\"\n    n_bands = 0\n    for path in raster_paths:\n        with rio.open(path) as src:\n            n_bands += src.count\n\n    return n_bands\n</code></pre>"},{"location":"module/utils/#elapid.utils.create_output_raster_profile","title":"<code>create_output_raster_profile(raster_paths, template_idx=0, windowed=True, nodata=None, count=1, compress=None, driver='GTiff', bigtiff=True, dtype='float32')</code>","text":"<p>Gets parameters for windowed reading/writing to output rasters.</p> <p>Parameters:</p> Name Type Description Default <code>raster_paths</code> <code>list</code> <p>raster paths of covariates to apply the model to</p> required <code>template_idx</code> <code>int</code> <p>index of the raster file to use as a template. template_idx=0 sets the first raster as template</p> <code>0</code> <code>windowed</code> <code>bool</code> <p>perform a block-by-block data read. slower, but reduces memory use</p> <code>True</code> <code>nodata</code> <code>Number</code> <p>output nodata value</p> <code>None</code> <code>count</code> <code>int</code> <p>number of bands in the prediction output</p> <code>1</code> <code>driver</code> <code>str</code> <p>output raster file format (from rasterio.drivers.raster_driver_extensions())</p> <code>'GTiff'</code> <code>compress</code> <code>str</code> <p>compression type to apply to the output file</p> <code>None</code> <code>bigtiff</code> <code>bool</code> <p>specify the output file as a bigtiff (for rasters &gt; 2GB)</p> <code>True</code> <code>dtype</code> <code>str</code> <p>rasterio data type string</p> <code>'float32'</code> <p>Returns:</p> Type Description <code>(windows, profile)</code> <p>an iterable and a dictionary for the window reads and the raster profile</p> Source code in <code>elapid/utils.py</code> <pre><code>def create_output_raster_profile(\n    raster_paths: list,\n    template_idx: int = 0,\n    windowed: bool = True,\n    nodata: Number = None,\n    count: int = 1,\n    compress: str = None,\n    driver: str = \"GTiff\",\n    bigtiff: bool = True,\n    dtype: str = \"float32\",\n) -&gt; Tuple[Iterable, Dict]:\n    \"\"\"Gets parameters for windowed reading/writing to output rasters.\n\n    Args:\n        raster_paths: raster paths of covariates to apply the model to\n        template_idx: index of the raster file to use as a template. template_idx=0 sets the first raster as template\n        windowed: perform a block-by-block data read. slower, but reduces memory use\n        nodata: output nodata value\n        count: number of bands in the prediction output\n        driver: output raster file format (from rasterio.drivers.raster_driver_extensions())\n        compress: compression type to apply to the output file\n        bigtiff: specify the output file as a bigtiff (for rasters &gt; 2GB)\n        dtype: rasterio data type string\n\n    Returns:\n        (windows, profile): an iterable and a dictionary for the window reads and the raster profile\n    \"\"\"\n    with rio.open(raster_paths[template_idx]) as src:\n        if windowed:\n            windows = [window for _, window in src.block_windows()]\n        else:\n            windows = [rio.windows.Window(0, 0, src.width, src.height)]\n\n        dst_profile = src.profile.copy()\n        dst_profile.update(\n            count=count,\n            dtype=dtype,\n            nodata=nodata,\n            compress=compress,\n            driver=driver,\n        )\n        if bigtiff and driver == \"GTiff\":\n            dst_profile.update(BIGTIFF=\"YES\")\n\n    return windows, dst_profile\n</code></pre>"},{"location":"module/utils/#elapid.utils.download_sample_data","title":"<code>download_sample_data(dir, name='ariolimax', quiet=False)</code>","text":"<p>Downloads sample raster and vector files from a web server.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>the directory to download the data to</p> required <code>name</code> <code>str</code> <p>the sample dataset to download. options include: \"ariolimax\" button's banana slug dataset</p> <code>'ariolimax'</code> <code>quiet</code> <code>bool</code> <p>disable the progress bar</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Downloads files to <code>dir</code></p> Source code in <code>elapid/utils.py</code> <pre><code>def download_sample_data(dir: str, name: str = \"ariolimax\", quiet: bool = False) -&gt; None:\n    \"\"\"Downloads sample raster and vector files from a web server.\n\n    Args:\n        dir: the directory to download the data to\n        name: the sample dataset to download. options include:\n            \"ariolimax\" button's banana slug dataset\n        quiet: disable the progress bar\n\n    Returns:\n        None. Downloads files to `dir`\n    \"\"\"\n    name = str.lower(name)\n    https = \"https://earth-chris.github.io/images/research\"\n\n    if name == \"ariolimax\":\n        fnames = [\n            \"ariolimax-ca.gpkg\",\n            \"ca-cloudcover-mean.tif\",\n            \"ca-cloudcover-stdv.tif\",\n            \"ca-leafareaindex-mean.tif\",\n            \"ca-leafareaindex-stdv.tif\",\n            \"ca-surfacetemp-mean.tif\",\n            \"ca-surfacetemp-stdv.tif\",\n        ]\n\n    try:\n        os.mkdir(dir)\n    except FileExistsError:\n        pass\n\n    tqdm = get_tqdm()\n    for fname in tqdm(fnames, disable=quiet, **tqdm_opts):\n        request.urlretrieve(f\"{https}/{fname}\", os.path.join(dir, fname))\n</code></pre>"},{"location":"module/utils/#elapid.utils.format_band_labels","title":"<code>format_band_labels(raster_paths, labels=None)</code>","text":"<p>Verify the number of labels matches the band count, create labels if none passed.</p> <p>Parameters:</p> Name Type Description Default <code>raster_paths</code> <code>list</code> <p>count the total number of bands in these rasters.</p> required <code>labels</code> <code>List[str]</code> <p>a list of band labels.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>labels</code> <p>creates default band labels if none are passed.</p> Source code in <code>elapid/utils.py</code> <pre><code>def format_band_labels(raster_paths: list, labels: List[str] = None):\n    \"\"\"Verify the number of labels matches the band count, create labels if none passed.\n\n    Args:\n        raster_paths: count the total number of bands in these rasters.\n        labels: a list of band labels.\n\n    Returns:\n        labels: creates default band labels if none are passed.\n    \"\"\"\n    n_bands = count_raster_bands(raster_paths)\n\n    if labels is None:\n        labels = make_band_labels(n_bands)\n\n    n_labels = len(labels)\n    assert n_labels == n_bands, f\"number of band labels ({n_labels}) != n_bands ({n_bands})\"\n\n    return labels.copy()\n</code></pre>"},{"location":"module/utils/#elapid.utils.get_raster_band_indexes","title":"<code>get_raster_band_indexes(raster_paths)</code>","text":"<p>Counts the number raster bands to index multi-source, multi-band covariates.</p> <p>Parameters:</p> Name Type Description Default <code>raster_paths</code> <code>list</code> <p>a list of raster paths</p> required <p>Returns:</p> Type Description <code>(nbands, band_idx)</code> <p>int and list of the total number of bands and the 0-based start/stop band index for each path</p> Source code in <code>elapid/utils.py</code> <pre><code>def get_raster_band_indexes(raster_paths: list) -&gt; Tuple[int, list]:\n    \"\"\"Counts the number raster bands to index multi-source, multi-band covariates.\n\n    Args:\n        raster_paths: a list of raster paths\n\n    Returns:\n        (nbands, band_idx): int and list of the total number of bands and the 0-based start/stop\n            band index for each path\n    \"\"\"\n    nbands = 0\n    band_idx = [0]\n    for i, raster_path in enumerate(raster_paths):\n        with rio.open(raster_path) as src:\n            nbands += src.count\n            band_idx.append(band_idx[i] + src.count)\n\n    return nbands, band_idx\n</code></pre>"},{"location":"module/utils/#elapid.utils.get_tqdm","title":"<code>get_tqdm()</code>","text":"<p>Returns a context-appropriate tqdm progress tracking function.</p> <p>Determines the appropriate tqdm based on the user context, as     behavior changes inside/outside of jupyter notebooks.</p> <p>Returns:</p> Name Type Description <code>tqdm</code> <code>Callable</code> <p>the context-specific tqdm module</p> Source code in <code>elapid/utils.py</code> <pre><code>def get_tqdm() -&gt; Callable:\n    \"\"\"Returns a context-appropriate tqdm progress tracking function.\n\n    Determines the appropriate tqdm based on the user context, as\n        behavior changes inside/outside of jupyter notebooks.\n\n    Returns:\n        tqdm: the context-specific tqdm module\n    \"\"\"\n    if in_notebook():\n        from tqdm.notebook import tqdm\n    else:\n        from tqdm import tqdm\n\n    return tqdm\n</code></pre>"},{"location":"module/utils/#elapid.utils.in_notebook","title":"<code>in_notebook()</code>","text":"<p>Evaluate whether the module is currently running in a jupyter notebook.</p> Source code in <code>elapid/utils.py</code> <pre><code>def in_notebook() -&gt; bool:\n    \"\"\"Evaluate whether the module is currently running in a jupyter notebook.\"\"\"\n    return \"ipykernel\" in sys.modules\n</code></pre>"},{"location":"module/utils/#elapid.utils.load_object","title":"<code>load_object(path, compressed=True)</code>","text":"<p>Reads a python object into memory that's been saved to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>the file path of the object to load</p> required <code>compressed</code> <code>bool</code> <p>flag to specify whether the file was compressed prior to saving</p> <code>True</code> <p>Returns:</p> Name Type Description <code>obj</code> <code>Any</code> <p>the python object that has been saved (e.g., a MaxentModel() instance)</p> Source code in <code>elapid/utils.py</code> <pre><code>def load_object(path: str, compressed: bool = True) -&gt; Any:\n    \"\"\"Reads a python object into memory that's been saved to disk.\n\n    Args:\n        path: the file path of the object to load\n        compressed: flag to specify whether the file was compressed prior to saving\n\n    Returns:\n        obj: the python object that has been saved (e.g., a MaxentModel() instance)\n    \"\"\"\n    with open(path, \"rb\") as f:\n        obj = f.read()\n\n    if compressed:\n        obj = gzip.decompress(obj)\n\n    return pickle.loads(obj)\n</code></pre>"},{"location":"module/utils/#elapid.utils.load_sample_data","title":"<code>load_sample_data(name='ariolimax', drop_geometry=False)</code>","text":"<p>Loads example species presence/background and covariate data.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the sample dataset to load. options include: \"ariolimax\" button's banana slug dataset \"bradypus\" from the R 'maxnet' package</p> <code>'ariolimax'</code> <p>Returns:</p> Type Description <code>(x, y)</code> <p>a tuple of dataframes containing covariate and response data, respectively</p> Source code in <code>elapid/utils.py</code> <pre><code>def load_sample_data(name: str = \"ariolimax\", drop_geometry: bool = False) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Loads example species presence/background and covariate data.\n\n    Args:\n        name: the sample dataset to load. options include:\n            \"ariolimax\" button's banana slug dataset\n            \"bradypus\" from the R 'maxnet' package\n\n    Returns:\n        (x, y): a tuple of dataframes containing covariate and response data, respectively\n    \"\"\"\n    name = str.lower(name)\n    assert name in [\"bradypus\", \"ariolimax\"], \"Invalid sample data requested\"\n\n    package_path = os.path.realpath(__file__)\n    package_dir = os.path.dirname(package_path)\n\n    if name == \"bradypus\":\n        file_path = os.path.join(package_dir, \"data\", \"bradypus.csv.gz\")\n        assert os.path.exists(file_path), \"sample data missing from install path.\"\n        df = pd.read_csv(file_path, compression=\"gzip\").astype(\"int64\")\n        y = df[\"presence\"].astype(\"int8\")\n        x = df.drop(columns=[\"presence\"]).astype({\"ecoreg\": \"category\"})\n        return x, y\n\n    if name == \"ariolimax\":\n        file_path = os.path.join(package_dir, \"data\", \"ariolimax.gpkg\")\n        assert os.path.exists(file_path), \"sample data missing from install path.\"\n        df = gpd.read_file(file_path)\n        columns_to_drop = [\"presence\"]\n        if drop_geometry:\n            columns_to_drop.append(\"geometry\")\n        x = df.drop(columns=columns_to_drop)\n        y = df[\"presence\"].astype(\"int8\")\n        return x, y\n</code></pre>"},{"location":"module/utils/#elapid.utils.make_band_labels","title":"<code>make_band_labels(n_bands)</code>","text":"<p>Creates a list of band names to assign as dataframe columns.</p> <p>Parameters:</p> Name Type Description Default <code>n_bands</code> <code>int</code> <p>total number of raster bands to create labels for.</p> required <p>Returns:</p> Name Type Description <code>labels</code> <code>list</code> <p>list of column names.</p> Source code in <code>elapid/utils.py</code> <pre><code>def make_band_labels(n_bands: int) -&gt; list:\n    \"\"\"Creates a list of band names to assign as dataframe columns.\n\n    Args:\n        n_bands: total number of raster bands to create labels for.\n\n    Returns:\n        labels: list of column names.\n    \"\"\"\n    n_zeros = n_digits(n_bands)\n    labels = [\"b{band_number:0{n_zeros}d}\".format(band_number=i + 1, n_zeros=n_zeros) for i in range(n_bands)]\n\n    return labels\n</code></pre>"},{"location":"module/utils/#elapid.utils.n_digits","title":"<code>n_digits(number)</code>","text":"<p>Counts the number of significant integer digits of a number.</p> <p>Parameters:</p> Name Type Description Default <code>number</code> <code>Number</code> <p>the number to evaluate.</p> required <p>Returns:</p> Name Type Description <code>order</code> <code>int</code> <p>number of digits required to represent a number</p> Source code in <code>elapid/utils.py</code> <pre><code>def n_digits(number: Number) -&gt; int:\n    \"\"\"Counts the number of significant integer digits of a number.\n\n    Args:\n        number: the number to evaluate.\n\n    Returns:\n        order: number of digits required to represent a number\n    \"\"\"\n    if number == 0:\n        order = 1\n    else:\n        order = np.floor(np.log10(number)).astype(int) + 1\n\n    return order\n</code></pre>"},{"location":"module/utils/#elapid.utils.repeat_array","title":"<code>repeat_array(x, length=1, axis=0)</code>","text":"<p>Repeats a 1D numpy array along an axis to an arbitrary length</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>the n-dimensional array to repeat</p> required <code>length</code> <code>int</code> <p>the number of times to repeat the array</p> <code>1</code> <code>axis</code> <code>int</code> <p>the axis along which to repeat the array (valid values include 0 to n+1)</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>An n+1 dimensional numpy array</p> Source code in <code>elapid/utils.py</code> <pre><code>def repeat_array(x: np.array, length: int = 1, axis: int = 0) -&gt; np.ndarray:\n    \"\"\"Repeats a 1D numpy array along an axis to an arbitrary length\n\n    Args:\n        x: the n-dimensional array to repeat\n        length: the number of times to repeat the array\n        axis: the axis along which to repeat the array (valid values include 0 to n+1)\n\n    Returns:\n        An n+1 dimensional numpy array\n    \"\"\"\n    return np.expand_dims(x, axis=axis).repeat(length, axis=axis)\n</code></pre>"},{"location":"module/utils/#elapid.utils.save_object","title":"<code>save_object(obj, path, compress=True)</code>","text":"<p>Writes a python object to disk for later access.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>a python object or variable to be saved (e.g., a MaxentModel() instance)</p> required <code>path</code> <code>str</code> <p>the output file path</p> required Source code in <code>elapid/utils.py</code> <pre><code>def save_object(obj: object, path: str, compress: bool = True) -&gt; None:\n    \"\"\"Writes a python object to disk for later access.\n\n    Args:\n        obj: a python object or variable to be saved (e.g., a MaxentModel() instance)\n        path: the output file path\n    \"\"\"\n    obj = pickle.dumps(obj)\n\n    if compress:\n        obj = gzip.compress(obj)\n\n    with open(path, \"wb\") as f:\n        f.write(obj)\n</code></pre>"},{"location":"module/utils/#elapid.utils.square_factor","title":"<code>square_factor(n)</code>","text":"<p>Compute a square form-factor to fit <code>n</code> items.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>the number of items to fit into a square.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(x, y) tuple of the square dimensions.</p> Source code in <code>elapid/utils.py</code> <pre><code>def square_factor(n: int) -&gt; tuple:\n    \"\"\"Compute a square form-factor to fit `n` items.\n\n    Args:\n        n: the number of items to fit into a square.\n\n    Returns:\n        (x, y) tuple of the square dimensions.\n    \"\"\"\n    val = np.ceil(np.sqrt(n))\n    val2 = int(n / val)\n    while val2 * val != float(n):\n        val -= 1\n        val2 = int(n / val)\n    return int(val), int(val2)\n</code></pre>"},{"location":"paper/paper/","title":"Summary","text":"<p>Species distribution modeling (SDM) is based on the Grinellean niche concept: the environmental conditions that allow individuals of a species to survive and reproduce will constrain the distributions of those species over space and time [@Grinnell:1917; @Wiens:2009]. The inputs to these models are typically spatially-explicit species occurrence records and a series of environmental covariates, which might include information on climate, topography, land cover or hydrology [@Booth:2014]. While many modeling methods have been developed to quantify and map these species-environment interactions, few software systems include both a) the appropriate statistical modeling routines and b) support for handling the full suite of geospatial analysis required to prepare data to fit, apply, and summarize these models.</p> <p><code>elapid</code> is both a geospatial analysis and a species distribution modeling package. It provides an interface between vector and raster data for selecting random point samples, annotating point locations with coincident raster data, and summarizing raster values inside a polygon with zonal statistics. It provides a series of covariate transformation routines for increasing feature dimensionality, quantifying interaction terms and normalizing unit scales. It provides a Python implementation of the popular Maxent SDM [@Phillips:2017] using infinitely weighted logistic regression [@Fithian:2013]. It also includes a standard Niche Envelope Model [@Nix:1986], both of which were written to match the software design patterns of modern machine learning packages like <code>sklearn</code> [@sklearn]. It also allows users to add spatial context to any model by providing methods for spatially splitting train/test data and computing geographically-explicit sample weights. <code>elapid</code> was designed as a contemporary SDM package, built on best practices from the past and aspiring to support the next generation of biodiversity modeling workflows.</p>","tags":["biogeography","species distribution modeling","gesopatial analysis","machine learning","Python"]},{"location":"paper/paper/#statement-of-need","title":"Statement of need","text":"<p>Species occurrence data\u2014georeferenced point locations where a species has been observed and identified\u2014are an important resource for understanding the environmental conditions that predict habitat suitability for that species. These data are now abundant thanks to the proliferation of institutional open data policies, large-scale collaborations among research groups, and advances in the quality and popularity of citizen science applications [@gbif; @inaturalist]. Tools for working with these data haven't necessarily kept pace, however, especially ones that support modern geospatial data formats and machine learning workflows.</p> <p><code>elapid</code> builds on a suite of well-known statistical modeling tools commonly used by biogeographers, extending them to add novel features, to work with cloud-hosted data, and to save and share models. It provides methods for managing the full lifecyle of modeling data: generating background point data, extracting raster values for each point (i.e. point annotation), splitting train/test data, fitting models, and applying predictions to rasters. It provides a very high degree of control for model design, which is important for several reasons.</p> <p>First is to provide simple and flexible methods for working with spatial data. Point data are managed as <code>GeoSeries</code> and <code>GeoDataFrame</code> objects [@geopandas], which can be easily merged and split using traditional indexing method as well as with geographic methods. They can also be reprojected on-the-fly. <code>elapid</code> reads and writes raster data with <code>rasterio</code>, which provides a similarly convenient set of methods for indexing and reading point locations from rasters [@rasterio]. These features are wrapped to handle many of the routine tasks and gotchas of working with geospatial data. It doesn't require data to be rigorously pre-processed so that all rasters are perfectly aligned, nor does it require that all datasets are in matching projections. <code>elapid</code> can extract pixel-level raster data from datasets at different resolutions, from multi-band files, and harmonize projections on-the-fly, for both model fitting and for inference.</p> <p>Another advantage of <code>elapid</code>'s flexible design is that it can be used to extend traditional species distribution models in ways that are difficult to implement in other software systems. For example, working with multi-temporal data\u2014fitting SDMs to occurrence records and environmental data from multiple time periods\u2014is supported. Each time period's occurrence data can be annotated using the coincident environmental data. Random background samples can be generated for each time period, ensuring the background represents a broad distribution of conditions across the full temporal extent. These presence and background samples are then concatenated into a single <code>GeoDataFrame</code> for model fitting. Fitted models can be applied to multi-temporal environmental data to map changes in habitat suitability over time, and can also be saved and restored later for future inference.</p> <p><code>elapid</code> is one among several open source species distribution modeling packages. The R package <code>ENMeval</code> is a good direct comparison [@Kass:2021]. <code>ENMeval</code> provides a series of tools for model fitting, model selection and cross-validation, making calls under the hood to <code>maxnet</code> and <code>dismo</code> [@Phillips:2017]. <code>elapid</code> implements similar methods for spatial cross-validation, builds on the rich feature transformation tools implemented in <code>maxnet</code>, and employs similar model fitting techniques. <code>elapid</code> provides additional tools to simplify working with geospatial datasets, and provides additional novel cross-validation methods like geographic k-fold and buffered leave-one-out strategies [@Ploton:2020]. It is also one of the first open source species distribution modeling packages in Python, and it does not include any proprietary software dependencies [@Brown:2014].</p>","tags":["biogeography","species distribution modeling","gesopatial analysis","machine learning","Python"]},{"location":"paper/paper/#why-maxent-still-matters","title":"Why Maxent still matters","text":"<p>The main scientific contribution of <code>elapid</code> is extending and modifying the Maxent SDM, a model and software system as popular as it is maligned [@Phillips:2008; @Fourcade:2018]. First published in 2006, Maxent remains relevant because it's a presence-only model designed to work with the kinds of species occurrence data data that have proliferated lately.</p> <p>Presence-only models formulate binary classification models as presence/background (1/0) instead of presence/absence, which changes how models are fit and interpreted [@Fithian:2013; @Merow:2013]. Background points are a spatially-random sample of the landscapes where a species might be found, which should be sampled with the same level of effort and bias as the species occurrence data. Presence/background models posit the null expectation that a species is equally likely to be found anywhere within it's range. Differences in environmental conditions between where a species occurs and across the full landscape should indicate niche preferences. Relative habitat suitability is then determined based on differences in the relative frequency distributions of conditions in these regions. Presence-only models reduce the burden of finding absence data, which are problematic to begin with, but they increase the burden of precisely selecting background points. These define what relative habitat suitability is relative to [@Elith:2011; @Barbet:2012].</p> <p><code>elapid</code> includes several methods for sampling the background. Points can be sampled uniformly within a polygon, like a range map or an ecoregion extent. Sampling points from rasters can be done uniformly across the full extent or only from pixels with valid, unmasked data. Working with bias rasters is also supported. Any raster with monotonically increasing values can be used as a sample probability map, increasing the probability that a sample is drawn in locations with higher pixel values. One important role for the niche envelope model is to create bias maps to ensure background points are only sampled within the broad climatic envelope where a species occurs. The target-group bias sampling method has also been shown to effectively correct for sample bias [@Barber:2022].</p> <p>A common criticism of Maxent is that, though it depends on spatially-explicit data, it's not really a spatial model. Covariate data are indexed and extracted spatially, but there are no model terms based on location, distance, or point density, and all samples are treated as independent measurements. While some argue that many of the ails of spatial autocorrelation are typically overstated [@Hawkins:2012], spatial data have unique and very interesting properties that should be handled carefully. Non-independence is inherent to spatial data, driven both by underlying ecological patterns and processes (e.g. dispersal, species interactions, climatic covariance) as well as by data collection biases (e.g. occurrence records are common near roads or trails despite many species typically preferring less fragmented habitats).</p> <p>Spatial models should include methods for handling spatially-specific modeling paradigms, particularly the lack of independence of nearby samples or spatial biases in sample density. Quantifying and understanding model skill requires accounting for these spatial autocorrelations, and <code>elapid</code> includes several methods for doing so. Checkerboard cross-validation can mitigate bias introduced by spatially clustered points. Creating spatially-explicit \\(k\\)-fold splits\u2014independent clusters based on x/y locations\u2014can quantify how well model predictions generalize to new areas. And tuning sample weights based on the density of nearby points decreases the risk of overfitting to autocorellated environmental features from areas with high sample density. This is particularly important for mitigating the effects of density-dependent non-independence.</p> <p>These methods are not solely restricted to the SDMs implemented in <code>elapid</code>. They can add spatial context to other machine learning models, too. Geographic sample weights can be used to fit random forests, boosted regression trees, generalized linear models, and other approaches commonly used to predict spatial distributions. <code>elapid</code> also includes a series of feature transformers, including the transformations used in Maxent, which can extend covariate feature space to improve model skill.</p> <p><code>elapid</code> was designed to provide a series of modern tools for quantifying biodiversity change. The target audience for the package includes ecologists, biodiversity scientists, spatial analysts and machine learning scientists. Working with software to understand the rapid changes reshaping our biosphere should be easy and enjoyable. Because thinking about the ongoing annihilation of nature that's driving our current extinction crisis is decidedly less so.</p>","tags":["biogeography","species distribution modeling","gesopatial analysis","machine learning","Python"]},{"location":"paper/paper/#acknowledgments","title":"Acknowledgments","text":"<p>Many thanks to Jeffrey R. Smith for many long and thought-provoking discussions on species distribution modeling. Thanks also to David C. Marvin for helping me think creatively about novel applications for Maxent. And many thanks to Gretchen C. Daily for promoting and supporting access to open source software for biodiversity and ecosystem services modeling.</p>","tags":["biogeography","species distribution modeling","gesopatial analysis","machine learning","Python"]},{"location":"paper/paper/#references","title":"References","text":"","tags":["biogeography","species distribution modeling","gesopatial analysis","machine learning","Python"]},{"location":"sdm/maxent/","title":"Maxent SDM","text":""},{"location":"sdm/maxent/#background","title":"Background","text":"<p>Maxent is a species distribution modeling (SDM) system that uses species observations and environmental data to predict where a species might be found under past, present or future environmental conditions.</p> <p>It's a presence/background model. Maxent uses data on where a species is present and, instead of data on where it is absent, it uses a random sample of the landscape where you might expect to find that species. This is convenient because it reduces data collection requirements, but it also introduces some unique challenges for fitting and interpreting models.</p> <p>Formally, Maxent estimates habitat suitability (i.e. the fundamental niche) using:</p> <ol> <li>species occurrence records (<code>y = 1</code>),</li> <li>randomly sampled \"background\" location records (<code>y = 0</code>), and</li> <li>environmental covariate data (<code>x</code>) for each location of <code>y</code>.</li> </ol> <p>Maxent doesn't directly estimate relationships between presence/background data and environmental covariates (so, not just <code>y ~ x</code>). Instead, it fits a series of feature transformatons to the covariate data (<code>z = f(x)</code>), like computing quadratic transformations or computing the pairwise products of each covariate. Maxent then estimates the conditional probability of finding a species given a set of environmental conditions as:</p> \\[ Pr(y = 1 | f(z)) = \\frac{f_1(z) \\cdot Pr(y = 1)}{f(z)} \\] <p>This can be interpreted as: \"the relative likelihood of observing a species at any point on the landscape is determined by differences in the distributions of environmental conditions where a species is found relative to the distributions at a random sample across the landscape.\"</p> <p>A null model would posit that a species is likely to be distributed uniformly across a landscape; you're equally likely to find it wherever you go. Maxent uses this null model\u2014the \"background\" conditions\u2014as a prior probability distribution to estimate relative suitability based on the conditions where species have been observed.</p> <p>Because of this formulation, the definition of both the landscape and the background are directly related to the definition of how habitat suitability is estimated. These should be defined with care.</p> <p><code>elapid</code> provides python tools for fitting Maxent models, computing features, and applying models to raster data. Below are some instructions for the first two.</p>"},{"location":"sdm/maxent/#using-maxent-in-elapid","title":"Using Maxent in <code>elapid</code>","text":""},{"location":"sdm/maxent/#training-models","title":"Training models","text":"<p>There are two primary Maxent functions: fitting features and fitting models. You can do it all at once with:</p> <pre><code>import elapid\n\nx, y = elapid.load_sample_data()\nmodel = elapid.MaxentModel()\nmodel.fit(x, y)\n</code></pre> <p>Where:</p> <ul> <li><code>x</code> is an ndarray or dataframe of environmental covariates of shape (<code>n_samples</code>, <code>n_covariates</code>)</li> <li><code>y</code> is an ndarray or series of species presence/background labels (rows labeled <code>1</code> or <code>0</code>)</li> </ul> <p>The <code>elapid.MaxentModel()</code> object takes these data, fits features from covariate data, computes sample weights and feature regularization, fits a series of models, and returns an estimator that can be used for applying predictions to new data.</p> <p><code>MaxentModel()</code> behaves like an sklearn <code>estimator</code> class. Use <code>model.fit(x, y)</code> to train a model, and <code>model.predict(x)</code> to generate model predictions.</p>"},{"location":"sdm/maxent/#feature-transformations","title":"Feature transformations","text":"<p>You can also generate and evaluate features before passing them to the model. Just set the <code>feature_types</code> to linear to ensure no additional features are fit.</p> <pre><code>model = ela.MaxentModel(feature_types=[\"linear\"])\nfeatures = elapid.MaxentFeatureTransformer()\nz = features.fit_transform(x)\nmodel.fit(z, y)\n</code></pre> <p><code>MaxentFeatureTransformer()</code> behaves like an sklearn <code>preprocessing</code> class. Use <code>features.fit(x_train)</code> to fit features, <code>features.transform(x_test)</code> to apply to new covariate data, or <code>features.fit_transform(x)</code> to fit features and return the transformed data.</p>"},{"location":"sdm/maxent/#configuration","title":"Configuration","text":"<p>The base Maxent classes can be modified with parameters that are available in other Maxent implementations:</p> <pre><code>model = elapid.MaxentModel(\n    feature_types = ['linear', 'hinge', 'product'], # the feature transformations\n    tau = 0.5, # prevalence scaler\n    clamp = True, # set covariate min/max based on range of training data\n    scorer = 'roc_auc', # metric to optimize (from sklearn.metrics.SCORERS)\n    beta_multiplier = 1.5, # regularization scaler (high values drop more features)\n    beta_lqp = 1.0, # linear, quadratic, product regularization scaler\n    beta_hinge = 1.0, # hinge regularization scaler\n    beta_threshold = 1.0, # threshold regularization scaler\n    beta_categorical = 1.0, # categorical regularization scaler\n    n_hinge_features = 10, # number of hinge features to compute\n    n_threshold_features = 10, # number of threshold features to compute\n    convergence_tolerance = 1e-07, # model fit convergence threshold\n    use_lambdas = 'best', # set to 'best' (least overfit), 'last' (highest score)\n    n_cpus = 4, # number of cpu cores to use\n)\n</code></pre> <p>You can find the default configuration parameters in elapid/config.</p>"},{"location":"sdm/maxent/#differences-between-elapid-and-maxnet","title":"Differences between <code>elapid</code> and <code>maxnet</code>","text":"<p><code>elapid</code> was written to match a series of routines defined in maxnet, the R version of Maxent. The goal was to provide a near-equivalent Python implementation.</p> <p>It has been tested to verify numerical equivalence when computing the custom parameters used to fit penalized logistic regression models with <code>glmnet</code>. This includes sample weights, lambda scores, regularization scores, and feature transformations.</p> <p>Yet there are some differences between models fit using <code>elapid</code> and <code>maxnet</code>. These differences are determined by opinionated changes to default model parameters. Before we enumerate these differences, let's quickly review an important topic: feature regularization.</p> <p>Regularization parameters\u2014or beta values\u2014are estimated on a feature-by-feature basis, and determine the likelihood that a feature is identified by the model as meaningful. Regularization is often used to reduce overfitting by dropping meaningless or noisy features. Increasing regularization values typically increases feature penalties and increases model generality. Several <code>elapid</code> defaults and feature transformation routines modify how regularization is handled compared to <code>maxnet</code>.</p>"},{"location":"sdm/maxent/#opinionated-default-model-parameters","title":"Opinionated default model parameters","text":"<p><code>beta_multiplier: 1.5</code> - users have control over how regularization is applied to specific feature types (e.g. <code>beta_hinge</code>, <code>beta_categorical</code>) and across all feature types (via the <code>beta_multiplier</code>).</p> <ul> <li><code>elapid</code> increases the default <code>beta_multipier</code> to 1.5, compared to 1.0 in <code>maxnet</code>, in order to increase regularization penalties.</li> <li>This was selected after some quantitative analyses\u2014finding a value that minimized cross-validation error in multiple SDM contexts\u2014and after inspecting partial dependence plots for ecological coherence.</li> <li>Increasing regularization across the board almost always leads to smoother model predictions and lower out-of-bag error rates, indicating highly penalized models generalize well. This seemed like a sensible, conservative value.</li> </ul> <p><code>use_lambdas: 'best'</code> - the underlying regularized logistic regression model fits a series of models using an array of lambda values, which modify the scale of regularization applied (the number of fits is determined by the <code>n_lambdas</code> parameter).</p> <ul> <li><code>maxnet</code> uses the 'last' lambda value, which typically applies the lowest regularization penalties.</li> <li><code>elapid</code> uses the 'best' lambda value, which is the lambda value that minimizes the cross-validation error estimated during model fitting.</li> </ul> <p><code>feature_types: [\"linear\", \"hinge\", \"product\"]</code> - <code>elapid</code> only estimates these three feature types by default to reduce complexity and to prioritize ecologically realistic models.</p> <ul> <li>linear features capture the direct effects of each environmental covariate on species occurrence patterns.</li> <li>hinge features provide context for each covariate's local value relative to the global range of that covariate's values.</li> <li>product features allow fitting interaction terms between covariates.</li> <li>quadratic features rarely provide any additional information beyond linear features, and are dropped by default.</li> <li>threshold features\u2014which quantify stepwise functions\u2014are uncommon in biogeography, and are regularly cited as a source of overfitting. These features tend to be redundant with hinge features, which are like smoothed threshold features across the range of each covariate, making threshold features redundant.</li> </ul> <p><code>n_hinge_features: 10</code> - <code>elapid</code> computes fewer hinge features by default than <code>maxnet</code> (50), which seems sufficiently complex.</p> <ul> <li>The number of hinge features effectively determines the number of bins that each covariate gets expanded into, computing hinges or \"knots\" at each bin.</li> <li>Increasing <code>n_hinge_features</code> will partition up the data into finer and finer bins, but I've found that 10-15 features captures enough detail.</li> </ul>"},{"location":"sdm/maxent/#feature-regularization-vs-normalization","title":"Feature regularization vs. normalization","text":"<p>One of the primary differences between <code>elapid</code> and <code>maxnet</code> is how feature normalization is handled, which has downstream effects on estimated regularization values.</p> <p>Normalization is the process of rescaling covariate values from their native range to a standardized range (e.g. converting mean annual temperatures and precipitation values from degrees C and mm/year to scaled 0.0-1.0 values). Normalization reduces the likelihood that model fits are overly sensitive to the range of observed covariates.</p> <p>By default, <code>elapid</code> normalizes all features to a 0-1 range prior to model fitting.</p> <p><code>maxnet</code> does not. Instead, it scales regularization penalties based on the range and variance of each covariate, increasing regularization scores for covariates with high numerical ranges (like population density, which could scale from 0-6,000 people per km2 in Tokyo) and decreasing regularization scores for covariates with low numerical ranges (like humidity, which ranges from 0.002 to 0.007 kg/kg in California).</p> <p>I don't know that there's a \"right\" way to handle this.</p> <p>By applying feature normalization, regularization values are consistent across each feature type in <code>elapid</code>. That is, all linear features receive uniform regularization penalties, and all hinge features receive uniform regularization penalties, though there are differences between linear and hinge regularization values (hinge features are more penalized).</p> <p><code>maxnet</code>, by contrast, uses regularization parameters in order to impose normalization constraints. But, due to the internal mechanics by which this is implemented, I've found that there can be some nonlinear effects, and that this approach is less consistent\u2014and the regularization values less interpretable\u2014than when using a uniform normalization and regularization strategy.</p> <p>There's still a high level of agreement between methods: comparing <code>elapid</code> and <code>maxnet</code> predictions to the same sample dataset computed an r2 score of 0.91 and a mean absolute error score of 0.047 (using the 0-1 scaled cloglog output). But the differences in model predictions arise in large part due to differences in how feature regularization is calculated.</p> <p><code>elapid</code> is opinionated by design, uniformly handling normalization and regularization to make it easier to understand model performance and to minimize any effects introduced by differences in covariate ranges.</p> <p>Ecological and environmental data span many scales, species niche preferences are complex, and small perturbations in environmental conditions can have large effects on estimated habitat suitability. Statistically-driven SDMs should be less sensitive to patterns like the absolute range of conditions and more sensitive to relative niche space that species occupy within that range, which we don't know a priori.</p>"},{"location":"sdm/maxent/#additional-reading","title":"Additional reading","text":"<p>Below are links to some useful journal articles describing how Maxent works.</p> <p>A practical guide to MaxEnt for modeling species\u2019 distributions: what it does, and why inputs and settings matter, Merow et al. 2013 [pdf]</p> <p>A statistical explanation of MaxEnt for ecologists, Elith et al. 2011 [pdf]</p> <p>Opening the black box: an open-source release of Maxent, Phillips et al. 2017 [pdf]</p> <p>Modeling of species distributions with Maxent: new extensions and a comprehensive evaluation, Philliips et al. 2008 [pdf]</p>"},{"location":"sdm/nicheenvelope/","title":"Niche Envelope Models","text":"<p>Niche envelope models use the range of environmental conditions where a species occurs to constrain predictions of where they might occur.</p> <p><code>elapid</code> implements a simple but straightforward approach to estimating niche envelopes.</p> <p>It uses the distribution of covariate values where a species is observed to assign binary (0/1) suitability scores on a covariate-by-covariate basis if the conditions at any location are within the range of observed covariate values.</p> <pre><code>import elapid as ela\n\nx, y = ela.load_sample_data()\nenvelope = ela.NicheEnvelopeModel(percentile_range=[2.5, 97.5])\nenvelope.fit(x, y)\n</code></pre> <p>The <code>percentile_range</code> parameter controls how inclusive the envelope is. Setting it to [0, 100] includes all observed covariate values within the range of suitability. Modifying this range may be useful to clip spurious or extreme values. The default is to use the 95<sup>th</sup> percentile range.</p> <p>Binary suitability is assessed on a per-covariate basis, so the envelope has as many dimensions as it does covariates. <code>elapid</code> includes three methods for estimating the envelope of suitability across these dimensions.</p> <pre><code># overlay options include ['average', 'intersection', 'union']\nenvelope = ela.NicheEnvelopeModel(overlay='intersection')\n</code></pre> <p><code>average</code> computes the pixel-wise mean of the the binary suitability scores across each covariate to produce a 0.0-1.0 score. <code>intersection</code> only includes areas with suitable conditions for all covariates within the envelope. Conversely, <code>union</code> includes an area within the envelop if any covariate is within the suitable range.</p>"},{"location":"sdm/nicheenvelope/#chaining-with-another-sdm","title":"Chaining with another SDM","text":"<p>Niche envelope models are also a useful way to select background points for other species distribution models.</p> <p>Since many methods expect that background points represent the landscape of potentially suitable locations for a species, a NicheEnvelopeModel can filter out points where a species is never expected to be found.</p> <pre><code># estimate the envelope of suitability\nx, y = ela.load_sample_data()\nenvelope = ela.NicheEnvelopeModel(overlay=\"average\")\nenvelope.fit(x, y)\n\n# only select points with &gt;50% envelope suitability\nin_range = envelope.predict(x) &gt; 0.5\nxsub, ysub = x[in_range], y[in_range]\n\n# fit a maxent model\nmaxent = ela.MaxentModel()\nmaxent.fit(xsub, ysub)\n</code></pre> <p>This approach reduces overfitting by excluding point locations that don't represent the range of conditions across the landscape where a species won't occur.</p>"}]}