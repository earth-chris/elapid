{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a08885",
   "metadata": {},
   "source": [
    "# Working With Geospatial Data\n",
    "\n",
    "This notebook reviews common patterns for working with vector and raster data using `elapid`.\n",
    "\n",
    "We'll review the core tools used for extracting raster data from points and polygons, assigning sample weights based on sample density, and creating spatially-explicit train/test data, while also showing general patterns for working with the `GeoDataFrame` and `GeoSeries` classes from `geopandas`, which are frequently used.\n",
    "\n",
    "The sample data we'll use includes species occurrence records from the genus `Ariolimax`, a group of North American terrestrial slugs, as well as some climate and vegetation data. Fiat slug!\n",
    "\n",
    "Several datasets used here are hosted on a web server. These files can be accessed locally with `ela.download_sample_data({output_directory})`, but we'll work with them directly on the web to demonstrate that reading remote data is nearly identical to reading local data thanks to the amazing libraries `elapid` is built on top of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae64985c",
   "metadata": {},
   "source": [
    "## Packages, paths & preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import elapid as ela\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from rasterio import plot as rioplot\n",
    "from sklearn import metrics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "https = \"https://earth-chris.github.io/images/research\"\n",
    "csv = f\"{https}/ariolimax-ca.csv\"\n",
    "raster_names = [\n",
    "    \"ca-cloudcover-mean.tif\",\n",
    "    \"ca-cloudcover-stdv.tif\",\n",
    "    \"ca-leafareaindex-mean.tif\",\n",
    "    \"ca-leafareaindex-stdv.tif\",\n",
    "    \"ca-surfacetemp-mean.tif\",\n",
    "    \"ca-surfacetemp-stdv.tif\",\n",
    "]\n",
    "rasters = [f\"{https}/{raster}\" for raster in raster_names]\n",
    "labels = [\"CloudCoverMean\", \"CloudCoverStdv\", \"LAIMean\", \"LAIStdv\", \"SurfaceTempMean\", \"SurfaceTempStdv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a727303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preferences\n",
    "mpl.style.use('ggplot')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pair_colors = ['#FFCC02', '#00458C']\n",
    "\n",
    "print(f\"Notebook last run with elapid version {ela.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747dcdf0",
   "metadata": {},
   "source": [
    "## Working with Point-format data\n",
    "\n",
    "Species distribution modeling is the science and the art of working with point-format species occurrence data. These are locations on the landscape where a species has been observed and identified and recorded as present in an x/y location. We then model the relative likelihood of observing that species in other locations based on the environmental conditions where that species is observed.\n",
    "\n",
    "Occurrence data are typically recorded with latitude/longitude coordinates for a single location, also known as \"point-format vector data.\" Environmental data are typically provided as image data with coordinate reference information, or \"raster data.\" `elapid` has a series of tools for working with, and as an interface between, these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858c0e40",
   "metadata": {},
   "source": [
    "### Reading x/y data\n",
    "\n",
    "SDM data often come as CSV files with a column for latitude and longitude. Let's convert that information into a `GeoDataFrame` so we can natively perform geospatial operations.\n",
    "\n",
    "The example CSV is a subset of this [GBIF occurrence dataset](https://www.gbif.org/occurrence/download/0272645-220831081235567), which contains data from four species of Banana Slug across California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49df2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and preview the data\n",
    "df = pd.read_csv(csv)\n",
    "\n",
    "# we convert the lat/lon columns into a GeoSeries dataset, which is spatially aware\n",
    "geometry = ela.xy_to_geoseries(\n",
    "    x = df['decimalLongitude'],\n",
    "    y = df['decimalLatitude']\n",
    ")\n",
    "\n",
    "# then merge the two together into a GeoDataFrame\n",
    "ariolimax = gpd.GeoDataFrame(df[[\"species\", \"year\"]], geometry=geometry)\n",
    "\n",
    "# and show the evolution\n",
    "print(\"DataFrame\")\n",
    "print(df.head())\n",
    "print(\"\\nGeoSeries\")\n",
    "print(geometry.head())\n",
    "print(\"\\nGeoDataFrame\")\n",
    "print(ariolimax.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb6185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now plot these points geographically\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "sp = ariolimax.plot(\n",
    "    column='species',\n",
    "    ax=ax,\n",
    "    legend=True,\n",
    "    legend_kwds={\"bbox_to_anchor\": (1, 0.5)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685ce960",
   "metadata": {},
   "source": [
    "### Converting from arrays to coordinates\n",
    "\n",
    "The `xy_to_geoseries` works with pretty much any iterable type, including arrays or lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a18b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a couple of dummy locations\n",
    "lons = [-122.49, 151.0]\n",
    "lats = [37.79, -33.87]\n",
    "\n",
    "# it's smart to explicity set the CRS/projection to ensure the points are put in the right place\n",
    "# the CRS below assigns the points the lat/lon crs type\n",
    "crs = \"EPSG:4326\"\n",
    "random_locations = ela.xy_to_geoseries(lons, lats, crs=crs)\n",
    "\n",
    "# these points now have lots of neat geographic methods and properties\n",
    "xmin, ymin, xmax, ymax = random_locations.total_bounds\n",
    "print(f\"Bounding box: [{xmin}, {ymin}, {xmax}, {ymax}]\")\n",
    "\n",
    "# including getting a description of the coordinate reference system (crs)\n",
    "print(\"\")\n",
    "pprint(random_locations.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d449f79e",
   "metadata": {},
   "source": [
    "## Working with raster data\n",
    "\n",
    "This demo includes 6 raster files defined at the start of the notebook, corresponding to the average and standard deviation in annual cloud cover, temperature, and vegetation growth across California.\n",
    "\n",
    "Following a brief overview of the rasters, the sections below will review how to extract data from them using point and polygon vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we'll just read and display the surface temperature data\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "surfacetemp_raster = rasters[4]\n",
    "with rio.open(surfacetemp_raster, 'r') as src:\n",
    "    profile = src.profile.copy()\n",
    "    \n",
    "    # this plots the mean annual temperature data\n",
    "    rioplot.show(src, ax=ax, cmap=\"Spectral_r\")\n",
    "    \n",
    "    # overlay the ariolimax data\n",
    "    ariolimax.to_crs(src.crs).plot(column='species', ax=ax, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"profile metadata\")\n",
    "pprint(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43866c66",
   "metadata": {},
   "source": [
    "## Drawing random pseudoabsence locations\n",
    "\n",
    "In addition to species occurrence records (where `y = 1`), species distributions models often require a set of random pseudo-absence/background points (`y = 0`). These are a random geographic sampling of where you might expect to find a species across the target landscape.\n",
    "\n",
    "The background point distribution is extremely important for presence-only models, and should be selected with care. `elapid` provides a range of methods for sample selection.\n",
    "\n",
    "For the unintiated, this paper by [Morgane Barbet-Massin et al.](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00172.x) is an excellent resource to consult when it comes to thinking about how background point samples should be selected. Likewise, [Phillips et al.](https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1890/07-2153.1) is another classic worth perusal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c190f2",
   "metadata": {},
   "source": [
    "### From a raster's extent\n",
    "\n",
    "You can use `elapid` to create a uniform random geographic sample from unmasked locations within a raster's extent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d240ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "count = 10_000\n",
    "pseudoabsence = ela.sample_raster(surfacetemp_raster, count)\n",
    "\n",
    "# plotting\n",
    "pseudoabsence.plot(markersize=0.5)\n",
    "pseudoabsence.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed56e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some pixel centroids were sampled multiple times\n",
    "# you can drop them if you'd like.\n",
    "pseudoabsence.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d1daf3",
   "metadata": {},
   "source": [
    "If you have a large raster that doesn't fit in memory, you can also set `ignore_mask=True` to use the rectangular bounds of the raster to draw samples. Many of the sample locations will be in `nodata` locations but we can address that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "pseudoabsence_uniform = ela.sample_raster(\n",
    "    surfacetemp_raster,\n",
    "    count,\n",
    "    ignore_mask=True,\n",
    ")\n",
    "\n",
    "# plotting\n",
    "pseudoabsence_uniform.plot(markersize=0.5)\n",
    "pseudoabsence_uniform.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b1cc7",
   "metadata": {},
   "source": [
    "### From a bias raster\n",
    "\n",
    "You could also pass a raster \"bias file\", where the raster grid cells contain information on the probability of sampling an area. Bias adjustments are often required because species occurrence records are often biased towards certain locations (near roads, parks, or trails, for example).\n",
    "\n",
    "The grid cells can be an arbitrary range of values. What's important is that the values encode a linear range of numbers that are higher where you're more likely to draw a sample. The probability of drawing a sample is dependent on two factors: the range of values provided and the frequency of values across the dataset.\n",
    "\n",
    "Because they're detrivores, we'll guess that people only go looking for banana slugs in places with lots of vegetation growth. We'll use the leaf area index (LAI) covariate to select more background samples from highly vegetated areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fded39e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lai_raster = rasters[2]\n",
    "pseudoabsence_bias = ela.sample_bias_file(lai_raster, count)\n",
    "pseudoabsence_bias.plot(markersize=0.5)\n",
    "pseudoabsence_bias.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a125a216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, duplicates can be dropped\n",
    "# (this can take some time to run)\n",
    "pseudoabsence_bias.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a3b7c",
   "metadata": {},
   "source": [
    "We can see that we are selecting a disproportionately high number of points along California's coast - home to dense redwood forests where slugs thrive - and disproportionately few samples in the dry deserts in the southeast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fe13b",
   "metadata": {},
   "source": [
    "### From a vector polygon\n",
    "\n",
    "Another way to address bias would be to select points only within an area where that species is known to occur. Within a certain ecoregion or biome, for example.\n",
    "\n",
    "The `Ariolimax` dataset contains occurrence records for four species within the genus. If we're interested in understanding the distributions of one of them, we could construct a rudimentary \"range map\" for this genus by computing the bounding geometry of all points then sampling within those bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dee637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a convex hull of all points by merging them into a single multipoint feature\n",
    "ariolimax_hull = gpd.GeoSeries(ariolimax.to_crs(pseudoabsence.crs).unary_union.convex_hull).set_crs(pseudoabsence.crs)\n",
    "pseudoabsence_range = ela.sample_geoseries(ariolimax_hull, count // 2)\n",
    "\n",
    "# plot 'em together\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "pseudoabsence.plot(markersize=0.25, ax=ax)\n",
    "ariolimax_hull.plot(color='yellow', alpha=0.7, ax=ax)\n",
    "pseudoabsence_range.plot(color='blue', ax=ax, markersize=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eb772e",
   "metadata": {},
   "source": [
    "Where the yellow polygon above shows the simple \"range map,\" the blue points are point samples selected from within that range, and the red points are the uniform sample, plotted for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd3f0f",
   "metadata": {},
   "source": [
    "## Point annotation\n",
    "\n",
    "Annotation refers to reading and storing raster values at the locations of a series of point occurrence records in a single GeoDataFrame table. Once you have your presence and pseudo-absence records, you can annotate these records with the covariate data from each location.\n",
    "\n",
    "This function, since it's geographically indexed, doesn't require the point data and the raster data to be in the same projection. elapid handles reprojection and resampling on the fly.\n",
    "\n",
    "It also allows you to pass multiple raster files, which can be in different projections, extents, or grid sizes. This means you don't have to explicitly re-sample your raster data prior to analysis, which is always a chore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c728a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll work with just the Ariolimax buttoni records moving forward\n",
    "buttoni = ariolimax[ariolimax['species'] == 'Ariolimax buttoni']\n",
    "presence = ela.annotate(\n",
    "    buttoni.geometry,\n",
    "    rasters,\n",
    "    labels=labels,\n",
    "    drop_na=True,\n",
    ")\n",
    "\n",
    "# we'll use the biased sample locations\n",
    "background = ela.annotate(\n",
    "    pseudoabsence_bias,\n",
    "    rasters,\n",
    "    labels=labels,\n",
    "    drop_na=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e78076",
   "metadata": {},
   "source": [
    "It took about 20 seconds to label nearly 12,000 points across six raster datasets hosted on the web. Neat.\n",
    "\n",
    "If you don't specify the labels, elapid will assign `['b1', 'b2', ..., 'bn']` names to each column.\n",
    "\n",
    "Setting `drop_na=True` requires that the raster datasets have nodata values assigned in their metadata. These point locations will be dropped from the output dataframe, which will have fewer rows than the input points.\n",
    "\n",
    "Some raster datasets contain `NaN` values. Let's drop those before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "presence.dropna(inplace=True)\n",
    "background.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e44f21",
   "metadata": {},
   "source": [
    "Let's plot some histograms to understand the similarities and differences between the conditions at presence locations and at background locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(6,9))\n",
    "\n",
    "for label, ax in zip(labels, axs.ravel()):\n",
    "    pvar = presence[label]\n",
    "    bvar = background[label]\n",
    "    ax.hist(\n",
    "        [pvar, bvar],\n",
    "        density=True,\n",
    "        alpha=0.7,\n",
    "        label=['presence', 'background'],\n",
    "        color=pair_colors,\n",
    "    )\n",
    "    ax.set_title(label)\n",
    "    \n",
    "handles, lbls = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, lbls, loc=(0.6, 0.9))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b755f76b",
   "metadata": {},
   "source": [
    "This checks out. Slugs are commonly found in areas with higher annual cloud cover, consistently cool temperatures, and high vegetation growth (even after adjusting for bias and oversampling areas with high LAI). Neat!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713a578a",
   "metadata": {},
   "source": [
    "## Zonal statistics\n",
    "\n",
    "In addition to the tools for working with Point data, `elapid` contains a routine for calculating zonal statistics from Polygon or MutliPolygon geometry types.\n",
    "\n",
    "This routine reads an array of raster data covering the extent of a polygon, masks the areas outside the polygon, and computes summary statistics such as the mean, standard deviation, and mode of the array.\n",
    "\n",
    "The stats reported are managed by a set of keywords (`count=True`, `sum=True`, `skew=True`). The `all=True` keyword is a shortcut to compute all of the stats. You'll still have to explicitly pass a list of percentiles, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70852689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute zonal stats within the convex hull\n",
    "zs = ela.zonal_stats(\n",
    "    ariolimax_hull,\n",
    "    rasters,\n",
    "    labels = labels,\n",
    "    mean = True,\n",
    "    stdv = True,\n",
    "    percentiles = [25, 75],\n",
    ")\n",
    "\n",
    "zs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bdae6e",
   "metadata": {},
   "source": [
    "What sets the `elapid` zonal stats method apart from other zonal stats packages is it:\n",
    "\n",
    "- handles reprojection on the fly, meaning the input vector/raster data don't need to be reprojected a priori\n",
    "- handles mutli-band input, computing summary stats on all bands (instead of having to specify which band)\n",
    "- handles multi-raster input, reading inputs in serial but creating a single output GeoDataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a2e1cf",
   "metadata": {},
   "source": [
    "## Geographic sample weights\n",
    "\n",
    "Despite ingesting spatial data, many statistically-driven SDMs are not spatial models in a traditional sense: they don't handle geographic information during model selection or in model scoring.\n",
    "\n",
    "One way to add spatial information to a model is to compute geographically-explicit sample weights. `elapid` does this by calculating weights based on the distance to the nearest neighbor. Points nearby other points receive lower weight scores; far-away points receive higher weight scores.\n",
    "\n",
    "The default is to compute weights based on the distance to the nearest point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb39f73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "presence['SampleWeight'] = ela.distance_weights(presence)\n",
    "presence.plot(column='SampleWeight', legend=True, cmap='YlOrBr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e92c75b",
   "metadata": {},
   "source": [
    "There is one lone point in the northwest corner that is assigned a very high weight because it is so isolated.\n",
    "\n",
    "Alternatively, you could compute the average distance to `n` nearest points instead to compute sample weights using point densities instead of the distance to the single nearest point. This may be useful if you have small clusters of a few points far away from large, densely populated regions, like the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a0c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 100\n",
    "background['SampleWeight'] = ela.distance_weights(background, n_neighbors=n_neighbors)\n",
    "presence['SampleWeight'] = ela.distance_weights(presence, n_neighbors=n_neighbors)\n",
    "presence.plot(column='SampleWeight', legend=True, cmap='YlOrBr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48108861",
   "metadata": {},
   "source": [
    "As you can see, there are tradeoffs for finding the best sample weights depending on the spatial distributions of each occurrence dataset. We'll use a 100 neighbor weight in this case to upweight the samples in the sierras (the northeastern cluster of points).\n",
    "\n",
    "This function uses `ela.nearest_point_distance()`, a handy function for computing the distance between each point and it's nearest neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46942f11",
   "metadata": {},
   "source": [
    "## Stacking dataframes\n",
    "\n",
    "Now that we have our presence points and our pseudoabsence points, we can merge them together into a single dataset for analysis.\n",
    "\n",
    "`elapid` provides a convenience function for merging GeoDataFrames, designed for stacking presence/background data. It will reproject geometries on the fly if needed, and you can optionally add a 'class' column to the output GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf5f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = ela.stack_geodataframes(presence, background, add_class_label=True)\n",
    "merged.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc19a5",
   "metadata": {},
   "source": [
    "## Model setup\n",
    "\n",
    "We won't get into much detail on model parameters, leaving that for another in-depth notebook. This section will focus on fitting models using default settings, then evaluate the effects of different forms of train/test splitting strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3fe783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and print the model defaults\n",
    "maxent = ela.MaxentModel()\n",
    "print(maxent.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340aa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the x/y data for model fitting\n",
    "x = merged.drop(columns=['class', 'SampleWeight'])\n",
    "y = merged['class']\n",
    "sample_weight = merged['SampleWeight']\n",
    "\n",
    "# fit and evaluate the model under naive conditions\n",
    "maxent.fit(x, y, sample_weight=sample_weight)\n",
    "ypred = maxent.predict(x)\n",
    "\n",
    "print(f\"Unweighted naive AUC score: {metrics.roc_auc_score(y, ypred):0.3f}\")\n",
    "print(f\"Weighted naive AUC score  : {metrics.roc_auc_score(y, ypred, sample_weight=sample_weight):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e92dbf",
   "metadata": {},
   "source": [
    "## Train / test splits\n",
    "\n",
    "Uniformly random train/test splits are generally discouraged in spatial modeling because of the strong spatial structure inherent in many datasets. The non-independence of these data is referred to as spatial autocorrelation. Using distance- or density-based sample weights is one way to mitigate these effects. Another is to split the data into geographically distinct train/test regions to try and prioritize model generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541d967",
   "metadata": {},
   "source": [
    "### Checkerboard splits\n",
    "\n",
    "With a \"checkerbox\" train/test split, points are intersected along a regular grid, and every other grid is used to split the data into train/test sets.\n",
    "\n",
    "The height and width of the grid used to split the data is controlled by the `grid_size` parameter. This should specify distance in the units of the point data's CRS. In our example, the units are in *meters*.\n",
    "\n",
    "The black and white structure of the checkerboard means this method can only generate one train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c121c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use a 50km grid\n",
    "grid_size = 50_000\n",
    "train, test = ela.checkerboard_split(merged, grid_size=grid_size)\n",
    "\n",
    "# we'll re-merge them for plotting purposes\n",
    "train['split'] = 'train'\n",
    "test['split'] = 'test'\n",
    "checker = ela.stack_geodataframes(train, test)\n",
    "checker.plot(column='split', markersize=0.75, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d19a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model fitting\n",
    "xtrain = train.drop(columns=['class', 'SampleWeight', 'split'])\n",
    "ytrain = train['class']\n",
    "sample_weight_train = train['SampleWeight']\n",
    "\n",
    "xtest = test.drop(columns=['class', 'SampleWeight', 'split'])\n",
    "ytest = test['class']\n",
    "sample_weight_test = test['SampleWeight']\n",
    "\n",
    "maxent.fit(xtrain, ytrain, sample_weight=sample_weight_train)\n",
    "ypred = maxent.predict(xtest)\n",
    "\n",
    "print(f\"Unweighted checkerboard AUC score: {metrics.roc_auc_score(ytest, ypred):0.3f}\")\n",
    "print(f\"Weighted checkerboard AUC score: {metrics.roc_auc_score(ytest, ypred, sample_weight=sample_weight_test):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c964b0",
   "metadata": {},
   "source": [
    "This lower AUC score is to be expected and should be interpreted as a good thing. It indicates that the naive model was overfitting to nearby points, and the inflated scores were likely the result of spatial autocorrelation.\n",
    "\n",
    "Future modelers might consider computing sample weights after splitting the train/test data to increase weights at checkerboard edges, which might improve model generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814a30d4",
   "metadata": {},
   "source": [
    "### Geographic *k*-fold splits\n",
    "\n",
    "Alternatively, you can create `k` geographically-clustered folds using the `GeographicKFold` cross validation strategy. This method is effective for understanding how well models fit in one region will generalize to areas outside the training domain.\n",
    "\n",
    "This method uses KMeans clusters fit with the x/y locations of the point data, grouping points into spatially distinct groups. This cross-validation strategy is a good way to test how well models generalize outside of their training extents into novel geographic regions.\n",
    "\n",
    "What makes this tricky is the clustering should only be applied to the presence data. So we'll geographically split the presence data and use a simple background train/test split for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e45f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists for plotting, tracking performance\n",
    "test_dfs = []\n",
    "auc_scores = []\n",
    "\n",
    "# split the train / test data\n",
    "fold = 0\n",
    "n_splits = 3\n",
    "gfolds = ela.GeographicKFold(n_splits=n_splits)\n",
    "\n",
    "for train_idx, test_idx in gfolds.split(presence):\n",
    "    # this returns arrays for indexing the original dataframe\n",
    "    # which requires using the pandas .iloc interface\n",
    "    p_train = presence.iloc[train_idx]\n",
    "    p_test = presence.iloc[test_idx]\n",
    "    \n",
    "    # simple background split\n",
    "    n_bg_train = len(background) // 2\n",
    "    b_train = background.iloc[:n_bg_train]\n",
    "    b_test = background.iloc[n_bg_train:]\n",
    "    \n",
    "    # merge 'em\n",
    "    train = ela.stack_geodataframes(p_train, b_train)\n",
    "    test = ela.stack_geodataframes(p_test, b_test)\n",
    "    \n",
    "    # set up model fitting\n",
    "    xtrain = train.drop(columns=['class', 'SampleWeight'])\n",
    "    ytrain = train['class']\n",
    "    sample_weight_train = train['SampleWeight']\n",
    "    xtest = test.drop(columns=['class', 'SampleWeight'])\n",
    "    ytest = test['class']\n",
    "    sample_weight_test = test['SampleWeight']\n",
    "\n",
    "    # evaluation\n",
    "    maxent.fit(xtrain, ytrain, sample_weight=sample_weight_train)\n",
    "    ypred = maxent.predict(xtest)\n",
    "    auc = f\"{metrics.roc_auc_score(ytest, ypred):0.3f}\"\n",
    "    auc_scores.append(auc)\n",
    "    \n",
    "    # store for plotting\n",
    "    fold += 1\n",
    "    p_test['fold'] = str(fold)\n",
    "    test_dfs.append(p_test)\n",
    "\n",
    "    \n",
    "print(f\"{n_splits}-fold xval AUC scores: {', '.join(auc_scores)}\")\n",
    "\n",
    "folds = gpd.GeoDataFrame(pd.concat(test_dfs, axis=0, ignore_index=True), crs=test.crs)\n",
    "folds.plot(column='fold', legend=True)\n",
    "print(\"Test fold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7797dcc2",
   "metadata": {},
   "source": [
    "The plot above shows the spatial distributions of test folds fom the geographic k-fold splits. The third split - the points in the Sierras to the east - had the lowest test performance. This can be interpreted in several ways, which we won't get into here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcfd36f",
   "metadata": {},
   "source": [
    "### Buffered leave-one-out\n",
    "\n",
    "Leave-one-out cross-validation refers to training on *n-1* points and testing on a single test point, looping over the full dataset to test on each point separately.\n",
    "\n",
    "The buffered leave-one-out strategy, as described by [Ploton et al.](https://www.nature.com/articles/s41467-020-18321-y), modifies this approach. While each point is iterated over for testing, the pool of available training points is reduced at each iteration by dropping training points located within a certain distance of the test data. The purpose of this method is to evaluate how a model performs far away from where it was trained.\n",
    "\n",
    "In SDM contexts, however, standard leave-one-out strategies may not be appropriate. Model performance is best evaluated on presence-only data; model performance on background points may not be meaningful.\n",
    "\n",
    "To only run the leave-one-out analysis on presence-only points, specify the column with the 0/1 class labels during train/test splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbdd7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use Ariolimax stramineus records here (fewer samples)\n",
    "stramineus = ariolimax[ariolimax['species'] == 'Ariolimax stramineus']\n",
    "presence = ela.annotate(\n",
    "    stramineus.geometry,\n",
    "    rasters,\n",
    "    labels=labels,\n",
    "    drop_na=True,\n",
    ")\n",
    "presence.dropna(inplace=True)\n",
    "merged = ela.stack_geodataframes(presence, background, add_class_label=True)\n",
    "\n",
    "# store the individual sample predictions and evaluate at the end\n",
    "yobs_scores = []\n",
    "ypred_scores = []\n",
    "\n",
    "# buffered leave-one-out, 5km buffer radius\n",
    "distance = 5_000\n",
    "bloo = ela.BufferedLeaveOneOut(distance=distance)\n",
    "for train_idx, test_idx in bloo.split(merged, class_label=\"class\"):\n",
    "    train = merged.iloc[train_idx]\n",
    "    test = merged.iloc[test_idx]\n",
    "    \n",
    "    # set up model fitting\n",
    "    xtrain = train.drop(columns=['class'])\n",
    "    ytrain = train['class']\n",
    "    xtest = test.drop(columns=['class'])\n",
    "    ytest = test['class']\n",
    "\n",
    "    # evaluation\n",
    "    maxent.fit(xtrain, ytrain)\n",
    "    ypred = maxent.predict(xtest)\n",
    "    ypred_scores.append(ypred)\n",
    "    yobs_scores.append(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcdc3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model predictions for just the background\n",
    "bg = merged[merged['class'] == 0]\n",
    "xbg = bg.drop(columns='class')\n",
    "ybg = bg['class'].to_numpy()\n",
    "bg_pred = maxent.predict(xbg)\n",
    "\n",
    "# concatenate the background and leave-one-out points\n",
    "bl_pred = np.array(ypred_scores)\n",
    "bl_obs = np.array(yobs_scores)\n",
    "ypred = np.vstack((bg_pred.reshape(-1,1), bl_pred))\n",
    "yobs = np.vstack((ybg.reshape(-1,1), bl_obs))\n",
    "\n",
    "# evaluate\n",
    "auc = metrics.roc_auc_score(yobs, ypred)\n",
    "print(f\"Buffered leave-one-out AUC: {auc:0.3f}\")\n",
    "\n",
    "# no buffer\n",
    "x = merged.drop(columns=['class'])\n",
    "y = merged['class']\n",
    "maxent.fit(x, y)\n",
    "ypred = maxent.predict(x)\n",
    "auc = metrics.roc_auc_score(y, ypred)\n",
    "print(f\"Naive model AUC: {auc:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73530ebb",
   "metadata": {},
   "source": [
    "This function also modifies the \"leave-one-out\" approach to support testing on multiple points per-fold. You may want to run your cross-validation to evaluate test performance across multiple ecoregions, for example.\n",
    "\n",
    "You can do this by passing the `group` keyword during train/test splitting, and the value should correspond to a column name in the GeoDataFrame you pass.\n",
    "\n",
    "\n",
    "```python\n",
    "bloo = ela.BufferedLeaveOneOut(distance=1000)\n",
    "for train_idx, test_idx in bloo.split(merged, group=\"ecoregion\"):\n",
    "    train_points = merged.iloc[train_idx]\n",
    "    test_points = merged.iloc[test_idx]\n",
    "    # assumes `merged` has an 'ecoregion' column\n",
    "\n",
    "```\n",
    "\n",
    "This will find the unique values in the `ecoregion` column and iterate over those values, using the points from each ecoregion as test folds and excluding points within 1000m of those locations from the training data. This example is simply demonstrative and will not work as-is because this group label doesn't exist for this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
